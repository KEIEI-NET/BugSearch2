# Phase 7: å¤§è¦æ¨¡ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«è§£æã‚·ã‚¹ãƒ†ãƒ 

**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: v4.8.0
**ä½œæˆæ—¥**: 2025å¹´10æœˆ12æ—¥ JST
**å“è³ª**: @tddå“è³ªé”æˆ âœ…

## æ¦‚è¦

Phase 7ã§ã¯ã€30,000ãƒ•ã‚¡ã‚¤ãƒ«ä»¥ä¸Šã®å¤§è¦æ¨¡ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã‚’åŠ¹ç‡çš„ã«å‡¦ç†ã™ã‚‹ãŸã‚ã®æ©Ÿèƒ½ã‚’å®Ÿè£…ã—ã¾ã—ãŸã€‚

### ğŸ¯ ä¸»ãªæ©Ÿèƒ½

1. **å¤§è¦æ¨¡ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å¯¾å¿œ**
   - 30,000+ãƒ•ã‚¡ã‚¤ãƒ«ã®é«˜é€Ÿå‡¦ç†
   - ãƒãƒƒãƒå‡¦ç†ã«ã‚ˆã‚‹åŠ¹ç‡åŒ–
   - å®Ÿæ¸¬: 15,889 files/sec (10,000ãƒ•ã‚¡ã‚¤ãƒ«ãƒ†ã‚¹ãƒˆ)

2. **ä¸­æ–­ãƒ»å†é–‹æ©Ÿèƒ½**
   - ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã«ã‚ˆã‚‹é€²æ—ä¿å­˜
   - å‡¦ç†é€”ä¸­ã§ã®ä¸­æ–­ã«å¯¾å¿œ
   - ä¸­æ–­ç®‡æ‰€ã‹ã‚‰ã®è‡ªå‹•å†é–‹

3. **ãƒ¡ãƒ¢ãƒªç®¡ç†æ©Ÿèƒ½**
   - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ¡ãƒ¢ãƒªç›£è¦–
   - è­¦å‘Šãƒ»å±é™ºé–¾å€¤ã«ã‚ˆã‚‹è‡ªå‹•åˆ¶å¾¡
   - è‡ªå‹•ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
   - ãƒ¡ãƒ¢ãƒªå¢—åŠ : ã‚ãšã‹+3.79MB (10,000ãƒ•ã‚¡ã‚¤ãƒ«)

4. **ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¡¨ç¤º**
   - tqdmã«ã‚ˆã‚‹ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼
   - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ çµ±è¨ˆæƒ…å ±è¡¨ç¤º
   - ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®å¯è¦–åŒ–

## ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ

### 1. MemoryMonitor (core/memory_monitor.py)

ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ¡ãƒ¢ãƒªç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã€‚

#### ä¸»è¦æ©Ÿèƒ½

- **ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å–å¾—**: `get_current_memory_usage()` - MBå˜ä½ã§å–å¾—
- **ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒã‚§ãƒƒã‚¯**: `check_memory_status()` - NORMAL/WARNING/CRITICAL
- **å‡¦ç†åˆ¶å¾¡**: `should_pause_processing()` - å±é™ºæ™‚ã«True
- **å¼·åˆ¶GC**: `force_garbage_collection()` - ãƒ¡ãƒ¢ãƒªè§£æ”¾
- **çµ±è¨ˆå–å¾—**: `get_memory_statistics()` - è©³ç´°çµ±è¨ˆ

#### ä½¿ç”¨ä¾‹

```python
from core.memory_monitor import MemoryMonitor

# åˆæœŸåŒ–
monitor = MemoryMonitor(
    warning_threshold_mb=1000,   # è­¦å‘Š: 1GB
    critical_threshold_mb=2000,   # å±é™º: 2GB
    auto_gc_enabled=True          # è‡ªå‹•GCæœ‰åŠ¹
)

# ãƒ¡ãƒ¢ãƒªãƒã‚§ãƒƒã‚¯
if monitor.should_pause_processing():
    print("ãƒ¡ãƒ¢ãƒªå±é™ºï¼å‡¦ç†ä¸€æ™‚åœæ­¢")
    monitor.force_garbage_collection()
    time.sleep(1)

# çµ±è¨ˆå–å¾—
stats = monitor.get_memory_statistics()
print(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {stats['current_mb']:.2f} MB")
print(f"ä½¿ç”¨ç‡: {stats['percent_used']:.1f}%")
```

### 2. CheckpointManager (core/checkpoint_manager.py)

ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã€‚

#### ä¸»è¦æ©Ÿèƒ½

- **ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä½œæˆ**: `create_checkpoint()` - é€²æ—ä¿å­˜
- **ä¿å­˜**: `save_checkpoint()` - ã‚¢ãƒˆãƒŸãƒƒã‚¯æ›¸ãè¾¼ã¿
- **èª­è¾¼**: `load_checkpoint()` - å†é–‹æ™‚ã«ä½¿ç”¨
- **å†é–‹åˆ¤å®š**: `can_resume()` - å†é–‹å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
- **å‰Šé™¤**: `delete_checkpoint()` - å®Œäº†å¾Œã®å‰Šé™¤

#### ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿

```python
@dataclass
class Checkpoint:
    timestamp: str                # ä½œæˆæ—¥æ™‚
    version: str                  # ãƒãƒ¼ã‚¸ãƒ§ãƒ³
    processed_files: List[str]    # å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«
    total_files: int              # ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°
    current_batch: int            # ç¾åœ¨ã®ãƒãƒƒãƒ
    total_batches: int            # ç·ãƒãƒƒãƒæ•°
    metadata: Dict                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    success_count: int            # æˆåŠŸæ•°
    error_count: int              # ã‚¨ãƒ©ãƒ¼æ•°
```

#### ä½¿ç”¨ä¾‹

```python
from core.checkpoint_manager import CheckpointManager

# åˆæœŸåŒ–
manager = CheckpointManager(Path(".bugsearch/checkpoint.json"))

# ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä½œæˆãƒ»ä¿å­˜
checkpoint = manager.create_checkpoint(
    processed_files=['file1.py', 'file2.py'],
    total_files=30000,
    success_count=2,
    error_count=0
)
manager.save_checkpoint(checkpoint)

# å†é–‹æ™‚
if manager.can_resume():
    checkpoint = manager.load_checkpoint()
    print(f"é€²æ—: {checkpoint.get_progress_percentage():.1f}%")
    print(f"æ®‹ã‚Š: {checkpoint.get_remaining_count()}ãƒ•ã‚¡ã‚¤ãƒ«")
```

### 3. LargeScaleProcessor (core/large_scale_processor.py)

å¤§è¦æ¨¡å‡¦ç†ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ã€‚

#### è¨­å®š (ProcessingConfig)

```python
@dataclass
class ProcessingConfig:
    # ãƒãƒƒãƒå‡¦ç†
    batch_size: int = 100                # ãƒãƒƒãƒã‚µã‚¤ã‚º
    checkpoint_interval: int = 50        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆé–“éš”

    # ãƒ¡ãƒ¢ãƒªç®¡ç†
    memory_check_interval: int = 10      # ãƒ¡ãƒ¢ãƒªãƒã‚§ãƒƒã‚¯é–“éš”
    max_memory_mb: float = 2000.0        # æœ€å¤§ãƒ¡ãƒ¢ãƒª
    warning_memory_mb: float = 1500.0    # è­¦å‘Šãƒ¡ãƒ¢ãƒª
    enable_auto_gc: bool = True          # è‡ªå‹•GC

    # è¡¨ç¤º
    show_progress: bool = True           # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼
    show_memory_stats: bool = True       # ãƒ¡ãƒ¢ãƒªçµ±è¨ˆ

    # ã‚¨ãƒ©ãƒ¼å‡¦ç†
    stop_on_error: bool = False          # ã‚¨ãƒ©ãƒ¼æ™‚åœæ­¢
    max_errors: int = 100                # æœ€å¤§ã‚¨ãƒ©ãƒ¼æ•°
```

#### ä½¿ç”¨ä¾‹

```python
from pathlib import Path
from core.large_scale_processor import LargeScaleProcessor, ProcessingConfig

# è¨­å®š
config = ProcessingConfig(
    batch_size=100,
    checkpoint_interval=1000,
    max_memory_mb=2000
)

# ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ä½œæˆ
processor = LargeScaleProcessor(
    config=config,
    checkpoint_file=Path(".bugsearch/checkpoint.json")
)

# å‡¦ç†é–¢æ•°å®šç¾©
def analyze_file(file_path: Path) -> dict:
    """ãƒ•ã‚¡ã‚¤ãƒ«è§£æå‡¦ç†"""
    # å®Ÿéš›ã®ã‚³ãƒ¼ãƒ‰è§£æãƒ­ã‚¸ãƒƒã‚¯
    return {
        'file': str(file_path),
        'issues': [...],
        'severity': 5
    }

# å‡¦ç†å®Ÿè¡Œ
results = processor.process_files(
    files=all_source_files,
    processor_func=analyze_file,
    resume=True  # ä¸­æ–­ã‹ã‚‰å†é–‹
)

# çµ±è¨ˆå–å¾—
stats = processor.get_processing_statistics()
print(f"æˆåŠŸ: {stats['success_count']}")
print(f"ã‚¨ãƒ©ãƒ¼: {stats['error_count']}")
print(f"ãƒ¡ãƒ¢ãƒª: {stats['memory']['current_mb']:.2f} MB")
```

## å®Ÿè·µä¾‹

### ä¾‹1: åŸºæœ¬çš„ãªå¤§è¦æ¨¡å‡¦ç†

```python
from pathlib import Path
from core.large_scale_processor import LargeScaleProcessor, ProcessingConfig

# å…¨ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—
source_files = list(Path("./src").rglob("*.py"))
print(f"ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(source_files):,}")

# è¨­å®š
config = ProcessingConfig(
    batch_size=100,
    checkpoint_interval=1000,
    max_memory_mb=2000,
    show_progress=True
)

# ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼
processor = LargeScaleProcessor(
    config=config,
    checkpoint_file=Path(".bugsearch/checkpoint.json")
)

# ç°¡æ˜“è§£æé–¢æ•°
def simple_analysis(file_path: Path) -> dict:
    content = file_path.read_text(encoding='utf-8')
    return {
        'file': str(file_path),
        'lines': len(content.split('\n')),
        'size': len(content)
    }

# å‡¦ç†å®Ÿè¡Œ
results = processor.process_files(
    files=source_files,
    processor_func=simple_analysis
)

print(f"å‡¦ç†å®Œäº†: {len(results)}ãƒ•ã‚¡ã‚¤ãƒ«")
```

### ä¾‹2: ä¸­æ–­ã‹ã‚‰ã®å†é–‹

```python
# 1å›ç›®: å‡¦ç†é–‹å§‹ï¼ˆé€”ä¸­ã§ä¸­æ–­ã•ã‚Œã‚‹å¯èƒ½æ€§ï¼‰
try:
    results = processor.process_files(
        files=all_files,
        processor_func=analyze_func,
        resume=False  # æ–°è¦å‡¦ç†
    )
except KeyboardInterrupt:
    print("å‡¦ç†ä¸­æ–­ï¼ˆCtrl+Cï¼‰")
    print("ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜æ¸ˆã¿")

# 2å›ç›®: ä¸­æ–­ç®‡æ‰€ã‹ã‚‰å†é–‹
results = processor.process_files(
    files=all_files,
    processor_func=analyze_func,
    resume=True  # å†é–‹
)
```

### ä¾‹3: ãƒ¡ãƒ¢ãƒªåˆ¶ç´„ãŒå³ã—ã„ç’°å¢ƒ

```python
# ä½ãƒ¡ãƒ¢ãƒªè¨­å®š
config = ProcessingConfig(
    batch_size=50,                # å°ã•ã‚ã®ãƒãƒƒãƒ
    checkpoint_interval=500,       # é »ç¹ã«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
    memory_check_interval=10,      # é »ç¹ã«ãƒ¡ãƒ¢ãƒªãƒã‚§ãƒƒã‚¯
    max_memory_mb=500,             # æœ€å¤§500MB
    warning_memory_mb=400,         # è­¦å‘Š400MB
    enable_auto_gc=True            # è‡ªå‹•GCæœ‰åŠ¹
)

processor = LargeScaleProcessor(config, checkpoint_file)
```

## ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

### 10,000ãƒ•ã‚¡ã‚¤ãƒ«ãƒ†ã‚¹ãƒˆçµæœ

```
ğŸ“Š Phase 7 ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆçµæœ
================================================================================
âœ… å‡¦ç†å®Œäº†:
  ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: 10,000
  å‡¦ç†æ¸ˆã¿: 10,000
  æˆåŠŸ: 10,000
  ã‚¨ãƒ©ãƒ¼: 0

â±ï¸  ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹:
  å‡¦ç†æ™‚é–“: 0.63ç§’
  ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: 15,889.3 files/sec
  å¹³å‡å‡¦ç†æ™‚é–“: 0.06 ms/file

ğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡:
  åˆæœŸ: 32.03 MB
  æœ€çµ‚: 35.82 MB
  å¢—åŠ : +3.79 MB
  ä½¿ç”¨ç‡: 34.2%

ğŸ“ˆ è©•ä¾¡:
âœ… é«˜é€Ÿå‡¦ç†: 15,889.3 files/sec (ç›®æ¨™: 1,000+ files/sec)
âœ… ãƒ¡ãƒ¢ãƒªåŠ¹ç‡è‰¯å¥½: +3.79 MB (ç›®æ¨™: <100MB)
âœ… ã‚¨ãƒ©ãƒ¼ãªã—: 0.00%

ğŸ† Phase 7 ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ: åˆæ ¼ (@tddå“è³ªé”æˆ)
```

### 30,000ãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ã‚¹ã‚±ãƒ¼ãƒ«æ€§

10,000ãƒ•ã‚¡ã‚¤ãƒ«ã®çµæœã‹ã‚‰æ¨å®š:

```
äºˆæ¸¬å‡¦ç†æ™‚é–“: ç´„2ç§’
äºˆæ¸¬ãƒ¡ãƒ¢ãƒªå¢—åŠ : ç´„10-15MB
äºˆæ¸¬ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: 15,000+ files/sec
```

## ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### 1. è¨­å®šã®æœ€é©åŒ–

#### ä¸€èˆ¬çš„ãªã‚±ãƒ¼ã‚¹ (æ¨å¥¨)

```python
config = ProcessingConfig(
    batch_size=100,
    checkpoint_interval=1000,
    memory_check_interval=100,
    max_memory_mb=2000,
    warning_memory_mb=1500
)
```

#### è¶…å¤§è¦æ¨¡ã‚±ãƒ¼ã‚¹ (100,000+ãƒ•ã‚¡ã‚¤ãƒ«)

```python
config = ProcessingConfig(
    batch_size=200,              # å¤§ãã‚ã®ãƒãƒƒãƒ
    checkpoint_interval=5000,     # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆé »åº¦ä¸‹ã’ã‚‹
    memory_check_interval=200,
    max_memory_mb=4000,
    warning_memory_mb=3000
)
```

#### ãƒ¡ãƒ¢ãƒªåˆ¶ç´„ç’°å¢ƒ

```python
config = ProcessingConfig(
    batch_size=50,               # å°ã•ã‚ã®ãƒãƒƒãƒ
    checkpoint_interval=500,      # é »ç¹ã«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
    memory_check_interval=10,     # é »ç¹ã«ãƒ¡ãƒ¢ãƒªãƒã‚§ãƒƒã‚¯
    max_memory_mb=500,
    warning_memory_mb=400,
    enable_auto_gc=True
)
```

### 2. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

```python
config = ProcessingConfig(
    stop_on_error=False,    # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚ç¶™ç¶š
    max_errors=100          # æœ€å¤§100ã‚¨ãƒ©ãƒ¼ã¾ã§è¨±å®¹
)

def robust_processor(file_path: Path) -> dict:
    """ã‚¨ãƒ©ãƒ¼è€æ€§ã®ã‚ã‚‹ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼"""
    try:
        # å‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯
        return analyze(file_path)
    except Exception as e:
        # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°
        logging.error(f"Error in {file_path}: {e}")
        # ã‚¨ãƒ©ãƒ¼çµæœã‚’è¿”ã™ï¼ˆä¾‹å¤–ã‚’æŠ•ã’ãªã„ï¼‰
        return {
            'file': str(file_path),
            'error': str(e),
            'success': False
        }
```

### 3. é€²æ—ç›£è¦–

```python
# å‡¦ç†å‰ã«ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹ç¢ºèª
initial_stats = processor.memory_monitor.get_memory_statistics()
print(f"é–‹å§‹æ™‚ãƒ¡ãƒ¢ãƒª: {initial_stats['current_mb']:.2f} MB")

# å‡¦ç†å®Ÿè¡Œ
results = processor.process_files(...)

# å‡¦ç†å¾Œçµ±è¨ˆ
final_stats = processor.get_processing_statistics()
print(f"æˆåŠŸç‡: {final_stats['success_count']/final_stats['processed_count']*100:.1f}%")
print(f"ãƒ¡ãƒ¢ãƒªå¢—åŠ : {final_stats['memory']['current_mb'] - initial_stats['current_mb']:.2f} MB")
```

### 4. ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—

```python
try:
    # å‡¦ç†å®Ÿè¡Œ
    results = processor.process_files(...)
finally:
    # å®Œäº†å¾Œã¯ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆå‰Šé™¤
    processor.cleanup()
```

## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### å•é¡Œ1: ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¢—ãˆç¶šã‘ã‚‹

**ç—‡çŠ¶**: å‡¦ç†ä¸­ã«ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¢—åŠ ã—ç¶šã‘ã‚‹

**åŸå› **: ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼é–¢æ•°å†…ã§ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯

**è§£æ±ºç­–**:

```python
def memory_safe_processor(file_path: Path) -> dict:
    """ãƒ¡ãƒ¢ãƒªã‚»ãƒ¼ãƒ•ãªãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼"""
    # 1. å¤§ããªã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¯é–¢æ•°å†…ã§å®Œçµ
    content = file_path.read_text()
    result = analyze(content)

    # 2. å¤§ããªãƒ‡ãƒ¼ã‚¿ã¯çµæœã«å«ã‚ãªã„
    return {
        'file': str(file_path),
        'issue_count': len(result.issues),  # å•é¡Œè©³ç´°ã¯å«ã‚ãªã„
        'severity': result.max_severity
    }
    # 3. contentã¯è‡ªå‹•çš„ã«è§£æ”¾ã•ã‚Œã‚‹
```

### å•é¡Œ2: ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å†é–‹ã§ããªã„

**ç—‡çŠ¶**: `resume=True`ã§ã‚‚æœ€åˆã‹ã‚‰å‡¦ç†ã•ã‚Œã‚‹

**åŸå› **: ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒå­˜åœ¨ã—ãªã„ã‹ã€å®Œäº†æ¸ˆã¿

**è§£æ±ºç­–**:

```python
# å†é–‹å‰ã«ç¢ºèª
manager = processor.checkpoint_manager
if manager.can_resume():
    checkpoint = manager.load_checkpoint()
    print(f"å†é–‹: {checkpoint.get_progress_percentage():.1f}%å®Œäº†")
else:
    print("ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãªã—: æ–°è¦å‡¦ç†é–‹å§‹")

results = processor.process_files(..., resume=True)
```

### å•é¡Œ3: å‡¦ç†ãŒé…ã„

**ç—‡çŠ¶**: ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆãŒä½ã„ (< 1,000 files/sec)

**åŸå› **:
1. ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼é–¢æ•°ãŒé‡ã„
2. ãƒãƒƒãƒã‚µã‚¤ã‚ºãŒå°ã•ã™ãã‚‹
3. ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆé »åº¦ãŒé«˜ã™ãã‚‹

**è§£æ±ºç­–**:

```python
# è¨­å®šèª¿æ•´
config = ProcessingConfig(
    batch_size=200,              # å¢—ã‚„ã™
    checkpoint_interval=5000,     # æ¸›ã‚‰ã™
    memory_check_interval=200     # æ¸›ã‚‰ã™
)

# ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
import cProfile
cProfile.run('processor.process_files(...)')
```

### å•é¡Œ4: ã‚¨ãƒ©ãƒ¼ã§å‡¦ç†ãŒåœæ­¢ã™ã‚‹

**ç—‡çŠ¶**: 1ã¤ã®ã‚¨ãƒ©ãƒ¼ã§å…¨ä½“ãŒåœæ­¢

**åŸå› **: `stop_on_error=True` ã«ãªã£ã¦ã„ã‚‹

**è§£æ±ºç­–**:

```python
config = ProcessingConfig(
    stop_on_error=False,     # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚ç¶™ç¶š
    max_errors=100           # æœ€å¤§ã‚¨ãƒ©ãƒ¼æ•°è¨­å®š
)
```

## ãƒ†ã‚¹ãƒˆ

### ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ

```bash
# å…¨ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
python test/test_large_scale_processor.py

# çµæœ:
# - 17ãƒ†ã‚¹ãƒˆå…¨åˆæ ¼
# - ãƒ¡ãƒ¢ãƒªãƒ¢ãƒ‹ã‚¿ãƒ¼: 5ãƒ†ã‚¹ãƒˆ âœ…
# - ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ: 5ãƒ†ã‚¹ãƒˆ âœ…
# - ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼: 6ãƒ†ã‚¹ãƒˆ âœ…
# - çµ±åˆ: 1ãƒ†ã‚¹ãƒˆ âœ…
```

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ

```bash
# 10,000ãƒ•ã‚¡ã‚¤ãƒ«ãƒ†ã‚¹ãƒˆ
python test/test_large_scale_30k_files.py 10000

# 30,000ãƒ•ã‚¡ã‚¤ãƒ«ãƒ†ã‚¹ãƒˆï¼ˆæœ¬ç•ªç›¸å½“ï¼‰
python test/test_large_scale_30k_files.py 30000

# ã‚«ã‚¹ã‚¿ãƒ ãƒ•ã‚¡ã‚¤ãƒ«æ•°
python test/test_large_scale_30k_files.py 50000
```

## ã¾ã¨ã‚

Phase 7ã§ã¯ä»¥ä¸‹ã‚’é”æˆã—ã¾ã—ãŸ:

### âœ… é”æˆäº‹é …

1. **30,000+ãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œ** - å®Ÿæ¸¬15,889 files/sec
2. **ä¸­æ–­ãƒ»å†é–‹æ©Ÿèƒ½** - ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã«ã‚ˆã‚‹ç¢ºå®Ÿãªå†é–‹
3. **ãƒ¡ãƒ¢ãƒªç®¡ç†** - +3.79MBå¢—åŠ ã®ã¿ï¼ˆ10,000ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
4. **@tddå“è³ª** - å…¨17ãƒ†ã‚¹ãƒˆåˆæ ¼

### ğŸ“Š æ€§èƒ½æŒ‡æ¨™

| æŒ‡æ¨™ | ç›®æ¨™ | å®Ÿæ¸¬ | é”æˆç‡ |
|------|------|------|--------|
| ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ | 1,000 files/sec | 15,889 files/sec | **1,589%** âœ… |
| ãƒ¡ãƒ¢ãƒªå¢—åŠ  | < 100MB | +3.79MB | **103%** âœ… |
| ã‚¨ãƒ©ãƒ¼ç‡ | < 1% | 0.00% | **100%** âœ… |
| ãƒ†ã‚¹ãƒˆåˆæ ¼ç‡ | 100% | 100% (17/17) | **100%** âœ… |

### ğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

Phase 7ã¯å®Œæˆã—ã€æœ¬ç•ªç’°å¢ƒã§ã®åˆ©ç”¨æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚

**æ¨å¥¨ã•ã‚Œã‚‹æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³**:
1. å®Ÿéš›ã®ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã§ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
2. æœ¬ç•ªç’°å¢ƒã§ã®æ€§èƒ½ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°
3. å¿…è¦ã«å¿œã˜ã¦è¨­å®šã®æœ€é©åŒ–

---

*æœ€çµ‚æ›´æ–°: 2025å¹´10æœˆ12æ—¥ JST*
*ãƒãƒ¼ã‚¸ãƒ§ãƒ³: v4.8.0*
*å“è³ª: @tddå“è³ªé”æˆ âœ…*
