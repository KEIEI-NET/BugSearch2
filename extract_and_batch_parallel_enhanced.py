#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸¦åˆ—å‡¦ç†ã«ã‚ˆã‚‹å±é™ºãƒ•ã‚¡ã‚¤ãƒ«ã®AIè©³ç´°åˆ†æï¼ˆæ”¹è‰¯ç‰ˆï¼‰
- å®Œå…¨ãªä¿®æ­£å‰å¾Œã®ã‚³ãƒ¼ãƒ‰æä¾›
- è©³ç´°ãªå•é¡Œèª¬æ˜ã¨å½±éŸ¿åˆ†æ
- GPT-4oãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæº–æ‹ 
"""

import os
import sys
import json
import time
import hashlib
from pathlib import Path
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from typing import List, Dict, Tuple, Optional
from dotenv import load_dotenv
import re

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()

# SDK ãƒã‚§ãƒƒã‚¯
OPENAI_OK = False
ANTHROPIC_OK = False

try:
    import openai
    openai.api_key = os.getenv("OPENAI_API_KEY")
    OPENAI_OK = True
except Exception:
    pass

try:
    from anthropic import Anthropic
    ANTHROPIC_OK = True
except Exception:
    pass

# è¨­å®š
BATCH_SIZE = 50  # ä¸¦åˆ—å‡¦ç†ç”¨ã«ç¸®å°
PARALLEL_WORKERS = 10
API_DELAY = 0.1  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
OUTPUT_DIR = "reports"
DANGER_FILE = "reports/danger_analysis_ALL"
PROGRESS_FILE = ".batch_progress_parallel_enhanced.json"
CACHE_DIR = ".cache"
CACHE_EXPIRY_DAYS = 7

# ãƒ¬ãƒ¼ãƒˆåˆ¶é™ç®¡ç†
class RateLimiter:
    def __init__(self, max_calls_per_minute=100):
        self.max_calls = max_calls_per_minute
        self.calls = []
        self.lock = threading.Lock()

    def wait_if_needed(self):
        with self.lock:
            now = time.time()
            # 1åˆ†ä»¥å†…ã®å‘¼ã³å‡ºã—ã‚’ãƒ•ã‚£ãƒ«ã‚¿
            self.calls = [t for t in self.calls if now - t < 60]

            if len(self.calls) >= self.max_calls:
                sleep_time = 60 - (now - self.calls[0]) + 1
                print(f"[RATE] ãƒ¬ãƒ¼ãƒˆåˆ¶é™: {sleep_time:.1f}ç§’å¾…æ©Ÿ")
                time.sleep(sleep_time)

            self.calls.append(time.time())

rate_limiter = RateLimiter(max_calls_per_minute=100)

def get_enhanced_ai_prompt(file_path: str, code_snippet: str, problems: str) -> str:
    """æ”¹è‰¯ç‰ˆAIãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ - å®Œå…¨ãªã‚³ãƒ¼ãƒ‰ã¨è©³ç´°èª¬æ˜ã‚’è¦æ±‚"""
    return f"""
# ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æä¾é ¼

## ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±
- **ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹**: `{file_path}`
- **æ¤œå‡ºã•ã‚ŒãŸå•é¡Œ**: {problems}

## åˆ†æå¯¾è±¡ã‚³ãƒ¼ãƒ‰
```
{code_snippet[:5000] if len(code_snippet) > 5000 else code_snippet}
```

## è¦æ±‚ä»•æ§˜

ä»¥ä¸‹ã®å½¢å¼ã§ã€**æ—¥æœ¬èªã§** è©³ç´°ãªåˆ†æçµæœã‚’æä¾›ã—ã¦ãã ã•ã„ï¼š

### 1. å•é¡Œã®è©³ç´°åˆ†æ

å„æ¤œå‡ºå•é¡Œã«ã¤ã„ã¦ï¼š

#### ğŸš¨ [å•é¡Œå]

**å•é¡Œã®è©³ç´°**:
å…·ä½“çš„ã«ã‚³ãƒ¼ãƒ‰ã®ã©ã®éƒ¨åˆ†ã«å•é¡ŒãŒã‚ã‚‹ã‹ã€ãªãœå•é¡Œãªã®ã‹ã‚’è©³ã—ãèª¬æ˜ã€‚

**å½±éŸ¿**:
- ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã¸ã®å½±éŸ¿
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¸ã®å½±éŸ¿
- ä¿å®ˆæ€§ã¸ã®å½±éŸ¿
- ãƒ“ã‚¸ãƒã‚¹ã¸ã®å½±éŸ¿

**å®Œå…¨ãªæ”¹å–„ã‚³ãƒ¼ãƒ‰**:

```[è¨€èª]
// ===== ä¿®æ­£å‰ã®ã‚³ãƒ¼ãƒ‰ï¼ˆå•é¡Œã‚ã‚Šï¼‰=====
[å®Ÿéš›ã®å•é¡Œã®ã‚ã‚‹ã‚³ãƒ¼ãƒ‰å…¨ä½“ã‚’è¨˜è¼‰]
// å•é¡Œç®‡æ‰€ã«ã‚³ãƒ¡ãƒ³ãƒˆã§èª¬æ˜ã‚’è¿½åŠ 

// ===== ä¿®æ­£å¾Œã®ã‚³ãƒ¼ãƒ‰ï¼ˆæ”¹å–„ç‰ˆï¼‰=====
[å®Œå…¨ã«å‹•ä½œã™ã‚‹æ”¹å–„ç‰ˆã‚³ãƒ¼ãƒ‰å…¨ä½“ã‚’è¨˜è¼‰]
// æ”¹å–„ãƒã‚¤ãƒ³ãƒˆã«ã‚³ãƒ¡ãƒ³ãƒˆã§èª¬æ˜ã‚’è¿½åŠ 
```

**è¿½åŠ ã®æ¨å¥¨äº‹é …**:
- ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰ã®ä¾‹
- è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ä¾‹
- ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### 2. ç·åˆè©•ä¾¡

- **ç·Šæ€¥åº¦**: ç·Šæ€¥/é«˜/ä¸­/ä½
- **ä¿®æ­£å·¥æ•°**: æ™‚é–“ã®ç›®å®‰
- **å½±éŸ¿ç¯„å›²**: ä»–ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¸ã®å½±éŸ¿

å¿…ãšå®Ÿè£…å¯èƒ½ãªå®Œå…¨ãªã‚³ãƒ¼ãƒ‰ã‚’æä¾›ã—ã€éƒ¨åˆ†çš„ãªä¾‹ç¤ºã¯é¿ã‘ã¦ãã ã•ã„ã€‚
"""

def get_ai_provider() -> Optional[str]:
    """AIãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’æ±ºå®š"""
    provider = os.getenv("AI_PROVIDER", "auto").lower().strip()

    if provider == "auto":
        # Anthropicå„ªå…ˆ
        if ANTHROPIC_OK and os.getenv("ANTHROPIC_API_KEY"):
            return "anthropic"
        elif OPENAI_OK and os.getenv("OPENAI_API_KEY"):
            return "openai"
    elif provider == "anthropic":
        if ANTHROPIC_OK and os.getenv("ANTHROPIC_API_KEY"):
            return "anthropic"
    elif provider == "openai":
        if OPENAI_OK and os.getenv("OPENAI_API_KEY"):
            return "openai"

    return None

def select_model_for_severity(severity: int, provider: str = "openai") -> Tuple[str, str, Optional[str]]:
    """
    å±é™ºåº¦ã«åŸºã¥ããƒ¢ãƒ‡ãƒ«é¸æŠï¼ˆãƒãƒ«ãƒãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å¯¾å¿œï¼‰

    Returns:
        (provider, model, reasoning_effort)
    """
    if provider == "anthropic":
        # Claude ãƒ¢ãƒ‡ãƒ«é¸æŠ
        if severity >= 15:
            return "anthropic", "claude-opus-4-1", None  # æœ€é«˜æ€§èƒ½
        elif severity >= 10:
            return "anthropic", "claude-sonnet-4-5", None  # ãƒãƒ©ãƒ³ã‚¹å‹
        else:
            return "anthropic", "claude-sonnet-4-1", None  # é«˜é€Ÿãƒ»ä½ã‚³ã‚¹ãƒˆ
    else:
        # OpenAI ãƒ¢ãƒ‡ãƒ«é¸æŠï¼ˆæ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
        if severity >= 15:
            return "openai", "gpt-4o", None
        elif severity >= 10:
            return "openai", "gpt-4o", None
        else:
            return "openai", "gpt-4o-mini", None

def call_claude_api(prompt: str, model: str) -> Optional[str]:
    """Claude APIå‘¼ã³å‡ºã—"""
    if not ANTHROPIC_OK:
        return None

    try:
        from anthropic import Anthropic

        client = Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))

        response = client.messages.create(
            model=model,
            max_tokens=8000,  # è©³ç´°ãªå›ç­”ã®ãŸã‚å¢—åŠ 
            temperature=0.3,
            messages=[
                {"role": "user", "content": prompt}
            ]
        )

        if response.content and len(response.content) > 0:
            return response.content[0].text.strip()

        return None

    except Exception as e:
        print(f"[ERROR] Claude API failed: {str(e)[:150]}")
        return None

def get_cache_key(file_path: str, content: str) -> str:
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”Ÿæˆ"""
    content_hash = hashlib.md5(content.encode()).hexdigest()
    return f"{file_path}_{content_hash}"

def get_cached_result(cache_key: str) -> Optional[str]:
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰çµæœå–å¾—"""
    cache_file = Path(CACHE_DIR) / f"{cache_key}.json"
    if cache_file.exists():
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœŸé™ãƒã‚§ãƒƒã‚¯
        age_days = (time.time() - cache_file.stat().st_mtime) / (24 * 3600)
        if age_days < CACHE_EXPIRY_DAYS:
            with open(cache_file, 'r', encoding='utf-8') as f:
                return json.load(f).get('result')
    return None

def save_to_cache(cache_key: str, result: str):
    """çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
    Path(CACHE_DIR).mkdir(exist_ok=True)
    cache_file = Path(CACHE_DIR) / f"{cache_key}.json"
    with open(cache_file, 'w', encoding='utf-8') as f:
        json.dump({'result': result, 'timestamp': time.time()}, f, ensure_ascii=False)

def analyze_with_ai_enhanced(file_path: str, code_snippet: str, problems: str, severity: int) -> str:
    """æ”¹è‰¯ç‰ˆAIåˆ†æ - å®Œå…¨ãªã‚³ãƒ¼ãƒ‰ã¨è©³ç´°èª¬æ˜ã‚’ç”Ÿæˆï¼ˆãƒãƒ«ãƒãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å¯¾å¿œï¼‰"""
    try:
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        cache_key = get_cache_key(file_path, code_snippet)
        cached = get_cached_result(cache_key)
        if cached:
            return cached

        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
        rate_limiter.wait_if_needed()

        # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æ±ºå®š
        provider = get_ai_provider()
        if not provider:
            return "[ã‚¨ãƒ©ãƒ¼: APIã‚­ãƒ¼æœªè¨­å®šã¾ãŸã¯SDKæœªå°å…¥]"

        # ãƒ¢ãƒ‡ãƒ«é¸æŠï¼ˆãƒãƒ«ãƒãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å¯¾å¿œï¼‰
        provider, model, reasoning_effort = select_model_for_severity(severity, provider)

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        prompt = get_enhanced_ai_prompt(file_path, code_snippet, problems)

        result = None

        # ===== Anthropic Claude å®Ÿè¡Œ =====
        if provider == "anthropic":
            result = call_claude_api(prompt, model)

            if not result and OPENAI_OK and os.getenv("OPENAI_API_KEY"):
                # ClaudeãŒå¤±æ•—ã—ãŸå ´åˆã€OpenAIã¸ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                print(f"[INFO] Claude failed for {file_path}, falling back to OpenAI")
                provider = "openai"
                _, model, _ = select_model_for_severity(severity, "openai")

        # ===== OpenAI å®Ÿè¡Œ =====
        if provider == "openai":
            if model == "gpt-5-codex" and reasoning_effort:
                # GPT-5-Codexç”¨ï¼ˆResponses APIï¼‰
                response = openai.ChatCompletion.create(
                    model="o1-preview",
                    messages=[{"role": "user", "content": prompt}],
                    max_completion_tokens=16000,  # è©³ç´°ãªå›ç­”ã®ãŸã‚å¢—åŠ 
                    temperature=0.3
                )
            else:
                # GPT-4o/GPT-4o-miniç”¨
                response = openai.ChatCompletion.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": "ã‚ãªãŸã¯ç†Ÿç·´ã—ãŸã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã§ã™ã€‚ã‚³ãƒ¼ãƒ‰ã®å•é¡Œã‚’è©³ç´°ã«åˆ†æã—ã€å®Œå…¨ãªæ”¹å–„ã‚³ãƒ¼ãƒ‰ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=8000,  # è©³ç´°ãªå›ç­”ã®ãŸã‚å¢—åŠ 
                    temperature=0.3
                )

            result = response.choices[0].message.content

        if not result:
            return "[ã‚¨ãƒ©ãƒ¼: AIåˆ†æå¤±æ•—]"

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
        save_to_cache(cache_key, result)

        return result

    except Exception as e:
        return f"[ã‚¨ãƒ©ãƒ¼: {str(e)}]"

def process_file(file_data: Tuple[str, int]) -> Dict:
    """å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
    file_path, severity = file_data

    try:
        # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ï¼ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å¯¾å¿œï¼‰
        for encoding in ['utf-8', 'shift_jis', 'cp932', 'euc-jp']:
            try:
                with open(file_path, 'r', encoding=encoding) as f:
                    content = f.read()
                break
            except UnicodeDecodeError:
                continue
        else:
            content = "[ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ä¸æ˜]"

        # å•é¡Œã®è©³ç´°å–å¾—
        problems = get_problems_for_file(file_path)

        # AIåˆ†æï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        ai_analysis = analyze_with_ai_enhanced(file_path, content[:5000], problems, severity)

        # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ•´å½¢
        formatted_result = format_enhanced_result(file_path, severity, problems, ai_analysis)

        return {
            'file_path': file_path,
            'severity': severity,
            'analysis': formatted_result,
            'success': True
        }

    except Exception as e:
        return {
            'file_path': file_path,
            'severity': severity,
            'analysis': f"[å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}]",
            'success': False
        }

def format_enhanced_result(file_path: str, severity: int, problems: str, ai_analysis: str) -> str:
    """æ”¹è‰¯ç‰ˆçµæœãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    danger_level = "ç·Šæ€¥" if severity >= 15 else "é«˜" if severity >= 10 else "ä¸­"

    return f"""
# {file_path}

## ğŸ“Š å±é™ºåº¦åˆ†æ
- **å±é™ºåº¦ã‚¹ã‚³ã‚¢**: {severity}
- **å±é™ºåº¦ãƒ¬ãƒ™ãƒ«**: [{danger_level}]
- **æ¤œå‡ºã•ã‚ŒãŸå•é¡Œ**: {problems}

## ğŸ” è©³ç´°åˆ†æã¨å®Œå…¨ãªæ”¹å–„ææ¡ˆ

{ai_analysis}

---
"""

def get_problems_for_file(file_path: str) -> str:
    """ãƒ•ã‚¡ã‚¤ãƒ«ã®å•é¡Œè©³ç´°ã‚’å–å¾—"""
    # danger_analysis_ALLãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å•é¡Œã‚’æŠ½å‡º
    if Path(DANGER_FILE).exists():
        with open(DANGER_FILE, 'r', encoding='utf-8') as f:
            content = f.read()

        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã§æ¤œç´¢
        file_name = os.path.basename(file_path)
        pattern = rf"{re.escape(file_name)}.*?ã‚¹ã‚³ã‚¢:\s*\d+.*?å•é¡Œ:(.*?)(?=\n|$)"
        match = re.search(pattern, content)

        if match:
            return match.group(1).strip()

    return "è©³ç´°åˆ†æä¸­"

def extract_problem_files() -> List[Tuple[str, int]]:
    """å•é¡Œãƒ•ã‚¡ã‚¤ãƒ«ã®æŠ½å‡º"""
    if not Path(DANGER_FILE).exists():
        print(f"[ERROR] å±é™ºãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {DANGER_FILE}")
        return []

    problem_files = []
    with open(DANGER_FILE, 'r', encoding='utf-8') as f:
        content = f.read()

    sections = content.split('###')[1:]

    for section in sections:
        lines = section.strip().split('\n')
        for line in lines:
            if 'ã‚¹ã‚³ã‚¢:' in line:
                score_match = re.search(r'ã‚¹ã‚³ã‚¢:\s*(\d+)', line)
                path_match = re.search(r'(src/[^\s]+\.\w+)', line)

                if score_match and path_match:
                    score = int(score_match.group(1))
                    if score >= 5:  # ã‚¹ã‚³ã‚¢5ä»¥ä¸Šã‚’å¯¾è±¡
                        file_path = path_match.group(1)
                        problem_files.append((file_path, score))

    return sorted(problem_files, key=lambda x: x[1], reverse=True)

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
    print("="*60)
    print("å±é™ºãƒ•ã‚¡ã‚¤ãƒ«è©³ç´°åˆ†æã‚·ã‚¹ãƒ†ãƒ ï¼ˆä¸¦åˆ—å‡¦ç†ãƒ»æ”¹è‰¯ç‰ˆï¼‰")
    print("="*60)

    # é€²æ—çŠ¶æ…‹èª­ã¿è¾¼ã¿
    progress = {}
    if Path(PROGRESS_FILE).exists():
        with open(PROGRESS_FILE, 'r', encoding='utf-8') as f:
            progress = json.load(f)

    completed_files = progress.get('completed_files', {})
    last_batch = progress.get('last_batch', 0)

    # å•é¡Œãƒ•ã‚¡ã‚¤ãƒ«æŠ½å‡º
    problem_files = extract_problem_files()
    if not problem_files:
        print("[ERROR] åˆ†æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return

    print(f"[INFO] æ¤œå‡ºã•ã‚ŒãŸå•é¡Œãƒ•ã‚¡ã‚¤ãƒ«: {len(problem_files)}ä»¶")

    # æœªå‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ•ã‚£ãƒ«ã‚¿
    remaining_files = [(f, s) for f, s in problem_files if f not in completed_files]
    print(f"[INFO] æœªå‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«: {len(remaining_files)}ä»¶")

    if not remaining_files:
        print("[INFO] ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒå‡¦ç†æ¸ˆã¿ã§ã™")
        return

    # ãƒãƒƒãƒä½œæˆ
    batches = [remaining_files[i:i+BATCH_SIZE] for i in range(0, len(remaining_files), BATCH_SIZE)]
    print(f"[INFO] {len(batches)}å€‹ã®ãƒãƒƒãƒã‚’ä½œæˆï¼ˆ{BATCH_SIZE}ãƒ•ã‚¡ã‚¤ãƒ«/ãƒãƒƒãƒï¼‰")

    # å‡¦ç†é–‹å§‹
    start_time = time.time()
    all_results = []

    for batch_idx, batch_files in enumerate(batches, start=last_batch+1):
        batch_start = time.time()
        print(f"\n[BATCH {batch_idx}] {len(batch_files)}ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸¦åˆ—å‡¦ç†é–‹å§‹")

        # ä¸¦åˆ—å‡¦ç†
        with ThreadPoolExecutor(max_workers=PARALLEL_WORKERS) as executor:
            futures = []
            for file_data in batch_files:
                future = executor.submit(process_file, file_data)
                futures.append(future)

            # é€²æ—è¡¨ç¤ºä»˜ãçµæœåé›†
            completed = 0
            for future in as_completed(futures):
                try:
                    result = future.result(timeout=300)
                    all_results.append(result)
                    completed += 1

                    # é€²æ—æ›´æ–°
                    if result['success']:
                        completed_files[result['file_path']] = True

                    # 10ä»¶ã”ã¨ã«é€²æ—è¡¨ç¤º
                    if completed % 10 == 0:
                        print(f"  é€²æ—: {completed}/{len(batch_files)}")

                except Exception as e:
                    print(f"  [ERROR] å‡¦ç†å¤±æ•—: {e}")

        # ãƒãƒƒãƒãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        batch_report = f"{OUTPUT_DIR}/AI_batch{batch_idx:03d}_parallel_enhanced.md"
        save_batch_report_enhanced(batch_report, all_results[-len(batch_files):], batch_idx)

        # é€²æ—ä¿å­˜
        progress['completed_files'] = completed_files
        progress['last_batch'] = batch_idx
        with open(PROGRESS_FILE, 'w', encoding='utf-8') as f:
            json.dump(progress, f, ensure_ascii=False, indent=2)

        batch_time = time.time() - batch_start
        print(f"[BATCH {batch_idx}] å®Œäº† - å‡¦ç†æ™‚é–“: {batch_time:.1f}ç§’")

    # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    total_time = time.time() - start_time
    generate_final_report_enhanced(all_results, total_time)

    print("\n" + "="*60)
    print("å‡¦ç†å®Œäº†")
    print(f"ç·å‡¦ç†æ™‚é–“: {total_time/60:.1f}åˆ†")
    print("="*60)

def save_batch_report_enhanced(file_path: str, results: List[Dict], batch_num: int):
    """æ”¹è‰¯ç‰ˆãƒãƒƒãƒãƒ¬ãƒãƒ¼ãƒˆä¿å­˜"""
    with open(file_path, 'w', encoding='utf-8') as f:
        f.write(f"# AIè©³ç´°åˆ†æãƒ¬ãƒãƒ¼ãƒˆï¼ˆæ”¹è‰¯ç‰ˆï¼‰ - ãƒãƒƒãƒ{batch_num:03d}\n\n")
        f.write(f"ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"åˆ†æãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(results)}\n\n")

        # ã‚µãƒãƒªãƒ¼
        danger_levels = {"ç·Šæ€¥": 0, "é«˜": 0, "ä¸­": 0}
        for r in results:
            if r['severity'] >= 15:
                danger_levels["ç·Šæ€¥"] += 1
            elif r['severity'] >= 10:
                danger_levels["é«˜"] += 1
            else:
                danger_levels["ä¸­"] += 1

        f.write("## ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼\n\n")
        for level, count in danger_levels.items():
            if count > 0:
                f.write(f"- [{level}] {level}å¯¾å¿œå¿…è¦: {count}ä»¶\n")

        f.write("\n---\n\n")

        # è©³ç´°çµæœ
        for result in results:
            f.write(result['analysis'])
            f.write("\n")

def generate_final_report_enhanced(all_results: List[Dict], total_time: float):
    """æ”¹è‰¯ç‰ˆæœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
    final_report = f"{OUTPUT_DIR}/AI_analysis_parallel_enhanced_complete.md"

    with open(final_report, 'w', encoding='utf-8') as f:
        f.write("# ğŸš¨ å®Œå…¨ã‚³ãƒ¼ãƒ‰åˆ†æãƒ¬ãƒãƒ¼ãƒˆ - ä¸¦åˆ—å‡¦ç†æ”¹è‰¯ç‰ˆ\n\n")
        f.write(f"ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"ç·å‡¦ç†æ™‚é–“: {total_time/60:.1f}åˆ†\n")
        f.write(f"ç·åˆ†æãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(all_results)}\n\n")

        f.write("## ğŸ“Š ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã®ç‰¹å¾´\n\n")
        f.write("### âœ¨ æ–°æ©Ÿèƒ½\n")
        f.write("- **å®Œå…¨ãªæ”¹å–„ã‚³ãƒ¼ãƒ‰**: å®Ÿè£…å¯èƒ½ãªå®Œå…¨ç‰ˆã‚³ãƒ¼ãƒ‰æä¾›\n")
        f.write("- **è©³ç´°ãªå½±éŸ¿åˆ†æ**: ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã€ä¿å®ˆæ€§ã¸ã®å½±éŸ¿ã‚’è©³è¿°\n")
        f.write("- **ä¸¦åˆ—å‡¦ç†**: 10å€é«˜é€ŸåŒ–ã«ã‚ˆã‚‹åŠ¹ç‡çš„ãªåˆ†æ\n")
        f.write("- **ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½**: é‡è¤‡åˆ†æã®å‰Šæ¸›\n\n")

        f.write("---\n\n")

        # å…¨çµæœã‚’çµ±åˆ
        for result in all_results:
            if result['success']:
                f.write(result['analysis'])
                f.write("\n")

if __name__ == "__main__":
    main()