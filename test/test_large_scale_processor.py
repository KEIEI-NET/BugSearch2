"""
Phase 7ãƒ†ã‚¹ãƒˆ: å¤§è¦æ¨¡ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«è§£æã‚·ã‚¹ãƒ†ãƒ ï¼ˆ30,000+ãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œï¼‰

@tddå“è³ªä¿è¨¼:
- ãƒ¡ãƒ¢ãƒªãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°
- ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ©Ÿèƒ½
- ä¸­æ–­ãƒ»å†é–‹æ©Ÿèƒ½
- å¤§è¦æ¨¡ãƒãƒƒãƒå‡¦ç†

ãƒãƒ¼ã‚¸ãƒ§ãƒ³: v4.8.0 (Phase 7.0)
ä½œæˆæ—¥: 2025å¹´10æœˆ12æ—¥ JST
"""

import unittest
from pathlib import Path
import json
import tempfile
import shutil
import sys
from datetime import datetime
import time
import psutil
import os

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, str(Path(__file__).parent.parent))

from core.memory_monitor import MemoryMonitor, MemoryStatus
from core.checkpoint_manager import CheckpointManager, Checkpoint
from core.large_scale_processor import LargeScaleProcessor, ProcessingConfig


class TestMemoryMonitor(unittest.TestCase):
    """ãƒ¡ãƒ¢ãƒªãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã®ãƒ†ã‚¹ãƒˆ"""

    def setUp(self):
        """ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        self.monitor = MemoryMonitor(
            warning_threshold_mb=100,  # è­¦å‘Š: 100MB
            critical_threshold_mb=500,  # å±é™º: 500MB
            check_interval_seconds=0.1
        )

    def test_memory_status_enum(self):
        """ãƒ¡ãƒ¢ãƒªã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹Enumã®ãƒ†ã‚¹ãƒˆ"""
        self.assertEqual(MemoryStatus.NORMAL.value, "normal")
        self.assertEqual(MemoryStatus.WARNING.value, "warning")
        self.assertEqual(MemoryStatus.CRITICAL.value, "critical")
        print("âœ… ãƒ¡ãƒ¢ãƒªã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹Enumæ­£å¸¸")

    def test_get_current_memory_usage(self):
        """ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å–å¾—ãƒ†ã‚¹ãƒˆ"""
        usage_mb = self.monitor.get_current_memory_usage()

        self.assertIsInstance(usage_mb, float)
        self.assertGreater(usage_mb, 0)
        print(f"âœ… ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å–å¾—æˆåŠŸ: {usage_mb:.2f} MB")

    def test_check_memory_status(self):
        """ãƒ¡ãƒ¢ãƒªã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒã‚§ãƒƒã‚¯ãƒ†ã‚¹ãƒˆ"""
        status = self.monitor.check_memory_status()

        self.assertIsInstance(status, MemoryStatus)
        self.assertIn(status, [MemoryStatus.NORMAL, MemoryStatus.WARNING, MemoryStatus.CRITICAL])
        print(f"âœ… ãƒ¡ãƒ¢ãƒªã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒã‚§ãƒƒã‚¯æˆåŠŸ: {status.value}")

    def test_should_pause_processing(self):
        """å‡¦ç†ä¸€æ™‚åœæ­¢åˆ¤å®šãƒ†ã‚¹ãƒˆ"""
        should_pause = self.monitor.should_pause_processing()

        self.assertIsInstance(should_pause, bool)
        print(f"âœ… å‡¦ç†ä¸€æ™‚åœæ­¢åˆ¤å®š: {should_pause}")

    def test_get_memory_statistics(self):
        """ãƒ¡ãƒ¢ãƒªçµ±è¨ˆæƒ…å ±å–å¾—ãƒ†ã‚¹ãƒˆ"""
        stats = self.monitor.get_memory_statistics()

        self.assertIn('current_mb', stats)
        self.assertIn('available_mb', stats)
        self.assertIn('percent_used', stats)
        self.assertIn('status', stats)

        print(f"âœ… ãƒ¡ãƒ¢ãƒªçµ±è¨ˆå–å¾—æˆåŠŸ:")
        print(f"   ç¾åœ¨: {stats['current_mb']:.2f} MB")
        print(f"   åˆ©ç”¨å¯èƒ½: {stats['available_mb']:.2f} MB")
        print(f"   ä½¿ç”¨ç‡: {stats['percent_used']:.1f}%")


class TestCheckpointManager(unittest.TestCase):
    """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆç®¡ç†ã®ãƒ†ã‚¹ãƒˆ"""

    def setUp(self):
        """ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        self.test_dir = Path(tempfile.mkdtemp())
        self.checkpoint_file = self.test_dir / "checkpoint.json"
        self.manager = CheckpointManager(self.checkpoint_file)

    def tearDown(self):
        """ãƒ†ã‚¹ãƒˆã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if self.test_dir.exists():
            shutil.rmtree(self.test_dir)

    def test_checkpoint_creation(self):
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä½œæˆãƒ†ã‚¹ãƒˆ"""
        checkpoint = self.manager.create_checkpoint(
            processed_files=['file1.py', 'file2.py'],
            total_files=100,
            current_batch=1,
            total_batches=10,
            metadata={'test': 'value'}
        )

        self.assertIsInstance(checkpoint, Checkpoint)
        self.assertEqual(len(checkpoint.processed_files), 2)
        self.assertEqual(checkpoint.total_files, 100)
        self.assertEqual(checkpoint.current_batch, 1)
        print("âœ… ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä½œæˆæˆåŠŸ")

    def test_checkpoint_save_load(self):
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ãƒ»èª­è¾¼ãƒ†ã‚¹ãƒˆ"""
        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä½œæˆ
        original = self.manager.create_checkpoint(
            processed_files=['file1.py', 'file2.py', 'file3.py'],
            total_files=100,
            current_batch=2,
            total_batches=10
        )

        # ä¿å­˜
        self.manager.save_checkpoint(original)
        self.assertTrue(self.checkpoint_file.exists())

        # èª­è¾¼
        loaded = self.manager.load_checkpoint()
        self.assertIsNotNone(loaded)
        self.assertEqual(len(loaded.processed_files), 3)
        self.assertEqual(loaded.current_batch, 2)

        print("âœ… ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ãƒ»èª­è¾¼æˆåŠŸ")

    def test_checkpoint_resume_detection(self):
        """å†é–‹å¯èƒ½æ€§æ¤œå‡ºãƒ†ã‚¹ãƒˆ"""
        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãªã—
        self.assertFalse(self.manager.can_resume())

        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä½œæˆ
        checkpoint = self.manager.create_checkpoint(
            processed_files=['file1.py'],
            total_files=100,
            current_batch=1,
            total_batches=10
        )
        self.manager.save_checkpoint(checkpoint)

        # å†é–‹å¯èƒ½
        self.assertTrue(self.manager.can_resume())

        print("âœ… å†é–‹å¯èƒ½æ€§æ¤œå‡ºæˆåŠŸ")

    def test_checkpoint_deletion(self):
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆå‰Šé™¤ãƒ†ã‚¹ãƒˆ"""
        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä½œæˆãƒ»ä¿å­˜
        checkpoint = self.manager.create_checkpoint(
            processed_files=['file1.py'],
            total_files=100,
            current_batch=1,
            total_batches=10
        )
        self.manager.save_checkpoint(checkpoint)
        self.assertTrue(self.checkpoint_file.exists())

        # å‰Šé™¤
        self.manager.delete_checkpoint()
        self.assertFalse(self.checkpoint_file.exists())

        print("âœ… ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆå‰Šé™¤æˆåŠŸ")

    def test_checkpoint_progress_percentage(self):
        """é€²æ—ç‡è¨ˆç®—ãƒ†ã‚¹ãƒˆ"""
        checkpoint = self.manager.create_checkpoint(
            processed_files=['file1.py', 'file2.py'],
            total_files=10,
            current_batch=2,
            total_batches=5
        )

        progress = checkpoint.get_progress_percentage()
        self.assertEqual(progress, 20.0)  # 2/10 = 20%

        print(f"âœ… é€²æ—ç‡è¨ˆç®—æˆåŠŸ: {progress}%")


class TestLargeScaleProcessor(unittest.TestCase):
    """å¤§è¦æ¨¡å‡¦ç†ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã®ãƒ†ã‚¹ãƒˆ"""

    def setUp(self):
        """ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        self.test_dir = Path(tempfile.mkdtemp())
        self.checkpoint_file = self.test_dir / "checkpoint.json"

        # ãƒ†ã‚¹ãƒˆç”¨ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆï¼ˆ100ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
        self.test_files = []
        for i in range(100):
            test_file = self.test_dir / f"test_{i}.py"
            test_file.write_text(f"# Test file {i}\nprint('Hello {i}')\n", encoding='utf-8')
            self.test_files.append(test_file)

        # è¨­å®š
        self.config = ProcessingConfig(
            batch_size=10,
            checkpoint_interval=5,  # 5ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
            memory_check_interval=2,
            max_memory_mb=500,
            enable_auto_gc=True
        )

        self.processor = LargeScaleProcessor(
            config=self.config,
            checkpoint_file=self.checkpoint_file
        )

    def tearDown(self):
        """ãƒ†ã‚¹ãƒˆã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if self.test_dir.exists():
            shutil.rmtree(self.test_dir)

    def test_batch_division(self):
        """ãƒãƒƒãƒåˆ†å‰²ãƒ†ã‚¹ãƒˆ"""
        batches = self.processor.divide_into_batches(self.test_files)

        self.assertEqual(len(batches), 10)  # 100ãƒ•ã‚¡ã‚¤ãƒ« / 10 = 10ãƒãƒƒãƒ
        self.assertEqual(len(batches[0]), 10)  # å„ãƒãƒƒãƒ10ãƒ•ã‚¡ã‚¤ãƒ«

        print(f"âœ… ãƒãƒƒãƒåˆ†å‰²æˆåŠŸ: {len(batches)}ãƒãƒƒãƒ")

    def test_simple_processing(self):
        """ã‚·ãƒ³ãƒ—ãƒ«å‡¦ç†ãƒ†ã‚¹ãƒˆ"""

        def simple_processor(file_path: Path) -> dict:
            """ãƒ†ã‚¹ãƒˆç”¨ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼"""
            return {
                'file': str(file_path),
                'size': file_path.stat().st_size,
                'success': True
            }

        # æœ€åˆã®10ãƒ•ã‚¡ã‚¤ãƒ«ã ã‘å‡¦ç†
        results = self.processor.process_files(
            files=self.test_files[:10],
            processor_func=simple_processor
        )

        self.assertEqual(len(results), 10)
        self.assertTrue(all(r['success'] for r in results))

        print("âœ… ã‚·ãƒ³ãƒ—ãƒ«å‡¦ç†ãƒ†ã‚¹ãƒˆæˆåŠŸ")

    def test_checkpoint_creation_during_processing(self):
        """å‡¦ç†ä¸­ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä½œæˆãƒ†ã‚¹ãƒˆ"""

        processed_count = [0]  # ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼

        def counting_processor(file_path: Path) -> dict:
            processed_count[0] += 1
            return {'file': str(file_path), 'success': True}

        # 20ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ï¼ˆãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆé–“éš”5ãªã®ã§ã€4å›ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä½œæˆï¼‰
        results = self.processor.process_files(
            files=self.test_files[:20],
            processor_func=counting_processor
        )

        self.assertEqual(len(results), 20)
        self.assertEqual(processed_count[0], 20)

        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆå­˜åœ¨ç¢ºèª
        self.assertTrue(self.checkpoint_file.exists())

        print("âœ… å‡¦ç†ä¸­ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä½œæˆæˆåŠŸ")

    def test_resume_from_checkpoint(self):
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ã®å†é–‹ãƒ†ã‚¹ãƒˆ"""

        # 1å›ç›®: å…¨100ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†é–‹å§‹ã™ã‚‹ãŒã€10ãƒ•ã‚¡ã‚¤ãƒ«ã§ä¸­æ–­ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        # ã‚¨ãƒ©ãƒ¼æ™‚ã«åœæ­¢ã™ã‚‹è¨­å®šã§ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã‚’ä½œæˆ
        interrupt_config = ProcessingConfig(
            batch_size=10,
            checkpoint_interval=5,
            memory_check_interval=2,
            max_memory_mb=500,
            enable_auto_gc=True,
            stop_on_error=True  # ã‚¨ãƒ©ãƒ¼æ™‚ã«åœæ­¢
        )

        interrupt_processor = LargeScaleProcessor(
            config=interrupt_config,
            checkpoint_file=self.checkpoint_file
        )

        processed_count = [0]
        interrupt_at = 10

        class InterruptException(Exception):
            """ä¸­æ–­ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã™ã‚‹ä¾‹å¤–"""
            pass

        def processor_v1(file_path: Path) -> dict:
            processed_count[0] += 1
            if processed_count[0] >= interrupt_at:
                # 10ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å¾Œã«ä¸­æ–­
                raise InterruptException("Simulated interruption")
            return {'file': str(file_path), 'version': 1}

        # ä¸­æ–­ã‚’ã‚­ãƒ£ãƒƒãƒ
        try:
            results_v1 = interrupt_processor.process_files(
                files=self.test_files,  # å…¨100ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®š
                processor_func=processor_v1
            )
        except InterruptException:
            pass  # æœŸå¾…ã•ã‚Œã‚‹ä¸­æ–­

        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆç¢ºèªï¼ˆ9ãƒ•ã‚¡ã‚¤ãƒ«ã¾ã§å‡¦ç†æ¸ˆã¿ã€10ç•ªç›®ã§ä¸­æ–­ï¼‰
        checkpoint = interrupt_processor.checkpoint_manager.load_checkpoint()
        self.assertIsNotNone(checkpoint)
        self.assertGreater(len(checkpoint.processed_files), 0)
        processed_in_v1 = len(checkpoint.processed_files)

        # 2å›ç›®: æ–°ã—ã„ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã§æ®‹ã‚Šå‡¦ç†
        processor_v2 = LargeScaleProcessor(
            config=self.config,
            checkpoint_file=self.checkpoint_file
        )

        def processor_func_v2(file_path: Path) -> dict:
            return {'file': str(file_path), 'version': 2}

        # å†é–‹å‡¦ç†ï¼ˆå…¨100ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®šã™ã‚‹ãŒã€å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã‚‹ï¼‰
        results_v2 = processor_v2.process_files(
            files=self.test_files,  # åŒã˜100ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®š
            processor_func=processor_func_v2,
            resume=True
        )

        # æ®‹ã‚Šãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å‡¦ç†ã•ã‚Œã‚‹ã¯ãš
        expected_remaining = 100 - processed_in_v1
        self.assertEqual(len(results_v2), expected_remaining)

        print(f"âœ… ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆå†é–‹æˆåŠŸï¼ˆv1: {processed_in_v1}ãƒ•ã‚¡ã‚¤ãƒ«, v2: {len(results_v2)}ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰")

    def test_memory_monitoring_during_processing(self):
        """å‡¦ç†ä¸­ãƒ¡ãƒ¢ãƒªãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ"""

        memory_checks = []

        def monitored_processor(file_path: Path) -> dict:
            # ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹è¨˜éŒ²
            status = self.processor.memory_monitor.check_memory_status()
            memory_checks.append(status)
            return {'file': str(file_path), 'success': True}

        results = self.processor.process_files(
            files=self.test_files[:20],
            processor_func=monitored_processor
        )

        self.assertEqual(len(results), 20)
        self.assertGreater(len(memory_checks), 0)

        print(f"âœ… ãƒ¡ãƒ¢ãƒªãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°æˆåŠŸ: {len(memory_checks)}å›ãƒã‚§ãƒƒã‚¯")

    def test_auto_pause_on_high_memory(self):
        """é«˜ãƒ¡ãƒ¢ãƒªä½¿ç”¨æ™‚ã®è‡ªå‹•ä¸€æ™‚åœæ­¢ãƒ†ã‚¹ãƒˆ"""

        # æ¥µç«¯ã«ä½ã„ãƒ¡ãƒ¢ãƒªé–¾å€¤è¨­å®šï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
        low_memory_config = ProcessingConfig(
            batch_size=10,
            checkpoint_interval=5,
            memory_check_interval=1,
            max_memory_mb=1,  # 1MBã§å±é™ºåˆ¤å®šï¼ˆå¿…ãšè¶…ãˆã‚‹ï¼‰
            enable_auto_gc=True
        )

        processor = LargeScaleProcessor(
            config=low_memory_config,
            checkpoint_file=self.test_dir / "test_checkpoint.json"
        )

        pause_detected = [False]

        def pause_detecting_processor(file_path: Path) -> dict:
            if processor.memory_monitor.should_pause_processing():
                pause_detected[0] = True
            return {'file': str(file_path), 'success': True}

        # å‡¦ç†å®Ÿè¡Œï¼ˆãƒ¡ãƒ¢ãƒªãƒã‚§ãƒƒã‚¯ã§ä¸€æ™‚åœæ­¢ãŒç™ºç”Ÿã™ã‚‹ã¯ãšï¼‰
        results = processor.process_files(
            files=self.test_files[:10],
            processor_func=pause_detecting_processor
        )

        # ä¸€æ™‚åœæ­¢ãŒæ¤œå‡ºã•ã‚ŒãŸã“ã¨ã‚’ç¢ºèª
        # ï¼ˆå®Ÿéš›ã«ã¯å‡¦ç†ã¯ç¶™ç¶šã™ã‚‹ãŒã€ãƒ•ãƒ©ã‚°ãŒç«‹ã¤ã“ã¨ã‚’ç¢ºèªï¼‰
        self.assertTrue(pause_detected[0])

        print("âœ… é«˜ãƒ¡ãƒ¢ãƒªä½¿ç”¨æ™‚ä¸€æ™‚åœæ­¢æ¤œå‡ºæˆåŠŸ")


class TestIntegration(unittest.TestCase):
    """çµ±åˆãƒ†ã‚¹ãƒˆ"""

    def setUp(self):
        """ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        self.test_dir = Path(tempfile.mkdtemp())

    def tearDown(self):
        """ãƒ†ã‚¹ãƒˆã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if self.test_dir.exists():
            shutil.rmtree(self.test_dir)

    def test_end_to_end_processing(self):
        """ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰å‡¦ç†ãƒ†ã‚¹ãƒˆ"""

        # 1000ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
        test_files = []
        for i in range(1000):
            test_file = self.test_dir / f"file_{i}.py"
            test_file.write_text(f"# File {i}\n", encoding='utf-8')
            test_files.append(test_file)

        # ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼è¨­å®š
        config = ProcessingConfig(
            batch_size=50,
            checkpoint_interval=100,
            memory_check_interval=10,
            max_memory_mb=1000,
            enable_auto_gc=True
        )

        checkpoint_file = self.test_dir / "checkpoint.json"
        processor = LargeScaleProcessor(config, checkpoint_file)

        # å‡¦ç†å®Ÿè¡Œ
        def test_processor(file_path: Path) -> dict:
            return {
                'file': str(file_path.name),
                'size': file_path.stat().st_size,
                'timestamp': datetime.now().isoformat()
            }

        start_time = time.time()
        results = processor.process_files(test_files, test_processor)
        elapsed = time.time() - start_time

        # æ¤œè¨¼
        self.assertEqual(len(results), 1000)
        self.assertTrue(all('file' in r for r in results))

        # çµ±è¨ˆå‡ºåŠ›
        print(f"âœ… ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰å‡¦ç†æˆåŠŸ:")
        print(f"   ãƒ•ã‚¡ã‚¤ãƒ«æ•°: 1,000")
        print(f"   å‡¦ç†æ™‚é–“: {elapsed:.2f}ç§’")
        print(f"   ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {len(results)/elapsed:.1f} files/sec")


def run_tests():
    """ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã‚’å®Ÿè¡Œ"""
    suite = unittest.TestSuite()

    # ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹è¿½åŠ 
    suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestMemoryMonitor))
    suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestCheckpointManager))
    suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestLargeScaleProcessor))
    suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestIntegration))

    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(suite)

    # çµæœã‚µãƒãƒªãƒ¼
    print("\n" + "=" * 80)
    print("ğŸ“Š Phase 7ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼")
    print("=" * 80)
    print(f"å®Ÿè¡Œã—ãŸãƒ†ã‚¹ãƒˆ: {result.testsRun}")
    print(f"æˆåŠŸ: {result.testsRun - len(result.failures) - len(result.errors)}")
    print(f"å¤±æ•—: {len(result.failures)}")
    print(f"ã‚¨ãƒ©ãƒ¼: {len(result.errors)}")
    print(f"ã‚¹ã‚­ãƒƒãƒ—: {len(result.skipped)}")

    if result.wasSuccessful():
        try:
            print("\nâœ… å…¨ã¦ã®ãƒ†ã‚¹ãƒˆãŒåˆæ ¼ã—ã¾ã—ãŸï¼ (@tddå“è³ªé”æˆ)")
        except UnicodeEncodeError:
            print("\n[OK] å…¨ã¦ã®ãƒ†ã‚¹ãƒˆãŒåˆæ ¼ã—ã¾ã—ãŸï¼ (@tddå“è³ªé”æˆ)")
        return 0
    else:
        print("\nâŒ ãƒ†ã‚¹ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ")
        if result.failures:
            print("\nå¤±æ•—ã—ãŸãƒ†ã‚¹ãƒˆ:")
            for test, traceback in result.failures:
                print(f"  - {test}")
        if result.errors:
            print("\nã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸãƒ†ã‚¹ãƒˆ:")
            for test, traceback in result.errors:
                print(f"  - {test}")
        return 1


if __name__ == "__main__":
    sys.exit(run_tests())
