#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
codex_review_enhanced.py â€” ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è‡ªå‹•åˆ¤å®šæ©Ÿèƒ½ä»˜ãç‰ˆ
æ—¥æœ¬èªã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã®æ–‡å­—åŒ–ã‘ã‚’é˜²ãã€æ­£ç¢ºãªåˆ†æã‚’å®Ÿç¾

ä¸»ãªæ”¹å–„ç‚¹:
  1) ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿æ™‚ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è‡ªå‹•åˆ¤å®š
  2) æ—¥æœ¬èªã‚³ãƒ¡ãƒ³ãƒˆã‚’æ­£ã—ãè§£æ
  3) ãƒ¬ãƒãƒ¼ãƒˆã¯UTF-8ã§çµ±ä¸€å‡ºåŠ›
  4) AIåˆ†æã‚‚æ–‡å­—åŒ–ã‘ãªãå®Ÿæ–½

pip install chromadb openai scikit-learn joblib regex chardet
"""
from __future__ import annotations
import argparse, hashlib, json, os, pathlib, re, sys, time
from dataclasses import dataclass, asdict
from typing import Any, Dict, List, Tuple, Optional
import chardet  # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ¤œå‡ºç”¨

# ===== Config =====
IGNORE_DIRS = {".git","node_modules","dist","build","out","bin","obj",".idea",".vscode",".next","coverage","target"}
DEFAULT_MAX_FILE_BYTES = 4_000_000  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ4MB
ENV_FILE = ".env"
INDEX_PATH = ".advice_index.jsonl"
VEC_PATH = ".advice_tfidf.pkl"
MATRIX_PATH = ".advice_matrix.pkl"
BATCH_SIZE = 5
MAX_PROMPT_SIZE = 8000
AI_TIMEOUT = 240

# é‡è¦åº¦ã‚¹ã‚³ã‚¢å®šç¾©
SEVERITY_SCORES = {
    "DB: ãƒ«ãƒ¼ãƒ—å†…SELECT (N+1) ç–‘ã„": 10,
    "DB: SELECT * â†’è² è·å¢—ã€‚åˆ—é™å®š": 8,
    "DB: å¤šé‡JOINâ†’é…å»¶ã®æã‚Œã€‚è¨ˆç”»/IDXç¢ºèª": 7,
    "DB: å¤§OFFSETâ†’é…å»¶ã€‚IDã‚«ãƒ¼ã‚½ãƒ«æ¨å¥¨": 6,
    "é‡‘é¡: æµ®å‹•å°æ•°ã§é‡‘é¡â†’èª¤å·®ã€‚Decimal/é€šè²¨å‹ã¸": 9,
    "é‡‘é¡: ç¨è¾¼/ç¨æŠœãƒ»ç«¯æ•°å‡¦ç†ã®æ··åœ¨æ³¨æ„": 7,
    "UI: XSSè„†å¼±æ€§ã®ç–‘ã„ï¼ˆæœªã‚µãƒ‹ã‚¿ã‚¤ã‚ºHTMLï¼‰": 8,
    "UI: å…¥åŠ›æ¤œè¨¼ãŒä¸ååˆ†": 5,
    "UI: å¤šé‡ã‚¯ãƒªãƒƒã‚¯é˜²æ­¢ãªã—": 3,
    "å°åˆ·: ã‚¨ãƒ©ãƒ¼å‡¦ç†ãªã—â†’é€”ä¸­åœæ­¢ãƒªã‚¹ã‚¯": 4,
    "å°åˆ·: æ”¹ãƒšãƒ¼ã‚¸åˆ¤å®šãŒä¸ååˆ†": 2,
}

# ===== Optional deps =====
SKLEARN_OK = False
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    import joblib
    SKLEARN_OK = True
except Exception:
    pass

OPENAI_OK = False
try:
    from openai import OpenAI
    OPENAI_OK = True
except Exception:
    pass

# ===== Utils =====
if ENV_FILE and pathlib.Path(ENV_FILE).exists():
    with open(ENV_FILE, "r", encoding="utf-8") as f:
        for line in f:
            if "=" in line: key, val = line.strip().split("=", 1); os.environ[key] = val

@dataclass
class Doc:
    path: str
    lang: str
    size: int
    sha1: str
    tags: List[str]
    summary: str
    text: str
    encoding: str  # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æƒ…å ±ã‚’è¿½åŠ 

def detect_encoding(file_path: pathlib.Path) -> str:
    """ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è‡ªå‹•æ¤œå‡º"""
    try:
        with open(file_path, 'rb') as f:
            raw_data = f.read(10000)  # æœ€åˆã®10KBã§åˆ¤å®š
            result = chardet.detect(raw_data)
            encoding = result['encoding']
            confidence = result.get('confidence', 0)

            # ä¿¡é ¼åº¦ãŒä½ã„å ´åˆã‚„Noneã®å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if not encoding or confidence < 0.7:
                # æ—¥æœ¬èªç’°å¢ƒã®ä¸€èˆ¬çš„ãªã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è©¦ã™
                for enc in ['utf-8', 'shift_jis', 'cp932', 'euc-jp', 'iso-2022-jp']:
                    try:
                        with open(file_path, 'r', encoding=enc) as test_f:
                            test_f.read(1000)
                        return enc
                    except:
                        continue
                return 'utf-8'  # æœ€çµ‚çš„ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

            # cp932ã¨shift_jisã¯äº’æ›æ€§ãŒã‚ã‚‹ãŸã‚çµ±ä¸€
            if encoding.lower() in ['cp932', 'shift_jis', 'shift-jis', 'sjis']:
                return 'cp932'

            return encoding.lower()
    except Exception as e:
        print(f"[WARNING] Encoding detection failed for {file_path}: {e}")
        return 'utf-8'

def read_file_with_encoding(file_path: pathlib.Path) -> Tuple[str, str]:
    """ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è‡ªå‹•æ¤œå‡ºã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
    encoding = detect_encoding(file_path)

    try:
        with open(file_path, 'r', encoding=encoding, errors='ignore') as f:
            content = f.read()
        return content, encoding
    except Exception as e:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒã‚¤ãƒŠãƒªãƒ¢ãƒ¼ãƒ‰ã§èª­ã¿è¾¼ã‚“ã§ãƒ‡ã‚³ãƒ¼ãƒ‰
        try:
            with open(file_path, 'rb') as f:
                raw_data = f.read()
            content = raw_data.decode(encoding, errors='ignore')
            return content, encoding
        except:
            # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return "", "unknown"

def sha1_bytes(data: bytes) -> str:
    return hashlib.sha1(data).hexdigest()[:10]

def detect_lang(p: pathlib.Path) -> str:
    ext = p.suffix.lower()
    if ext in (".py",): return "python"
    if ext in (".js",".mjs",".cjs",".jsx"): return "javascript"
    if ext in (".ts",".tsx"): return "typescript"
    if ext in (".cs",".csx"): return "csharp"
    if ext in (".java",): return "java"
    if ext in (".pas",".dfm",".dpr"): return "delphi"
    if ext in (".rb",): return "ruby"
    if ext in (".go",): return "go"
    if ext in (".rs",): return "rust"
    if ext in (".php",): return "php"
    if ext in (".c",".h"): return "c"
    if ext in (".cpp",".hpp",".cc",".hh",".cxx",".hxx"): return "cpp"
    return "other"

def make_summary(text: str) -> str:
    lines = text.splitlines()[:10]
    return " / ".join(l.strip()[:80] for l in lines if l.strip())[:400]

# ===== Rule engine =====
def scan_money(text: str) -> List[str]:
    m: List[str] = []
    if re.search(r"\b(float|double|real)\b.*\b(price|cost|money|amount|tax|total|sum|charge|fee|pay)", text, re.IGNORECASE):
        m.append("é‡‘é¡: æµ®å‹•å°æ•°ã§é‡‘é¡â†’èª¤å·®ã€‚Decimal/é€šè²¨å‹ã¸")
    if re.search(r"\b(total|sum|amount)\s*=.*\+.*tax", text, re.IGNORECASE) and re.search(r"(Math\.(floor|round)|parseInt|truncate)", text, re.IGNORECASE):
        m.append("é‡‘é¡: ç¨è¾¼/ç¨æŠœãƒ»ç«¯æ•°å‡¦ç†ã®æ··åœ¨æ³¨æ„")
    return m

def scan_print(text: str) -> List[str]:
    m: List[str] = []
    if re.search(r"(window\.print|PrintDocument|Printer|PrintPage|HasMorePages|NewPage|PrintPreview)", text, re.IGNORECASE) and not re.search(r"(try|catch|error|exception)", text, re.IGNORECASE):
        m.append("å°åˆ·: ã‚¨ãƒ©ãƒ¼å‡¦ç†ãªã—â†’é€”ä¸­åœæ­¢ãƒªã‚¹ã‚¯")
    if re.search(r"\.HasMorePages\s*=\s*true", text, re.IGNORECASE) and not re.search(r"(yPos|currentY|pageHeight|margin)", text, re.IGNORECASE):
        m.append("å°åˆ·: æ”¹ãƒšãƒ¼ã‚¸åˆ¤å®šãŒä¸ååˆ†")
    return m

def scan_ui(text: str) -> List[str]:
    m: List[str] = []
    if re.search(r"(button|submit|click|tap|press)", text, re.IGNORECASE) and not re.search(r"(disabled|loading|spinner|prevent|debounce|throttle)", text, re.IGNORECASE):
        m.append("UI: å¤šé‡ã‚¯ãƒªãƒƒã‚¯é˜²æ­¢ãªã—")
    if re.search(r"(innerHTML\s*=|dangerouslySetInnerHTML|v-html)", text) and not re.search(r"(sanitize|escape|DOMPurify|textContent)", text, re.IGNORECASE):
        m.append("UI: XSSè„†å¼±æ€§ã®ç–‘ã„ï¼ˆæœªã‚µãƒ‹ã‚¿ã‚¤ã‚ºHTMLï¼‰")
    if re.search(r"(input|textarea|select)", text, re.IGNORECASE) and not re.search(r"(validate|pattern|required|maxlength|regex|test|match)", text, re.IGNORECASE):
        m.append("UI: å…¥åŠ›æ¤œè¨¼ãŒä¸ååˆ†")
    return m

def scan_db(text: str) -> List[str]:
    m: List[str] = []
    if re.search(r"SELECT\s+\*\s+FROM", text, re.IGNORECASE):
        m.append("DB: SELECT * â†’è² è·å¢—ã€‚åˆ—é™å®š")
    if re.search(r"(for|while|foreach|map).*\n.*\n.*\n.*(SELECT|INSERT|UPDATE|DELETE)", text, re.IGNORECASE):
        m.append("DB: ãƒ«ãƒ¼ãƒ—å†…SELECT (N+1) ç–‘ã„")
    if re.search(r"(JOIN.*){3,}", text, re.IGNORECASE):
        m.append("DB: å¤šé‡JOINâ†’é…å»¶ã®æã‚Œã€‚è¨ˆç”»/IDXç¢ºèª")
    if re.search(r"(OFFSET|LIMIT)\s+\d{4,}", text, re.IGNORECASE):
        m.append("DB: å¤§OFFSETâ†’é…å»¶ã€‚IDã‚«ãƒ¼ã‚½ãƒ«æ¨å¥¨")
    return m

def make_tags(text: str) -> List[str]:
    t = []
    if re.search(r"\b(price|cost|money|amount|tax|total|sum|charge|fee|pay|invoice|billing|currency|decimal|round)", text, re.IGNORECASE): t.append("money")
    if re.search(r"\b(print|printer|page|paper|report|pdf|export|PrintDocument|PrintPage|HasMorePages)", text, re.IGNORECASE): t.append("print")
    if re.search(r"\b(button|click|submit|form|input|select|modal|dialog|toast|spinner|loading|disabled)", text, re.IGNORECASE): t.append("uiux")
    if re.search(r"\b(SELECT|INSERT|UPDATE|DELETE|JOIN|WHERE|FROM|query|sql|database|transaction|connection)", text, re.IGNORECASE): t.append("db")
    if re.search(r"\b(api|http|fetch|axios|request|response|endpoint|REST|webhook|curl)", text, re.IGNORECASE): t.append("net")
    if re.search(r"\b(file|fs|stream|read|write|upload|download|path|directory|folder)", text, re.IGNORECASE): t.append("io")
    return sorted(set(t))

# ===== Indexer with encoding detection =====
def should_index(p: pathlib.Path, exclude_langs: set) -> bool:
    if p.is_dir(): return False
    if any(part.startswith(".") for part in p.parts): return False
    lang = detect_lang(p)
    if lang in exclude_langs:
        return False
    if lang == "other":
        important_ext = {".xml", ".json", ".yaml", ".yml", ".toml", ".ini", ".config", ".md", ".txt", ".html", ".htm", ".css"}
        if p.suffix.lower() in important_ext: return True
        if p.name in {"Dockerfile", "Makefile", "Rakefile", "Gemfile", ".env", ".env.example"}: return True
        return False
    return True

def cmd_index(repo: pathlib.Path, index_path: pathlib.Path, exclude_langs: set = None, max_file_bytes: int = None):
    if exclude_langs is None:
        exclude_langs = set()
    if max_file_bytes is None:
        max_file_bytes = DEFAULT_MAX_FILE_BYTES

    paths = []
    large_files = []
    encoding_stats = {}  # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°çµ±è¨ˆ

    for p in repo.rglob("*"):
        if p.is_file() and not any(d in p.parts for d in IGNORE_DIRS):
            file_size = p.stat().st_size
            if file_size > max_file_bytes:
                large_files.append((str(p.relative_to(repo)), file_size))
                continue
            if should_index(p, exclude_langs):
                paths.append(p)

    count = 0
    with open(index_path, "w", encoding="utf-8") as w:
        for p in sorted(paths):
            lang = detect_lang(p)
            try:
                # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è‡ªå‹•æ¤œå‡ºã—ã¦èª­ã¿è¾¼ã¿
                txt, encoding = read_file_with_encoding(p)

                # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°çµ±è¨ˆã‚’è¨˜éŒ²
                encoding_stats[encoding] = encoding_stats.get(encoding, 0) + 1

                if not txt:
                    continue

                tags = make_tags(txt)
                rel_path = str(p.relative_to(repo))
                encoded = txt.encode("utf-8", errors="ignore")

                doc = Doc(
                    path=rel_path,
                    lang=lang,
                    size=len(encoded),
                    sha1=sha1_bytes(encoded),
                    tags=tags,
                    summary=make_summary(txt),
                    text=txt,
                    encoding=encoding  # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æƒ…å ±ã‚’ä¿å­˜
                )
                w.write(json.dumps(asdict(doc), ensure_ascii=False) + "\n")
                count += 1
            except Exception as e:
                print(f"[ERROR] Failed to process {p}: {e}")
                continue

    print(f"[OK] Indexed {count} files -> {index_path}")

    # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°çµ±è¨ˆã‚’è¡¨ç¤º
    if encoding_stats:
        print("[INFO] Encoding statistics:")
        for enc, cnt in sorted(encoding_stats.items(), key=lambda x: -x[1])[:5]:
            print(f"  - {enc}: {cnt} files")

    # å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ­ã‚°ã‚’è¨˜éŒ²
    if large_files:
        log_path = write_large_file_log(large_files, index_path.parent.resolve(), max_file_bytes)
        if log_path:
            threshold_mb = max_file_bytes / 1_000_000
            print(f"[WARNING] Skipped {len(large_files)} files exceeding ~{threshold_mb:.1f} MB. Details: {log_path}")

def write_large_file_log(large_files: List[Tuple[str, int]], output_dir: pathlib.Path, max_file_bytes: int) -> Optional[pathlib.Path]:
    if not large_files: return None
    reports_dir = output_dir / "reports"
    reports_dir.mkdir(parents=True, exist_ok=True)
    log_path = reports_dir / "large_files_over_limit.log"

    with open(log_path, "w", encoding="utf-8") as f:
        f.write(f"Large files exceeding limit ({max_file_bytes:,} bytes):\n\n")
        for file_path, size in sorted(large_files, key=lambda x: x[1], reverse=True):
            f.write(f"{size:12,} bytes  {file_path}\n")
        f.write(f"\nTotal: {len(large_files)} files\n")

    return log_path

# ===== Retrieval =====
def load_index(index_path: pathlib.Path) -> List[Dict[str,Any]]:
    arr: List[Dict[str,Any]] = []
    with open(index_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line:
                arr.append(json.loads(line))
    return arr

def retrieve(query: str, index: List[Dict[str,Any]], topk: int = 10) -> List[Dict[str,Any]]:
    if SKLEARN_OK:
        vec_p = pathlib.Path(VEC_PATH)
        mat_p = pathlib.Path(MATRIX_PATH)
        if vec_p.exists() and mat_p.exists():
            return retrieve_tfidf(query, index, vec_p, mat_p, topk)
    return retrieve_naive(query, index, topk)

def retrieve_naive(query: str, index: List[Dict[str,Any]], topk: int) -> List[Dict[str,Any]]:
    q_words = set(w.lower() for w in re.split(r"\W+", query) if w)
    for doc in index:
        txt_words = set(w.lower() for w in re.split(r"\W+", doc.get("text","")[:3000]) if w)
        doc["_score"] = len(q_words & txt_words) / (len(q_words) + 0.01)
    return sorted(index, key=lambda x: x.get("_score",0), reverse=True)[:topk]

def retrieve_tfidf(query: str, index: List[Dict[str,Any]], vec_path: pathlib.Path, mat_path: pathlib.Path, topk: int) -> List[Dict[str,Any]]:
    from sklearn.metrics.pairwise import cosine_similarity
    vec = joblib.load(vec_path)
    mat = joblib.load(mat_path)
    q_vec = vec.transform([query])
    scores = cosine_similarity(q_vec, mat).flatten()
    for i, doc in enumerate(index):
        if i < len(scores):
            doc["_score"] = float(scores[i])
    return sorted(index, key=lambda x: x.get("_score",0), reverse=True)[:topk]

# ===== Vectorization =====
def cmd_vectorize(index_path: pathlib.Path):
    if not SKLEARN_OK:
        print("scikit-learnæœªå°å…¥ã€‚pip install scikit-learn joblib")
        return
    index = load_index(index_path)
    texts = [d.get("text","")[:10000] for d in index]
    vec = TfidfVectorizer(max_features=5000, ngram_range=(1,2), stop_words=None)
    mat = vec.fit_transform(texts)
    joblib.dump(vec, VEC_PATH)
    joblib.dump(mat, MATRIX_PATH)
    print(f"[OK] Vectorized -> {VEC_PATH}, {MATRIX_PATH}")

# ===== Advice rules =====
def rule_advices(d: Dict[str,Any]) -> List[str]:
    text = d.get("text", "")[:20000]
    out: List[str] = []
    out += scan_money(text)
    out += scan_print(text)
    out += scan_ui(text)
    out += scan_db(text)
    return out[:8]

# ===== é‡è¦åº¦ã‚¹ã‚³ã‚¢è¨ˆç®— =====
def calculate_severity_score(suspicions: List[str]) -> int:
    """å•é¡Œãƒªã‚¹ãƒˆã‹ã‚‰é‡è¦åº¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
    total_score = 0
    for suspicion in suspicions:
        total_score += SEVERITY_SCORES.get(suspicion, 1)
    return total_score

def make_advice_entry_with_severity(d: Dict[str,Any]) -> Dict[str,Any]:
    """é‡è¦åº¦ã‚¹ã‚³ã‚¢ä»˜ãã®ã‚¨ãƒ³ãƒˆãƒªã‚’ä½œæˆ"""
    suspicions = rule_advices(d)
    severity_score = calculate_severity_score(suspicions)

    # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æƒ…å ±ã‚‚å«ã‚ã‚‹
    encoding_info = d.get("encoding", "unknown")

    return {
        "path": d["path"],
        "lang": d["lang"],
        "encoding": encoding_info,
        "score": round(d.get("_score",0.0),4),
        "tags": d.get("tags",[]),
        "suspicions": suspicions,
        "severity_score": severity_score,
        "severity_level": get_severity_level(severity_score)
    }

def get_severity_level(score: int) -> str:
    """ã‚¹ã‚³ã‚¢ã‹ã‚‰é‡è¦åº¦ãƒ¬ãƒ™ãƒ«ã‚’åˆ¤å®š"""
    if score >= 10:
        return "ğŸ”´ ç·Šæ€¥"
    elif score >= 7:
        return "ğŸŸ  é«˜"
    elif score >= 4:
        return "ğŸŸ¡ ä¸­"
    elif score >= 1:
        return "ğŸ”µ ä½"
    else:
        return "âšª å•é¡Œãªã—"

# ===== AI Review with Solutions =====
def ai_review_with_solutions(query: str, items: List[Dict[str,Any]]) -> str:
    """æ”¹å–„æ¡ˆã¨ä¿®æ­£ã‚³ãƒ¼ãƒ‰ä¾‹ã‚’å«ã‚€AIãƒ¬ãƒ“ãƒ¥ãƒ¼"""
    if not OPENAI_OK:
        return "(AIæœªæœ‰åŠ¹: openaiæœªå°å…¥)"

    api_key = os.environ.get("OPENAI_API_KEY")
    if not api_key:
        return "(AIæœªæœ‰åŠ¹: OPENAI_API_KEY æœªè¨­å®š)"

    # gpt-4oã‚’ä½¿ç”¨ï¼ˆGPT-5ã¯ç©ºã®å¿œç­”ã‚’è¿”ã™ãŸã‚ï¼‰
    model = os.environ.get("OPENAI_MODEL", "gpt-4o")
    client = OpenAI(api_key=api_key)

    # åˆ†æå¯¾è±¡ã®ã‚³ãƒ¼ãƒ‰ã‚’æº–å‚™
    code_samples = []
    for item in items[:5]:  # ä¸Šä½5ä»¶
        code_samples.append({
            "path": item["path"],
            "lang": item["lang"],
            "encoding": item.get("encoding", "unknown"),
            "issues": item.get("suspicions", []),
            "code": item.get("text", "")[:2000]  # å„ãƒ•ã‚¡ã‚¤ãƒ«ã®æœ€åˆã®2000æ–‡å­—
        })

    prompt = f"""
ã‚ãªãŸã¯ã‚·ãƒ‹ã‚¢ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼ã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¯ã‚¨ãƒª: {query}

ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æã—ã€å…·ä½“çš„ãªæ”¹å–„æ¡ˆã‚’æä¾›ã—ã¦ãã ã•ã„ï¼š

{json.dumps(code_samples, ensure_ascii=False, indent=2)[:MAX_PROMPT_SIZE]}

## è¦æ±‚äº‹é …ï¼š
1. å„ãƒ•ã‚¡ã‚¤ãƒ«ã®å•é¡Œç‚¹ã‚’æ˜ç¢ºã«æŒ‡æ‘˜
2. å…·ä½“çš„ãªæ”¹å–„æ¡ˆã®æç¤º
3. ä¿®æ­£ã‚³ãƒ¼ãƒ‰ä¾‹ï¼ˆBefore/Afterå½¢å¼ï¼‰
4. æ—¥æœ¬èªã®æ–‡å­—åŒ–ã‘ã«æ³¨æ„ã—ã€é©åˆ‡ãªã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å¯¾å¿œã‚’ææ¡ˆ

é‡è¦: å®Ÿéš›ã«å‹•ä½œã™ã‚‹å…·ä½“çš„ãªä¿®æ­£ã‚³ãƒ¼ãƒ‰ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
"""

    try:
        print("[INFO] Calling AI for detailed analysis...")
        resp = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "You are an expert code reviewer. Provide specific, actionable improvements with working code examples in Japanese."},
                {"role": "user", "content": prompt}
            ],
            temperature=1,
            max_completion_tokens=3000,
            timeout=AI_TIMEOUT
        )

        result = resp.choices[0].message.content
        if not result or result.strip() == "":
            return "(AIã‹ã‚‰ç©ºã®å¿œç­”)"

        return result.strip()

    except Exception as e:
        return f"(AIã‚¨ãƒ©ãƒ¼: {str(e)[:200]})"

# ===== Report Generation =====
def render_report_sorted(title: str, items: List[Dict[str,Any]], ai_summary: str|None) -> str:
    """é‡è¦åº¦é †ã«ã‚½ãƒ¼ãƒˆã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆï¼ˆUTF-8ï¼‰"""
    lines: List[str] = [
        f"# {title}",
        "",
        f"ç”Ÿæˆ: {time.strftime('%Y-%m-%d %H:%M:%S')}",
        f"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å¯¾å¿œ: è‡ªå‹•æ¤œå‡º",
        ""
    ]

    if ai_summary:
        lines += [
            "## ğŸ¤– AIã«ã‚ˆã‚‹è©³ç´°åˆ†æã¨æ”¹å–„æ¡ˆ",
            "",
            ai_summary,
            "",
            "---",
            ""
        ]

    # é‡è¦åº¦ã§ã‚½ãƒ¼ãƒˆï¼ˆé«˜ã„é †ï¼‰
    sorted_items = sorted(items, key=lambda x: x.get('severity_score', 0), reverse=True)

    # é‡è¦åº¦åˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    critical_items = [it for it in sorted_items if it.get('severity_score', 0) >= 10]
    high_items = [it for it in sorted_items if 7 <= it.get('severity_score', 0) < 10]
    medium_items = [it for it in sorted_items if 4 <= it.get('severity_score', 0) < 7]
    low_items = [it for it in sorted_items if 1 <= it.get('severity_score', 0) < 4]
    no_issue_items = [it for it in sorted_items if it.get('severity_score', 0) == 0]

    # çµ±è¨ˆæƒ…å ±
    lines += [
        "## ğŸ“Š å•é¡Œã®åˆ†å¸ƒ",
        f"- ğŸ”´ ç·Šæ€¥: {len(critical_items)}ä»¶",
        f"- ğŸŸ  é«˜: {len(high_items)}ä»¶",
        f"- ğŸŸ¡ ä¸­: {len(medium_items)}ä»¶",
        f"- ğŸ”µ ä½: {len(low_items)}ä»¶",
        f"- âšª å•é¡Œãªã—: {len(no_issue_items)}ä»¶",
        f"- **åˆè¨ˆ**: {len(items)}ä»¶",
        ""
    ]

    # é‡è¦åº¦åˆ¥ã«å‡ºåŠ›
    if critical_items:
        lines.append("## ğŸ”´ ç·Šæ€¥å¯¾å¿œãŒå¿…è¦ãªå•é¡Œ\n")
        for i, it in enumerate(critical_items, 1):
            lines += format_item(i, it)

    if high_items:
        lines.append("## ğŸŸ  é«˜å„ªå…ˆåº¦ã®å•é¡Œ\n")
        for i, it in enumerate(high_items, len(critical_items) + 1):
            lines += format_item(i, it)

    if medium_items:
        lines.append("## ğŸŸ¡ ä¸­å„ªå…ˆåº¦ã®å•é¡Œ\n")
        for i, it in enumerate(medium_items, len(critical_items) + len(high_items) + 1):
            lines += format_item(i, it)

    if low_items:
        lines.append("## ğŸ”µ ä½å„ªå…ˆåº¦ã®å•é¡Œ\n")
        for i, it in enumerate(low_items, len(critical_items) + len(high_items) + len(medium_items) + 1):
            lines += format_item(i, it)

    if no_issue_items:
        lines.append("## âšª å•é¡ŒãŒæ¤œå‡ºã•ã‚Œãªã‹ã£ãŸãƒ•ã‚¡ã‚¤ãƒ«\n")
        for i, it in enumerate(no_issue_items, len(items) - len(no_issue_items) + 1):
            lines += format_item_minimal(i, it)

    return "\n".join(lines)

def format_item(num: int, item: Dict[str,Any]) -> List[str]:
    """å•é¡Œã‚ã‚Šã‚¢ã‚¤ãƒ†ãƒ ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    lines = [
        f"### {num}. {item['path']}",
        f"- **è¨€èª**: {item['lang']}",
        f"- **ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°**: {item.get('encoding', 'unknown')}",
        f"- **é‡è¦åº¦**: {item.get('severity_level', 'ä¸æ˜')} (ã‚¹ã‚³ã‚¢: {item.get('severity_score', 0)})",
        f"- **æ¤œç´¢ã‚¹ã‚³ã‚¢**: {item.get('score', 0)}",
        f"- **ã‚¿ã‚°**: {', '.join(item.get('tags', [])) or '-'}",
        "- **æ¤œå‡ºã•ã‚ŒãŸå•é¡Œ**:"
    ]

    susp = item.get("suspicions", [])
    if susp:
        for s in susp:
            severity = SEVERITY_SCORES.get(s, 1)
            emoji = "ğŸ”´" if severity >= 8 else "ğŸŸ " if severity >= 5 else "ğŸŸ¡" if severity >= 3 else "ğŸ”µ"
            lines.append(f"  - {emoji} {s}")
    else:
        lines.append("  - (ãªã—)")

    lines.append("")
    return lines

def format_item_minimal(num: int, item: Dict[str,Any]) -> List[str]:
    """å•é¡Œãªã—ã‚¢ã‚¤ãƒ†ãƒ ã®ç°¡æ½”ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    return [
        f"### {num}. {item['path']} ({item['lang']}, {item.get('encoding', 'unknown')})",
        f"- ã‚¿ã‚°: {', '.join(item.get('tags', [])) or '-'}",
        ""
    ]

# ===== Commands =====
def cmd_query(query: str, topk: int, mode: str, index_path: pathlib.Path, out: Optional[pathlib.Path]):
    index = load_index(index_path)
    cands = retrieve(query, index, topk=topk)
    items = [make_advice_entry_with_severity(d) for d in cands]

    ai_summary = None
    if mode in ("ai", "hybrid"):
        # itemsã«ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚ã‚‹
        for i, item in enumerate(items):
            if i < len(cands):
                item["text"] = cands[i].get("text", "")
        ai_summary = ai_review_with_solutions(query, items)

    rep = render_report_sorted(f"æ¤œç´¢ãƒ¬ãƒãƒ¼ãƒˆ: {query}", items, ai_summary)

    if out:
        out.parent.mkdir(parents=True, exist_ok=True)
        # UTF-8ã§ãƒ¬ãƒãƒ¼ãƒˆã‚’å‡ºåŠ›
        out.write_text(rep, encoding="utf-8")
        print(f"[OK] wrote {out} (UTF-8)")
    else:
        print(rep)

def cmd_advise(topk: int, mode: str, index_path: pathlib.Path, out: Optional[pathlib.Path]):
    seed_query = "é‡‘é¡ ä¸æ•´åˆ ç¨ å°æ•° å°åˆ· Print HasMorePages NewPage UI UX ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£ DB N+1 JOIN é…ã„"
    index = load_index(index_path)
    cands = retrieve(seed_query, index, topk=topk)
    items = [make_advice_entry_with_severity(d) for d in cands]

    ai_summary = None
    if mode in ("ai", "hybrid"):
        # itemsã«ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚ã‚‹
        for i, item in enumerate(items):
            if i < len(cands):
                item["text"] = cands[i].get("text", "")
        ai_summary = ai_review_with_solutions("æ¨ªæ–­åŠ©è¨€ï¼ˆé‡‘é¡/å°åˆ·/UI/DBï¼‰", items)

    rep = render_report_sorted("å…¨ä½“åŠ©è¨€ï¼ˆæ¨ªæ–­ãƒã‚§ãƒƒã‚¯ï¼‰", items, ai_summary)

    if out:
        out.parent.mkdir(parents=True, exist_ok=True)
        # UTF-8ã§ãƒ¬ãƒãƒ¼ãƒˆã‚’å‡ºåŠ›
        out.write_text(rep, encoding="utf-8")
        print(f"[OK] wrote {out} (UTF-8)")
    else:
        print(rep)

# ===== CLI =====
if __name__ == "__main__":
    ap = argparse.ArgumentParser(description="èª­å–å°‚ç”¨ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è‡ªå‹•æ¤œå‡ºç‰ˆï¼‰")
    sub = ap.add_subparsers(dest="cmd", required=True)

    ap_idx = sub.add_parser("index", help="ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ï¼ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è‡ªå‹•æ¤œå‡ºï¼‰")
    ap_idx.add_argument("repo", type=str)
    ap_idx.add_argument("--exclude-langs", type=str, nargs="*", help="é™¤å¤–ã™ã‚‹è¨€èª (ä¾‹: delphi java)")
    ap_idx.add_argument("--max-file-mb", type=float, default=4.0, help="æœ€å¤§ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º(MB) ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ4MB")

    ap_vec = sub.add_parser("vectorize", help="(ä»»æ„) TF-IDFãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆ")
    ap_vec.add_argument("--index", type=str, default=INDEX_PATH)

    ap_q = sub.add_parser("query", help="è‡ªç„¶è¨€èªã§é–¢é€£ã‚½ãƒ¼ã‚¹ã‚’æ¢ç´¢ï¼ˆæ”¹å–„æ¡ˆä»˜ãï¼‰")
    ap_q.add_argument("query", type=str, help="æ¤œç´¢ã‚¯ã‚¨ãƒª")
    ap_q.add_argument("--topk", type=int, default=30, help="æ¤œç´¢ä¸Šä½Nä»¶")
    ap_q.add_argument("--mode", choices=["rules","ai","hybrid"], default="hybrid")
    ap_q.add_argument("--index", type=str, default=INDEX_PATH)
    ap_q.add_argument("--out", type=str, help="å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ« (UTF-8 Markdown)")

    ap_adv = sub.add_parser("advise", help="å…¨ä½“åŠ©è¨€ï¼ˆé‡è¦åº¦é †ãƒ»æ”¹å–„æ¡ˆä»˜ãï¼‰")
    ap_adv.add_argument("--topk", type=int, default=80, help="åŠ©è¨€å¯¾è±¡ä¸Šä½Nä»¶")
    ap_adv.add_argument("--mode", choices=["rules","ai","hybrid"], default="hybrid")
    ap_adv.add_argument("--index", type=str, default=INDEX_PATH)
    ap_adv.add_argument("--out", type=str, help="å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ« (UTF-8 Markdown)")

    args = ap.parse_args()

    if args.cmd == "index":
        repo = pathlib.Path(args.repo)
        exclude_langs = set(args.exclude_langs) if args.exclude_langs else set()
        max_file_bytes = int(args.max_file_mb * 1_000_000)
        cmd_index(repo, pathlib.Path(INDEX_PATH), exclude_langs, max_file_bytes)

    elif args.cmd == "vectorize":
        cmd_vectorize(pathlib.Path(args.index))

    elif args.cmd == "query":
        out = pathlib.Path(args.out) if args.out else None
        cmd_query(args.query, args.topk, args.mode, pathlib.Path(args.index), out)

    elif args.cmd == "advise":
        out = pathlib.Path(args.out) if args.out else None
        cmd_advise(args.topk, args.mode, pathlib.Path(args.index), out)