#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
codex_review_ultimate.py â€” æœ€çµ‚ç‰ˆï¼š2æ®µéšè§£æã‚·ã‚¹ãƒ†ãƒ 

å‡¦ç†ãƒ•ãƒ­ãƒ¼:
  1) ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã§å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é«˜é€Ÿè§£æ
  2) é‡è¦åº¦ã®é«˜ã„å•é¡Œã‚’æŒã¤ãƒ•ã‚¡ã‚¤ãƒ«ã‚’AIã§è©³ç´°è§£æï¼ˆ1ãƒ•ã‚¡ã‚¤ãƒ«ãšã¤ï¼‰
  3) 2ç¨®é¡ã®ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ç‰ˆ / AIæ”¹å–„æ¡ˆä»˜ãç‰ˆï¼‰

ä¸»ãªç‰¹å¾´:
  - ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è‡ªå‹•æ¤œå‡º
  - é€²æ—è¡¨ç¤ºï¼ˆXX/YYãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ä¸­ï¼‰
  - ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¯¾ç­–ï¼ˆå€‹åˆ¥å‡¦ç†ã€ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ï¼‰
  - å…·ä½“çš„ãªæ”¹å–„ã‚³ãƒ¼ãƒ‰ä¾‹ã®æç¤º

pip install chromadb openai scikit-learn joblib regex chardet
"""
from __future__ import annotations
import argparse, hashlib, json, os, pathlib, re, sys, time
from dataclasses import dataclass, asdict, field
from typing import Any, Dict, List, Tuple, Optional
import chardet
from datetime import datetime

# ===== Config =====
IGNORE_DIRS = {".git","node_modules","dist","build","out","bin","obj",".idea",".vscode",".next","coverage","target"}
DEFAULT_MAX_FILE_BYTES = 4_000_000
ENV_FILE = ".env"
INDEX_PATH = ".advice_index.jsonl"
VEC_PATH = ".advice_tfidf.pkl"
MATRIX_PATH = ".advice_matrix.pkl"
AI_TIMEOUT = 60  # å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
AI_MAX_RETRIES = 2  # AIãƒªãƒˆãƒ©ã‚¤å›æ•°
AI_MIN_SEVERITY = 7  # AIè§£æã™ã‚‹æœ€å°é‡è¦åº¦ã‚¹ã‚³ã‚¢
AI_MAX_FILES = 20  # AIè§£æã™ã‚‹æœ€å¤§ãƒ•ã‚¡ã‚¤ãƒ«æ•°

# é‡è¦åº¦ã‚¹ã‚³ã‚¢å®šç¾©
SEVERITY_SCORES = {
    "DB: ãƒ«ãƒ¼ãƒ—å†…SELECT (N+1) ç–‘ã„": 10,
    "DB: SELECT * â†’è² è·å¢—ã€‚åˆ—é™å®š": 8,
    "DB: å¤šé‡JOINâ†’é…å»¶ã®æã‚Œã€‚è¨ˆç”»/IDXç¢ºèª": 7,
    "DB: å¤§OFFSETâ†’é…å»¶ã€‚IDã‚«ãƒ¼ã‚½ãƒ«æ¨å¥¨": 6,
    "é‡‘é¡: æµ®å‹•å°æ•°ã§é‡‘é¡â†’èª¤å·®ã€‚Decimal/é€šè²¨å‹ã¸": 9,
    "é‡‘é¡: ç¨è¾¼/ç¨æŠœãƒ»ç«¯æ•°å‡¦ç†ã®æ··åœ¨æ³¨æ„": 7,
    "UI: XSSè„†å¼±æ€§ã®ç–‘ã„ï¼ˆæœªã‚µãƒ‹ã‚¿ã‚¤ã‚ºHTMLï¼‰": 8,
    "UI: å…¥åŠ›æ¤œè¨¼ãŒä¸ååˆ†": 5,
    "UI: å¤šé‡ã‚¯ãƒªãƒƒã‚¯é˜²æ­¢ãªã—": 3,
    "å°åˆ·: ã‚¨ãƒ©ãƒ¼å‡¦ç†ãªã—â†’é€”ä¸­åœæ­¢ãƒªã‚¹ã‚¯": 4,
    "å°åˆ·: æ”¹ãƒšãƒ¼ã‚¸åˆ¤å®šãŒä¸ååˆ†": 2,
}

# ===== Optional deps =====
SKLEARN_OK = False
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    import joblib
    SKLEARN_OK = True
except:
    pass

OPENAI_OK = False
try:
    from openai import OpenAI
    OPENAI_OK = True
except:
    pass

# ===== Utils =====
if ENV_FILE and pathlib.Path(ENV_FILE).exists():
    with open(ENV_FILE, "r", encoding="utf-8") as f:
        for line in f:
            if "=" in line:
                key, val = line.strip().split("=", 1)
                os.environ[key] = val

@dataclass
class Doc:
    path: str
    lang: str
    size: int
    sha1: str
    tags: List[str]
    summary: str
    text: str
    encoding: str

@dataclass
class RuleResult:
    """ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹è§£æçµæœ"""
    path: str
    full_path: str
    lang: str
    encoding: str
    severity_score: int
    severity_level: str
    issues: List[str]
    tags: List[str]
    sample_code: str = ""  # å•é¡Œç®‡æ‰€ã®ã‚³ãƒ¼ãƒ‰ã‚µãƒ³ãƒ—ãƒ«
    suggested_fix: str = ""  # ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®æ”¹å–„æ¡ˆ

@dataclass
class AIResult:
    """AIè§£æçµæœ"""
    path: str
    full_path: str
    analysis: str
    improvements: List[Dict[str, str]] = field(default_factory=list)
    error: Optional[str] = None

def detect_encoding(file_path: pathlib.Path) -> str:
    """ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è‡ªå‹•æ¤œå‡º"""
    try:
        with open(file_path, 'rb') as f:
            raw_data = f.read(10000)
            result = chardet.detect(raw_data)
            encoding = result['encoding']
            confidence = result.get('confidence', 0)

            if not encoding or confidence < 0.7:
                for enc in ['utf-8', 'shift_jis', 'cp932', 'euc-jp', 'iso-2022-jp']:
                    try:
                        with open(file_path, 'r', encoding=enc) as test_f:
                            test_f.read(1000)
                        return enc
                    except:
                        continue
                return 'utf-8'

            if encoding.lower() in ['cp932', 'shift_jis', 'shift-jis', 'sjis']:
                return 'cp932'

            return encoding.lower()
    except:
        return 'utf-8'

def read_file_with_encoding(file_path: pathlib.Path) -> Tuple[str, str]:
    """ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è‡ªå‹•æ¤œå‡ºã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
    encoding = detect_encoding(file_path)

    try:
        with open(file_path, 'r', encoding=encoding, errors='ignore') as f:
            content = f.read()
        return content, encoding
    except:
        try:
            with open(file_path, 'rb') as f:
                raw_data = f.read()
            content = raw_data.decode(encoding, errors='ignore')
            return content, encoding
        except:
            return "", "unknown"

def sha1_bytes(data: bytes) -> str:
    return hashlib.sha1(data).hexdigest()[:10]

def detect_lang(p: pathlib.Path) -> str:
    ext = p.suffix.lower()
    if ext in (".py",): return "python"
    if ext in (".js",".mjs",".cjs",".jsx"): return "javascript"
    if ext in (".ts",".tsx"): return "typescript"
    if ext in (".cs",".csx"): return "csharp"
    if ext in (".java",): return "java"
    if ext in (".pas",".dfm",".dpr"): return "delphi"
    if ext in (".rb",): return "ruby"
    if ext in (".go",): return "go"
    if ext in (".rs",): return "rust"
    if ext in (".php",): return "php"
    if ext in (".c",".h"): return "c"
    if ext in (".cpp",".hpp",".cc",".hh",".cxx",".hxx"): return "cpp"
    return "other"

def make_summary(text: str) -> str:
    lines = text.splitlines()[:10]
    return " / ".join(l.strip()[:80] for l in lines if l.strip())[:400]

# ===== Rule-based Analysis with Code Samples =====
def scan_money_with_sample(text: str) -> List[Tuple[str, str, str]]:
    """é‡‘é¡é–¢é€£ã®å•é¡Œã‚’æ¤œå‡ºã—ã€ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ã¨æ”¹å–„æ¡ˆã‚’è¿”ã™"""
    results = []

    # æµ®å‹•å°æ•°ç‚¹ã§ã®é‡‘é¡è¨ˆç®—
    pattern = r"(\b(?:float|double|real)\b.*\b(?:price|cost|money|amount|tax|total|sum|charge|fee|pay).*)"
    matches = re.finditer(pattern, text, re.IGNORECASE)
    for match in matches:
        sample = match.group(0)[:200]
        issue = "é‡‘é¡: æµ®å‹•å°æ•°ã§é‡‘é¡â†’èª¤å·®ã€‚Decimal/é€šè²¨å‹ã¸"
        fix = """
// Before:
float price = 100.0f;
float tax = price * 0.1f;  // ç²¾åº¦å•é¡Œ

// After:
decimal price = 100.0m;
decimal tax = price * 0.1m;  // æ­£ç¢ºãªè¨ˆç®—"""
        results.append((issue, sample, fix))
        break  # æœ€åˆã®1ä»¶ã®ã¿

    # ç¨è¨ˆç®—ã®ç«¯æ•°å‡¦ç†
    if re.search(r"\b(total|sum|amount)\s*=.*\+.*tax", text, re.IGNORECASE):
        pattern = r"(.*(?:total|sum|amount)\s*=.*\+.*tax.*)"
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            sample = match.group(0)[:200]
            issue = "é‡‘é¡: ç¨è¾¼/ç¨æŠœãƒ»ç«¯æ•°å‡¦ç†ã®æ··åœ¨æ³¨æ„"
            fix = """
// çµ±ä¸€ã—ãŸç«¯æ•°å‡¦ç†
decimal CalculateTotalWithTax(decimal price, decimal taxRate) {
    decimal tax = Math.Round(price * taxRate, 0, MidpointRounding.ToEven);
    return price + tax;
}"""
            results.append((issue, sample, fix))

    return results

def scan_db_with_sample(text: str) -> List[Tuple[str, str, str]]:
    """DBé–¢é€£ã®å•é¡Œã‚’æ¤œå‡ºã—ã€ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ã¨æ”¹å–„æ¡ˆã‚’è¿”ã™"""
    results = []

    # SELECT * ã®ä½¿ç”¨
    pattern = r"(.*SELECT\s+\*\s+FROM.*)"
    matches = re.finditer(pattern, text, re.IGNORECASE)
    for match in matches:
        sample = match.group(0)[:200]
        issue = "DB: SELECT * â†’è² è·å¢—ã€‚åˆ—é™å®š"
        fix = """
// Before:
SELECT * FROM Users WHERE id = @id

// After:
SELECT id, name, email FROM Users WHERE id = @id"""
        results.append((issue, sample, fix))
        break

    # N+1å•é¡Œ
    if re.search(r"(for|while|foreach).*\n.*\n.*\n.*(SELECT|INSERT|UPDATE|DELETE)", text, re.IGNORECASE):
        issue = "DB: ãƒ«ãƒ¼ãƒ—å†…SELECT (N+1) ç–‘ã„"
        sample = "ãƒ«ãƒ¼ãƒ—å†…ã§SQLã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œã—ã¦ã„ã‚‹å¯èƒ½æ€§"
        fix = """
// Before:
foreach(var userId in userIds) {
    var user = db.Query("SELECT * FROM Users WHERE id = " + userId);
}

// After:
var users = db.Query("SELECT * FROM Users WHERE id IN @userIds", new { userIds });"""
        results.append((issue, sample, fix))

    return results

def scan_ui_with_sample(text: str) -> List[Tuple[str, str, str]]:
    """UIé–¢é€£ã®å•é¡Œã‚’æ¤œå‡ºã—ã€ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ã¨æ”¹å–„æ¡ˆã‚’è¿”ã™"""
    results = []

    # XSSè„†å¼±æ€§
    if re.search(r"(innerHTML\s*=|dangerouslySetInnerHTML)", text):
        issue = "UI: XSSè„†å¼±æ€§ã®ç–‘ã„ï¼ˆæœªã‚µãƒ‹ã‚¿ã‚¤ã‚ºHTMLï¼‰"
        sample = "innerHTML ã¾ãŸã¯ dangerouslySetInnerHTML ã®ä½¿ç”¨"
        fix = """
// Before:
element.innerHTML = userInput;

// After:
element.textContent = userInput;  // ã¾ãŸã¯ã‚µãƒ‹ã‚¿ã‚¤ã‚º
element.innerHTML = DOMPurify.sanitize(userInput);"""
        results.append((issue, sample, fix))

    # å…¥åŠ›æ¤œè¨¼ä¸è¶³
    if re.search(r"(input|textarea|select)", text, re.IGNORECASE) and not re.search(r"(validate|pattern|required)", text, re.IGNORECASE):
        issue = "UI: å…¥åŠ›æ¤œè¨¼ãŒä¸ååˆ†"
        sample = "å…¥åŠ›ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«æ¤œè¨¼ãŒè¦‹å½“ãŸã‚‰ãªã„"
        fix = """
// å…¥åŠ›æ¤œè¨¼ã®è¿½åŠ 
<input type="email" required pattern="[a-z0-9._%+-]+@[a-z0-9.-]+\\.[a-z]{2,}$" />

// JSã§ã®æ¤œè¨¼
if (!input.value.match(/^[a-z0-9._%+-]+@[a-z0-9.-]+\\.[a-z]{2,}$/i)) {
    showError("æœ‰åŠ¹ãªãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„");
}"""
        results.append((issue, sample, fix))

    return results

def scan_print_with_sample(text: str) -> List[Tuple[str, str, str]]:
    """å°åˆ·é–¢é€£ã®å•é¡Œã‚’æ¤œå‡ºã—ã€ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ã¨æ”¹å–„æ¡ˆã‚’è¿”ã™"""
    results = []

    if re.search(r"(PrintDocument|PrintPage)", text, re.IGNORECASE) and not re.search(r"(try|catch|error)", text, re.IGNORECASE):
        issue = "å°åˆ·: ã‚¨ãƒ©ãƒ¼å‡¦ç†ãªã—â†’é€”ä¸­åœæ­¢ãƒªã‚¹ã‚¯"
        sample = "å°åˆ·å‡¦ç†ã«ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãŒãªã„"
        fix = """
// Before:
printDocument.Print();

// After:
try {
    printDocument.Print();
} catch (Exception ex) {
    Logger.LogError($"å°åˆ·ã‚¨ãƒ©ãƒ¼: {ex.Message}");
    MessageBox.Show("å°åˆ·ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚");
}"""
        results.append((issue, sample, fix))

    return results

def analyze_with_rules(text: str) -> List[Tuple[str, str, str]]:
    """ã™ã¹ã¦ã®ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹è§£æã‚’å®Ÿè¡Œ"""
    all_results = []
    all_results.extend(scan_money_with_sample(text))
    all_results.extend(scan_db_with_sample(text))
    all_results.extend(scan_ui_with_sample(text))
    all_results.extend(scan_print_with_sample(text))
    return all_results

def make_tags(text: str) -> List[str]:
    t = []
    if re.search(r"\b(price|cost|money|amount|tax|total|sum|charge|fee|pay)", text, re.IGNORECASE):
        t.append("money")
    if re.search(r"\b(print|printer|page|paper|report|pdf|export|PrintDocument)", text, re.IGNORECASE):
        t.append("print")
    if re.search(r"\b(button|click|submit|form|input|select|modal|dialog)", text, re.IGNORECASE):
        t.append("uiux")
    if re.search(r"\b(SELECT|INSERT|UPDATE|DELETE|JOIN|WHERE|FROM|query|sql)", text, re.IGNORECASE):
        t.append("db")
    if re.search(r"\b(api|http|fetch|axios|request|response|endpoint|REST)", text, re.IGNORECASE):
        t.append("net")
    if re.search(r"\b(file|fs|stream|read|write|upload|download|path)", text, re.IGNORECASE):
        t.append("io")
    return sorted(set(t))

# ===== Indexer =====
def should_index(p: pathlib.Path, exclude_langs: set) -> bool:
    if p.is_dir(): return False
    if any(part.startswith(".") for part in p.parts): return False
    lang = detect_lang(p)
    if lang in exclude_langs:
        return False
    if lang == "other":
        important_ext = {".xml", ".json", ".yaml", ".yml", ".toml", ".ini", ".config", ".md", ".txt", ".html", ".htm", ".css"}
        if p.suffix.lower() in important_ext: return True
        if p.name in {"Dockerfile", "Makefile", "Rakefile", "Gemfile", ".env", ".env.example"}: return True
        return False
    return True

def cmd_index(repo: pathlib.Path, index_path: pathlib.Path, exclude_langs: set = None, max_file_bytes: int = None):
    if exclude_langs is None:
        exclude_langs = set()
    if max_file_bytes is None:
        max_file_bytes = DEFAULT_MAX_FILE_BYTES

    paths = []
    large_files = []

    for p in repo.rglob("*"):
        if p.is_file() and not any(d in p.parts for d in IGNORE_DIRS):
            file_size = p.stat().st_size
            if file_size > max_file_bytes:
                large_files.append((str(p.relative_to(repo)), file_size))
                continue
            if should_index(p, exclude_langs):
                paths.append(p)

    count = 0
    with open(index_path, "w", encoding="utf-8") as w:
        for p in sorted(paths):
            lang = detect_lang(p)
            try:
                txt, encoding = read_file_with_encoding(p)
                if not txt:
                    continue

                tags = make_tags(txt)
                rel_path = str(p.relative_to(repo))
                encoded = txt.encode("utf-8", errors="ignore")

                doc = Doc(
                    path=rel_path,
                    lang=lang,
                    size=len(encoded),
                    sha1=sha1_bytes(encoded),
                    tags=tags,
                    summary=make_summary(txt),
                    text=txt,
                    encoding=encoding
                )
                w.write(json.dumps(asdict(doc), ensure_ascii=False) + "\n")
                count += 1
            except Exception as e:
                print(f"[ERROR] Failed to process {p}: {e}")
                continue

    print(f"[OK] Indexed {count} files -> {index_path}")

    if large_files:
        reports_dir = index_path.parent / "reports"
        reports_dir.mkdir(parents=True, exist_ok=True)
        log_path = reports_dir / "large_files_over_limit.log"
        with open(log_path, "w", encoding="utf-8") as f:
            f.write(f"Large files exceeding limit ({max_file_bytes:,} bytes):\n\n")
            for file_path, size in sorted(large_files, key=lambda x: x[1], reverse=True):
                f.write(f"{size:12,} bytes  {file_path}\n")
            f.write(f"\nTotal: {len(large_files)} files\n")
        print(f"[WARNING] Skipped {len(large_files)} files exceeding limit")

# ===== Retrieval =====
def load_index(index_path: pathlib.Path) -> List[Dict[str,Any]]:
    arr: List[Dict[str,Any]] = []
    with open(index_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line:
                arr.append(json.loads(line))
    return arr

def retrieve(query: str, index: List[Dict[str,Any]], topk: int = 10) -> List[Dict[str,Any]]:
    if SKLEARN_OK:
        vec_p = pathlib.Path(VEC_PATH)
        mat_p = pathlib.Path(MATRIX_PATH)
        if vec_p.exists() and mat_p.exists():
            return retrieve_tfidf(query, index, vec_p, mat_p, topk)
    return retrieve_naive(query, index, topk)

def retrieve_naive(query: str, index: List[Dict[str,Any]], topk: int) -> List[Dict[str,Any]]:
    q_words = set(w.lower() for w in re.split(r"\W+", query) if w)
    for doc in index:
        txt_words = set(w.lower() for w in re.split(r"\W+", doc.get("text","")[:3000]) if w)
        doc["_score"] = len(q_words & txt_words) / (len(q_words) + 0.01)
    return sorted(index, key=lambda x: x.get("_score",0), reverse=True)[:topk]

def retrieve_tfidf(query: str, index: List[Dict[str,Any]], vec_path: pathlib.Path, mat_path: pathlib.Path, topk: int) -> List[Dict[str,Any]]:
    from sklearn.metrics.pairwise import cosine_similarity
    vec = joblib.load(vec_path)
    mat = joblib.load(mat_path)
    q_vec = vec.transform([query])
    scores = cosine_similarity(q_vec, mat).flatten()
    for i, doc in enumerate(index):
        if i < len(scores):
            doc["_score"] = float(scores[i])
    return sorted(index, key=lambda x: x.get("_score",0), reverse=True)[:topk]

# ===== Vectorization =====
def cmd_vectorize(index_path: pathlib.Path):
    if not SKLEARN_OK:
        print("scikit-learnæœªå°å…¥ã€‚pip install scikit-learn joblib")
        return
    index = load_index(index_path)
    texts = [d.get("text","")[:10000] for d in index]
    vec = TfidfVectorizer(max_features=5000, ngram_range=(1,2), stop_words=None)
    mat = vec.fit_transform(texts)
    joblib.dump(vec, VEC_PATH)
    joblib.dump(mat, MATRIX_PATH)
    print(f"[OK] Vectorized -> {VEC_PATH}, {MATRIX_PATH}")

# ===== AI Analysis (1 file at a time) =====
def analyze_single_file_with_ai(file_path: str, code_text: str, issues: List[str], retry_count: int = 0) -> AIResult:
    """å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’AIã§è©³ç´°è§£æ"""
    if not OPENAI_OK:
        return AIResult(path=file_path, full_path=file_path, analysis="", error="OpenAIæœªå°å…¥")

    api_key = os.environ.get("OPENAI_API_KEY")
    if not api_key:
        return AIResult(path=file_path, full_path=file_path, analysis="", error="API keyæœªè¨­å®š")

    # é€²æ—è¡¨ç¤º
    print(f"  è§£æä¸­: {file_path}")
    print(f"  æ¤œå‡ºå•é¡Œ: {', '.join(issues[:3])}")

    client = OpenAI(api_key=api_key)
    model = "gpt-4o"  # GPT-5ãŒç©ºå¿œç­”ã®ãŸã‚gpt-4oä½¿ç”¨

    # ã‚³ãƒ¼ãƒ‰ã®ä¸€éƒ¨ã‚’æŠ½å‡ºï¼ˆæœ€å¤§5000æ–‡å­—ï¼‰
    code_snippet = code_text[:5000]

    prompt = f"""
ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è©³ç´°ã«åˆ†æã—ã€å…·ä½“çš„ãªæ”¹å–„æ¡ˆã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚

ãƒ•ã‚¡ã‚¤ãƒ«: {file_path}
æ¤œå‡ºã•ã‚ŒãŸå•é¡Œ: {json.dumps(issues, ensure_ascii=False)}

ã‚³ãƒ¼ãƒ‰:
```
{code_snippet}
```

ä»¥ä¸‹ã®å½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ï¼š

## å•é¡Œç®‡æ‰€ã®ç‰¹å®š
- è¡Œç•ªå·ã¾ãŸã¯é–¢æ•°å
- å•é¡Œã®è©³ç´°èª¬æ˜

## æ”¹å–„æ¡ˆ
### å•é¡Œ1: [å•é¡Œå]
**Before:**
```
[ç¾åœ¨ã®ã‚³ãƒ¼ãƒ‰]
```

**After:**
```
[æ”¹å–„ã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰]
```

**èª¬æ˜:** ãªãœã“ã®æ”¹å–„ãŒå¿…è¦ã‹

é‡è¦: å®Ÿéš›ã«å‹•ä½œã™ã‚‹å…·ä½“çš„ãªã‚³ãƒ¼ãƒ‰ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
"""

    try:
        resp = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "You are an expert code reviewer. Provide specific improvements in Japanese."},
                {"role": "user", "content": prompt}
            ],
            temperature=1,
            max_completion_tokens=2000,
            timeout=AI_TIMEOUT
        )

        result = resp.choices[0].message.content
        if not result or result.strip() == "":
            if retry_count < AI_MAX_RETRIES:
                print(f"  [RETRY] ç©ºã®å¿œç­”ã®ãŸã‚å†è©¦è¡Œ ({retry_count + 1}/{AI_MAX_RETRIES})")
                time.sleep(2)
                return analyze_single_file_with_ai(file_path, code_text, issues, retry_count + 1)
            else:
                return AIResult(path=file_path, full_path=file_path, analysis="", error="ç©ºã®å¿œç­”")

        print(f"  [OK] AIè§£æå®Œäº† ({len(result)} æ–‡å­—)")
        return AIResult(path=file_path, full_path=file_path, analysis=result)

    except Exception as e:
        error_msg = str(e)[:200]
        if retry_count < AI_MAX_RETRIES:
            print(f"  [RETRY] ã‚¨ãƒ©ãƒ¼: {error_msg} ({retry_count + 1}/{AI_MAX_RETRIES})")
            time.sleep(2)
            return analyze_single_file_with_ai(file_path, code_text, issues, retry_count + 1)
        else:
            print(f"  [ERROR] AIè§£æå¤±æ•—: {error_msg}")
            return AIResult(path=file_path, full_path=file_path, analysis="", error=error_msg)

# ===== Two-Phase Analysis System =====
def analyze_all_files(index: List[Dict[str, Any]], query: str, topk: int) -> Tuple[List[RuleResult], List[AIResult]]:
    """å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã§è§£æã—ã€é‡è¦ãªã‚‚ã®ã‚’AIã§è©³ç´°è§£æ"""

    print("\n" + "="*60)
    print("ã€Phase 1ã€‘ ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹è§£æé–‹å§‹")
    print("="*60)

    # Phase 1: ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹è§£æ
    rule_results = []
    for i, doc in enumerate(index[:topk]):
        print(f"\r[{i+1}/{min(topk, len(index))}] {doc['path']}", end="")

        text = doc.get("text", "")
        issues_with_samples = analyze_with_rules(text)

        # å•é¡Œã®ã¿ã‚’æŠ½å‡º
        issues = [issue for issue, _, _ in issues_with_samples]
        severity_score = sum(SEVERITY_SCORES.get(issue, 1) for issue in issues)

        # ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ã¨æ”¹å–„æ¡ˆã‚’ã¾ã¨ã‚ã‚‹
        sample_codes = []
        suggested_fixes = []
        for issue, sample, fix in issues_with_samples[:3]:  # ä¸Šä½3ä»¶
            sample_codes.append(f"ã€{issue}ã€‘\n{sample}")
            suggested_fixes.append(f"ã€{issue}ã€‘\n{fix}")

        result = RuleResult(
            path=doc["path"],
            full_path=doc["path"],
            lang=doc.get("lang", "unknown"),
            encoding=doc.get("encoding", "unknown"),
            severity_score=severity_score,
            severity_level=get_severity_level(severity_score),
            issues=issues,
            tags=doc.get("tags", []),
            sample_code="\n\n".join(sample_codes) if sample_codes else "",
            suggested_fix="\n\n".join(suggested_fixes) if suggested_fixes else ""
        )
        rule_results.append(result)

    print(f"\n[OK] ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹è§£æå®Œäº†: {len(rule_results)}ãƒ•ã‚¡ã‚¤ãƒ«")

    # Phase 2: AIè©³ç´°è§£æï¼ˆé‡è¦åº¦ã®é«˜ã„ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ï¼‰
    print("\n" + "="*60)
    print("ã€Phase 2ã€‘ AIè©³ç´°è§£æé–‹å§‹")
    print("="*60)

    # é‡è¦åº¦ã§ã‚½ãƒ¼ãƒˆã—ã¦AIè§£æå¯¾è±¡ã‚’é¸å®š
    high_severity_results = [r for r in rule_results if r.severity_score >= AI_MIN_SEVERITY]
    high_severity_results.sort(key=lambda x: x.severity_score, reverse=True)
    ai_targets = high_severity_results[:AI_MAX_FILES]

    print(f"AIè§£æå¯¾è±¡: {len(ai_targets)}ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆé‡è¦åº¦{AI_MIN_SEVERITY}ä»¥ä¸Šï¼‰")

    ai_results = []
    for i, rule_result in enumerate(ai_targets):
        print(f"\nã€{i+1}/{len(ai_targets)}ã€‘ AIè§£æ")

        # å¯¾å¿œã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è¦‹ã¤ã‘ã‚‹
        doc = next((d for d in index if d["path"] == rule_result.path), None)
        if doc:
            ai_result = analyze_single_file_with_ai(
                rule_result.full_path,
                doc.get("text", ""),
                rule_result.issues
            )
            ai_results.append(ai_result)
        else:
            print(f"  [SKIP] ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

    print(f"\n[OK] AIè§£æå®Œäº†: {len(ai_results)}ãƒ•ã‚¡ã‚¤ãƒ«")

    return rule_results, ai_results

def get_severity_level(score: int) -> str:
    """ã‚¹ã‚³ã‚¢ã‹ã‚‰é‡è¦åº¦ãƒ¬ãƒ™ãƒ«ã‚’åˆ¤å®š"""
    if score >= 10:
        return "ğŸ”´ ç·Šæ€¥"
    elif score >= 7:
        return "ğŸŸ  é«˜"
    elif score >= 4:
        return "ğŸŸ¡ ä¸­"
    elif score >= 1:
        return "ğŸ”µ ä½"
    else:
        return "âšª å•é¡Œãªã—"

# ===== Report Generation =====
def generate_rule_based_report(query: str, rule_results: List[RuleResult]) -> str:
    """ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆæ”¹å–„æ¡ˆä»˜ãï¼‰"""
    lines = [
        f"# ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹è§£æãƒ¬ãƒãƒ¼ãƒˆ: {query}",
        "",
        f"ç”Ÿæˆ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        f"è§£æãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(rule_results)}",
        "",
        "## ğŸ“Š å•é¡Œã®åˆ†å¸ƒ",
    ]

    # é‡è¦åº¦åˆ¥ã«é›†è¨ˆ
    severity_groups = {
        "ğŸ”´ ç·Šæ€¥": [],
        "ğŸŸ  é«˜": [],
        "ğŸŸ¡ ä¸­": [],
        "ğŸ”µ ä½": [],
        "âšª å•é¡Œãªã—": []
    }

    for r in rule_results:
        severity_groups[r.severity_level].append(r)

    for level, items in severity_groups.items():
        lines.append(f"- {level}: {len(items)}ä»¶")

    lines.extend(["", "---", ""])

    # é‡è¦åº¦é †ã«å‡ºåŠ›
    sorted_results = sorted(rule_results, key=lambda x: x.severity_score, reverse=True)

    for i, result in enumerate(sorted_results, 1):
        lines.extend([
            f"## {i}. {result.path}",
            f"- **è¨€èª**: {result.lang}",
            f"- **ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°**: {result.encoding}",
            f"- **é‡è¦åº¦**: {result.severity_level} (ã‚¹ã‚³ã‚¢: {result.severity_score})",
            f"- **ã‚¿ã‚°**: {', '.join(result.tags) if result.tags else 'ãªã—'}",
            "",
            "### æ¤œå‡ºã•ã‚ŒãŸå•é¡Œ:",
        ])

        if result.issues:
            for issue in result.issues:
                severity = SEVERITY_SCORES.get(issue, 1)
                emoji = "ğŸ”´" if severity >= 8 else "ğŸŸ " if severity >= 5 else "ğŸŸ¡" if severity >= 3 else "ğŸ”µ"
                lines.append(f"- {emoji} {issue}")
        else:
            lines.append("- ãªã—")

        if result.sample_code:
            lines.extend([
                "",
                "### å•é¡Œç®‡æ‰€ã®ã‚³ãƒ¼ãƒ‰ä¾‹:",
                "```",
                result.sample_code,
                "```"
            ])

        if result.suggested_fix:
            lines.extend([
                "",
                "### ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹æ”¹å–„æ¡ˆ:",
                "```",
                result.suggested_fix,
                "```"
            ])

        lines.extend(["", "---", ""])

    return "\n".join(lines)

def generate_ai_enhanced_report(query: str, rule_results: List[RuleResult], ai_results: List[AIResult]) -> str:
    """AIæ”¹å–„æ¡ˆä»˜ããƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
    lines = [
        f"# AIæ”¹å–„æ¡ˆä»˜ãè§£æãƒ¬ãƒãƒ¼ãƒˆ: {query}",
        "",
        f"ç”Ÿæˆ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        f"ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹è§£æ: {len(rule_results)}ãƒ•ã‚¡ã‚¤ãƒ«",
        f"AIè©³ç´°è§£æ: {len(ai_results)}ãƒ•ã‚¡ã‚¤ãƒ«",
        "",
        "## ğŸ¤– AIè©³ç´°è§£æçµæœ",
        ""
    ]

    # AIè§£æçµæœã‚’å‡ºåŠ›
    for i, ai_result in enumerate(ai_results, 1):
        lines.extend([
            f"### [{i}/{len(ai_results)}] {ai_result.path}",
            ""
        ])

        if ai_result.error:
            lines.append(f"âš ï¸ **ã‚¨ãƒ©ãƒ¼**: {ai_result.error}")
        elif ai_result.analysis:
            lines.append(ai_result.analysis)
        else:
            lines.append("*è§£æçµæœãªã—*")

        lines.extend(["", "---", ""])

    # ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹çµæœã®ã‚µãƒãƒªãƒ¼
    lines.extend([
        "## ğŸ“‹ ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹è§£æã‚µãƒãƒªãƒ¼",
        ""
    ])

    # AIè§£æã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒ¼ã‚¯
    ai_analyzed_paths = {ar.path for ar in ai_results}

    high_severity = [r for r in rule_results if r.severity_score >= AI_MIN_SEVERITY]
    other = [r for r in rule_results if r.severity_score < AI_MIN_SEVERITY]

    lines.append("### ğŸ”¥ é«˜é‡è¦åº¦ï¼ˆAIè§£ææ¸ˆã¿ï¼‰")
    for r in high_severity[:AI_MAX_FILES]:
        mark = "âœ…" if r.path in ai_analyzed_paths else "âŒ"
        lines.append(f"- {mark} {r.path} ({r.severity_level}, ã‚¹ã‚³ã‚¢: {r.severity_score})")

    if other:
        lines.append("\n### ğŸ“ ãã®ä»–ã®ãƒ•ã‚¡ã‚¤ãƒ«")
        for r in other[:10]:  # æœ€å¤§10ä»¶
            lines.append(f"- {r.path} ({r.severity_level}, ã‚¹ã‚³ã‚¢: {r.severity_score})")

    return "\n".join(lines)

# ===== Commands =====
def cmd_query(query: str, topk: int, mode: str, index_path: pathlib.Path, out_base: Optional[str]):
    """æ¤œç´¢ã¨è§£æã‚’å®Ÿè¡Œ"""
    index = load_index(index_path)
    cands = retrieve(query, index, topk=topk)

    # 2æ®µéšè§£æã‚’å®Ÿè¡Œ
    rule_results, ai_results = analyze_all_files(cands, query, topk)

    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    rule_report = generate_rule_based_report(query, rule_results)
    ai_report = generate_ai_enhanced_report(query, rule_results, ai_results)

    # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
    if out_base:
        base_path = pathlib.Path(out_base)
        base_path.parent.mkdir(parents=True, exist_ok=True)

        # ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ãƒ¬ãƒãƒ¼ãƒˆ
        rule_path = base_path.parent / f"{base_path.stem}_rules.md"
        rule_path.write_text(rule_report, encoding="utf-8")
        print(f"\n[OK] ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ãƒ¬ãƒãƒ¼ãƒˆ: {rule_path}")

        # AIæ”¹å–„æ¡ˆä»˜ããƒ¬ãƒãƒ¼ãƒˆ
        ai_path = base_path.parent / f"{base_path.stem}_ai.md"
        ai_path.write_text(ai_report, encoding="utf-8")
        print(f"[OK] AIæ”¹å–„æ¡ˆä»˜ããƒ¬ãƒãƒ¼ãƒˆ: {ai_path}")
    else:
        print("\n" + "="*60)
        print("ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ãƒ¬ãƒãƒ¼ãƒˆ")
        print("="*60)
        print(rule_report[:2000] + "...")
        print("\n" + "="*60)
        print("AIæ”¹å–„æ¡ˆä»˜ããƒ¬ãƒãƒ¼ãƒˆ")
        print("="*60)
        print(ai_report[:2000] + "...")

def cmd_advise(topk: int, mode: str, index_path: pathlib.Path, out_base: Optional[str]):
    """å…¨ä½“åŠ©è¨€ã‚’å®Ÿè¡Œ"""
    seed_query = "é‡‘é¡ ä¸æ•´åˆ ç¨ å°æ•° å°åˆ· Print UI UX DB N+1 JOIN"
    cmd_query(seed_query, topk, mode, index_path, out_base)

# ===== CLI =====
if __name__ == "__main__":
    ap = argparse.ArgumentParser(description="ç©¶æ¥µç‰ˆã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ„ãƒ¼ãƒ«ï¼ˆ2æ®µéšè§£æï¼‰")
    sub = ap.add_subparsers(dest="cmd", required=True)

    ap_idx = sub.add_parser("index", help="ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–")
    ap_idx.add_argument("repo", type=str)
    ap_idx.add_argument("--exclude-langs", type=str, nargs="*", help="é™¤å¤–ã™ã‚‹è¨€èª")
    ap_idx.add_argument("--max-file-mb", type=float, default=4.0, help="æœ€å¤§ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º(MB)")

    ap_vec = sub.add_parser("vectorize", help="TF-IDFãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆ")
    ap_vec.add_argument("--index", type=str, default=INDEX_PATH)

    ap_q = sub.add_parser("query", help="æ¤œç´¢ã¨2æ®µéšè§£æ")
    ap_q.add_argument("query", type=str, help="æ¤œç´¢ã‚¯ã‚¨ãƒª")
    ap_q.add_argument("--topk", type=int, default=50, help="è§£æå¯¾è±¡ä¸Šä½Nä»¶")
    ap_q.add_argument("--mode", choices=["rules","ai","hybrid"], default="hybrid")
    ap_q.add_argument("--index", type=str, default=INDEX_PATH)
    ap_q.add_argument("--out", type=str, help="å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹åï¼ˆ_rules.md ã¨ _ai.md ãŒç”Ÿæˆã•ã‚Œã‚‹ï¼‰")

    ap_adv = sub.add_parser("advise", help="å…¨ä½“åŠ©è¨€ï¼ˆ2æ®µéšè§£æï¼‰")
    ap_adv.add_argument("--topk", type=int, default=100, help="è§£æå¯¾è±¡ä¸Šä½Nä»¶")
    ap_adv.add_argument("--mode", choices=["rules","ai","hybrid"], default="hybrid")
    ap_adv.add_argument("--index", type=str, default=INDEX_PATH)
    ap_adv.add_argument("--out", type=str, help="å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹å")

    args = ap.parse_args()

    if args.cmd == "index":
        repo = pathlib.Path(args.repo)
        exclude_langs = set(args.exclude_langs) if args.exclude_langs else set()
        max_file_bytes = int(args.max_file_mb * 1_000_000)
        cmd_index(repo, pathlib.Path(INDEX_PATH), exclude_langs, max_file_bytes)

    elif args.cmd == "vectorize":
        cmd_vectorize(pathlib.Path(args.index))

    elif args.cmd == "query":
        cmd_query(args.query, args.topk, args.mode, pathlib.Path(args.index), args.out)

    elif args.cmd == "advise":
        cmd_advise(args.topk, args.mode, pathlib.Path(args.index), args.out)