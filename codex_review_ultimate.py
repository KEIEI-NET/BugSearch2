#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
codex_review_ultimate.py â€” æœ€çµ‚ç‰ˆï¼š2æ®µéšè§£æã‚·ã‚¹ãƒ†ãƒ 

å‡¦ç†ãƒ•ãƒ­ãƒ¼:
  1) ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã§å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é«˜é€Ÿè§£æ
  2) é‡è¦åº¦ã®é«˜ã„å•é¡Œã‚’æŒã¤ãƒ•ã‚¡ã‚¤ãƒ«ã‚’AIã§è©³ç´°è§£æï¼ˆ1ãƒ•ã‚¡ã‚¤ãƒ«ãšã¤ï¼‰
  3) 2ç¨®é¡ã®ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ç‰ˆ / AIæ”¹å–„æ¡ˆä»˜ãç‰ˆï¼‰

ä¸»ãªç‰¹å¾´:
  - ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è‡ªå‹•æ¤œå‡º
  - é€²æ—è¡¨ç¤ºï¼ˆXX/YYãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ä¸­ï¼‰
  - ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¯¾ç­–ï¼ˆå€‹åˆ¥å‡¦ç†ã€ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ï¼‰
  - å…·ä½“çš„ãªæ”¹å–„ã‚³ãƒ¼ãƒ‰ä¾‹ã®æç¤º

pip install chromadb openai scikit-learn joblib regex chardet
"""
from __future__ import annotations
import argparse, hashlib, json, os, pathlib, re, sys, time
from collections import defaultdict
from dataclasses import dataclass, asdict, field
from fnmatch import fnmatch
from typing import Any, Dict, List, Tuple, Optional
import chardet
from datetime import datetime

# ===== Config =====
IGNORE_DIRS = {".git","node_modules","dist","build","out","bin","obj",".idea",".vscode",".next","coverage","target",".venv","venv","env","lib64","lib","__pycache__",".mypy_cache",".pytest_cache",".tox"}
DEFAULT_MAX_FILE_BYTES = 4_000_000
ENV_FILE = ".env"
INDEX_PATH = ".advice_index.jsonl"
VEC_PATH = ".advice_tfidf.pkl"
MATRIX_PATH = ".advice_matrix.pkl"
AI_TIMEOUT = 60  # å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
AI_MAX_RETRIES = 2  # AIãƒªãƒˆãƒ©ã‚¤å›æ•°
AI_MIN_SEVERITY = 7  # AIè§£æã™ã‚‹æœ€å°é‡è¦åº¦ã‚¹ã‚³ã‚¢
AI_MAX_FILES = 20  # AIè§£æã™ã‚‹æœ€å¤§ãƒ•ã‚¡ã‚¤ãƒ«æ•°
BATCH_SIZE_DEFAULT = 500

# é‡è¦åº¦ã‚¹ã‚³ã‚¢å®šç¾©
SEVERITY_SCORES = {
    "DB: ãƒ«ãƒ¼ãƒ—å†…SELECT (N+1) ç–‘ã„": 10,
    "DB: SELECT * â†’è² è·å¢—ã€‚åˆ—é™å®š": 8,
    "DB: å¤šé‡JOINâ†’é…å»¶ã®æã‚Œã€‚è¨ˆç”»/IDXç¢ºèª": 7,
    "DB: å¤§OFFSETâ†’é…å»¶ã€‚IDã‚«ãƒ¼ã‚½ãƒ«æ¨å¥¨": 6,
    "é‡‘é¡: æµ®å‹•å°æ•°ã§é‡‘é¡â†’èª¤å·®ã€‚Decimal/é€šè²¨å‹ã¸": 9,
    "é‡‘é¡: ç¨è¾¼/ç¨æŠœãƒ»ç«¯æ•°å‡¦ç†ã®æ··åœ¨æ³¨æ„": 7,
    "UI: XSSè„†å¼±æ€§ã®ç–‘ã„ï¼ˆæœªã‚µãƒ‹ã‚¿ã‚¤ã‚ºHTMLï¼‰": 8,
    "UI: å…¥åŠ›æ¤œè¨¼ãŒä¸ååˆ†": 5,
    "UI: å¤šé‡ã‚¯ãƒªãƒƒã‚¯é˜²æ­¢ãªã—": 3,
    "å°åˆ·: ã‚¨ãƒ©ãƒ¼å‡¦ç†ãªã—â†’é€”ä¸­åœæ­¢ãƒªã‚¹ã‚¯": 4,
    "å°åˆ·: æ”¹ãƒšãƒ¼ã‚¸åˆ¤å®šãŒä¸ååˆ†": 2,
}

# ===== Optional deps =====
SKLEARN_OK = False
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    import joblib
    SKLEARN_OK = True
except:
    pass

OPENAI_OK = False
try:
    from openai import OpenAI
    OPENAI_OK = True
except:
    pass

# ===== Utils =====
if ENV_FILE and pathlib.Path(ENV_FILE).exists():
    with open(ENV_FILE, "r", encoding="utf-8") as f:
        for line in f:
            if "=" in line:
                key, val = line.strip().split("=", 1)
                os.environ[key] = val

@dataclass
class Doc:
    path: str
    lang: str
    size: int
    sha1: str
    tags: List[str]
    summary: str
    text: str
    encoding: str

@dataclass
class RuleResult:
    """ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹è§£æçµæœ"""
    path: str
    full_path: str
    lang: str
    encoding: str
    severity_score: int
    severity_level: str
    issues: List[str]
    tags: List[str]
    sample_code: str = ""  # å•é¡Œç®‡æ‰€ã®ã‚³ãƒ¼ãƒ‰ã‚µãƒ³ãƒ—ãƒ«
    suggested_fix: str = ""  # ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®æ”¹å–„æ¡ˆ

@dataclass
class AIResult:
    """AIè§£æçµæœ"""
    path: str
    full_path: str
    analysis: str
    improvements: List[Dict[str, str]] = field(default_factory=list)
    error: Optional[str] = None


@dataclass
class IndexStats:
    enabled: bool
    total_start: float = 0.0
    counts: Dict[str, int] = None
    timings: Dict[str, float] = None

    def __post_init__(self) -> None:
        if self.enabled:
            self.total_start = time.perf_counter()
            self.counts = defaultdict(int)
            self.timings = defaultdict(float)
        else:
            self.counts = defaultdict(int)
            self.timings = defaultdict(float)

    def bump(self, key: str, delta: int = 1) -> None:
        if self.enabled:
            self.counts[key] += delta

    def add_time(self, key: str, duration: float) -> None:
        if self.enabled:
            self.timings[key] += duration

    def render_summary(self) -> Optional[str]:
        if not self.enabled:
            return None
        total_elapsed = time.perf_counter() - self.total_start
        indexed = self.counts.get("indexed", 0)
        seen = self.counts.get("seen", 0)
        skipped_large = self.counts.get("skipped_large", 0)
        skipped_errors = self.counts.get("skipped_errors", 0)
        skipped_filter = self.counts.get("skipped_filter", 0)
        limit_stop = self.counts.get("limit_stop", 0)
        timeout_stop = self.counts.get("timeout_stop", 0)
        avg_read = (self.timings.get("read", 0.0) / indexed) if indexed else 0.0
        return (
            f"[PROFILE] Indexed {indexed}/{seen} files in {total_elapsed:.2f}s\n"
            f"           read={self.timings.get('read', 0.0):.2f}s stat={self.timings.get('stat', 0.0):.2f}s write={self.timings.get('write', 0.0):.2f}s\n"
            f"           avg_read_per_file={avg_read*1000:.1f}ms large_skipped={skipped_large} filter_skipped={skipped_filter} errors={skipped_errors} limits={limit_stop} timeouts={timeout_stop}"
        )

    def to_rows(self) -> List[Dict[str, Any]]:
        if not self.enabled:
            return []
        return [{
            "total_seconds": time.perf_counter() - self.total_start,
            "indexed_files": self.counts.get("indexed", 0),
            "seen_files": self.counts.get("seen", 0),
            "skipped_large": self.counts.get("skipped_large", 0),
            "skipped_errors": self.counts.get("skipped_errors", 0),
            "skipped_filter": self.counts.get("skipped_filter", 0),
            "limit_stop": self.counts.get("limit_stop", 0),
            "timeout_stop": self.counts.get("timeout_stop", 0),
            "read_seconds": self.timings.get("read", 0.0),
            "stat_seconds": self.timings.get("stat", 0.0),
            "write_seconds": self.timings.get("write", 0.0)
        }]

    def export(self, path: pathlib.Path) -> None:
        if not self.enabled:
            return
        rows = self.to_rows()
        if not rows:
            return
        path.parent.mkdir(parents=True, exist_ok=True)
        if path.suffix.lower() == ".csv":
            import csv
            with path.open("w", encoding="utf-8", newline="") as fh:
                writer = csv.DictWriter(fh, fieldnames=list(rows[0].keys()))
                writer.writeheader()
                writer.writerows(rows)
        else:
            path.write_text("\n".join(json.dumps(r, ensure_ascii=False) for r in rows) + "\n", encoding="utf-8")

def detect_encoding(file_path: pathlib.Path) -> str:
    """ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è‡ªå‹•æ¤œå‡º"""
    try:
        with open(file_path, 'rb') as f:
            raw_data = f.read(10000)
            result = chardet.detect(raw_data)
            encoding = result['encoding']
            confidence = result.get('confidence', 0)

            if not encoding or confidence < 0.7:
                for enc in ['utf-8', 'shift_jis', 'cp932', 'euc-jp', 'iso-2022-jp']:
                    try:
                        with open(file_path, 'r', encoding=enc) as test_f:
                            test_f.read(1000)
                        return enc
                    except:
                        continue
                return 'utf-8'

            if encoding.lower() in ['cp932', 'shift_jis', 'shift-jis', 'sjis']:
                return 'cp932'

            return encoding.lower()
    except:
        return 'utf-8'

def read_file_with_encoding(file_path: pathlib.Path) -> Tuple[str, str]:
    """ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è‡ªå‹•æ¤œå‡ºã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
    encoding = detect_encoding(file_path)

    try:
        with open(file_path, 'r', encoding=encoding, errors='ignore') as f:
            content = f.read()
        return content, encoding
    except:
        try:
            with open(file_path, 'rb') as f:
                raw_data = f.read()
            content = raw_data.decode(encoding, errors='ignore')
            return content, encoding
        except:
            return "", "unknown"

def sha1_bytes(data: bytes) -> str:
    return hashlib.sha1(data).hexdigest()[:10]

def detect_lang(p: pathlib.Path) -> str:
    ext = p.suffix.lower()
    if ext in (".py",): return "python"
    if ext in (".js",".mjs",".cjs",".jsx"): return "javascript"
    if ext in (".ts",".tsx"): return "typescript"
    if ext in (".cs",".csx"): return "csharp"
    if ext in (".java",): return "java"
    if ext in (".pas",".dfm",".dpr"): return "delphi"
    if ext in (".rb",): return "ruby"
    if ext in (".go",): return "go"
    if ext in (".rs",): return "rust"
    if ext in (".php",): return "php"
    if ext in (".c",".h"): return "c"
    if ext in (".cpp",".hpp",".cc",".hh",".cxx",".hxx"): return "cpp"
    return "other"

def is_text_file(p: pathlib.Path) -> bool:
    try:
        with open(p, "rb") as f:
            return b"\x00" not in f.read(4096)
    except Exception:
        return False

def make_summary(text: str) -> str:
    lines = text.splitlines()[:10]
    return " / ".join(l.strip()[:80] for l in lines if l.strip())[:400]

# ===== Rule-based Analysis with Code Samples =====
def scan_money_with_sample(text: str) -> List[Tuple[str, str, str]]:
    """é‡‘é¡é–¢é€£ã®å•é¡Œã‚’æ¤œå‡ºã—ã€ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ã¨æ”¹å–„æ¡ˆã‚’è¿”ã™"""
    results = []

    # æµ®å‹•å°æ•°ç‚¹ã§ã®é‡‘é¡è¨ˆç®—
    pattern = r"(\b(?:float|double|real)\b.*\b(?:price|cost|money|amount|tax|total|sum|charge|fee|pay).*)"
    matches = re.finditer(pattern, text, re.IGNORECASE)
    for match in matches:
        sample = match.group(0)[:200]
        issue = "é‡‘é¡: æµ®å‹•å°æ•°ã§é‡‘é¡â†’èª¤å·®ã€‚Decimal/é€šè²¨å‹ã¸"
        fix = """
// Before:
float price = 100.0f;
float tax = price * 0.1f;  // ç²¾åº¦å•é¡Œ

// After:
decimal price = 100.0m;
decimal tax = price * 0.1m;  // æ­£ç¢ºãªè¨ˆç®—"""
        results.append((issue, sample, fix))
        break  # æœ€åˆã®1ä»¶ã®ã¿

    # ç¨è¨ˆç®—ã®ç«¯æ•°å‡¦ç†
    if re.search(r"\b(total|sum|amount)\s*=.*\+.*tax", text, re.IGNORECASE):
        pattern = r"(.*(?:total|sum|amount)\s*=.*\+.*tax.*)"
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            sample = match.group(0)[:200]
            issue = "é‡‘é¡: ç¨è¾¼/ç¨æŠœãƒ»ç«¯æ•°å‡¦ç†ã®æ··åœ¨æ³¨æ„"
            fix = """
// çµ±ä¸€ã—ãŸç«¯æ•°å‡¦ç†
decimal CalculateTotalWithTax(decimal price, decimal taxRate) {
    decimal tax = Math.Round(price * taxRate, 0, MidpointRounding.ToEven);
    return price + tax;
}"""
            results.append((issue, sample, fix))

    return results

def scan_db_with_sample(text: str) -> List[Tuple[str, str, str]]:
    """DBé–¢é€£ã®å•é¡Œã‚’æ¤œå‡ºã—ã€ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ã¨æ”¹å–„æ¡ˆã‚’è¿”ã™"""
    results = []

    # SELECT * ã®ä½¿ç”¨
    pattern = r"(.*SELECT\s+\*\s+FROM.*)"
    matches = re.finditer(pattern, text, re.IGNORECASE)
    for match in matches:
        sample = match.group(0)[:200]
        issue = "DB: SELECT * â†’è² è·å¢—ã€‚åˆ—é™å®š"
        fix = """
// Before:
SELECT * FROM Users WHERE id = @id

// After:
SELECT id, name, email FROM Users WHERE id = @id"""
        results.append((issue, sample, fix))
        break

    # N+1å•é¡Œ
    if re.search(r"(for|while|foreach).*\n.*\n.*\n.*(SELECT|INSERT|UPDATE|DELETE)", text, re.IGNORECASE):
        issue = "DB: ãƒ«ãƒ¼ãƒ—å†…SELECT (N+1) ç–‘ã„"
        sample = "ãƒ«ãƒ¼ãƒ—å†…ã§SQLã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œã—ã¦ã„ã‚‹å¯èƒ½æ€§"
        fix = """
// Before:
foreach(var userId in userIds) {
    var user = db.Query("SELECT * FROM Users WHERE id = " + userId);
}

// After:
var users = db.Query("SELECT * FROM Users WHERE id IN @userIds", new { userIds });"""
        results.append((issue, sample, fix))

    return results

def scan_ui_with_sample(text: str) -> List[Tuple[str, str, str]]:
    """UIé–¢é€£ã®å•é¡Œã‚’æ¤œå‡ºã—ã€ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ã¨æ”¹å–„æ¡ˆã‚’è¿”ã™"""
    results = []

    # XSSè„†å¼±æ€§
    if re.search(r"(innerHTML\s*=|dangerouslySetInnerHTML)", text):
        issue = "UI: XSSè„†å¼±æ€§ã®ç–‘ã„ï¼ˆæœªã‚µãƒ‹ã‚¿ã‚¤ã‚ºHTMLï¼‰"
        sample = "innerHTML ã¾ãŸã¯ dangerouslySetInnerHTML ã®ä½¿ç”¨"
        fix = """
// Before:
element.innerHTML = userInput;

// After:
element.textContent = userInput;  // ã¾ãŸã¯ã‚µãƒ‹ã‚¿ã‚¤ã‚º
element.innerHTML = DOMPurify.sanitize(userInput);"""
        results.append((issue, sample, fix))

    # å…¥åŠ›æ¤œè¨¼ä¸è¶³
    if re.search(r"(input|textarea|select)", text, re.IGNORECASE) and not re.search(r"(validate|pattern|required)", text, re.IGNORECASE):
        issue = "UI: å…¥åŠ›æ¤œè¨¼ãŒä¸ååˆ†"
        sample = "å…¥åŠ›ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«æ¤œè¨¼ãŒè¦‹å½“ãŸã‚‰ãªã„"
        fix = """
// å…¥åŠ›æ¤œè¨¼ã®è¿½åŠ 
<input type="email" required pattern="[a-z0-9._%+-]+@[a-z0-9.-]+\\.[a-z]{2,}$" />

// JSã§ã®æ¤œè¨¼
if (!input.value.match(/^[a-z0-9._%+-]+@[a-z0-9.-]+\\.[a-z]{2,}$/i)) {
    showError("æœ‰åŠ¹ãªãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„");
}"""
        results.append((issue, sample, fix))

    return results

def scan_print_with_sample(text: str) -> List[Tuple[str, str, str]]:
    """å°åˆ·é–¢é€£ã®å•é¡Œã‚’æ¤œå‡ºã—ã€ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ã¨æ”¹å–„æ¡ˆã‚’è¿”ã™"""
    results = []

    if re.search(r"(PrintDocument|PrintPage)", text, re.IGNORECASE) and not re.search(r"(try|catch|error)", text, re.IGNORECASE):
        issue = "å°åˆ·: ã‚¨ãƒ©ãƒ¼å‡¦ç†ãªã—â†’é€”ä¸­åœæ­¢ãƒªã‚¹ã‚¯"
        sample = "å°åˆ·å‡¦ç†ã«ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãŒãªã„"
        fix = """
// Before:
printDocument.Print();

// After:
try {
    printDocument.Print();
} catch (Exception ex) {
    Logger.LogError($"å°åˆ·ã‚¨ãƒ©ãƒ¼: {ex.Message}");
    MessageBox.Show("å°åˆ·ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚");
}"""
        results.append((issue, sample, fix))

    return results

def analyze_with_rules(text: str) -> List[Tuple[str, str, str]]:
    """ã™ã¹ã¦ã®ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹è§£æã‚’å®Ÿè¡Œ"""
    all_results = []
    all_results.extend(scan_money_with_sample(text))
    all_results.extend(scan_db_with_sample(text))
    all_results.extend(scan_ui_with_sample(text))
    all_results.extend(scan_print_with_sample(text))
    return all_results

def make_tags(text: str) -> List[str]:
    t = []
    if re.search(r"\b(price|cost|money|amount|tax|total|sum|charge|fee|pay)", text, re.IGNORECASE):
        t.append("money")
    if re.search(r"\b(print|printer|page|paper|report|pdf|export|PrintDocument)", text, re.IGNORECASE):
        t.append("print")
    if re.search(r"\b(button|click|submit|form|input|select|modal|dialog)", text, re.IGNORECASE):
        t.append("uiux")
    if re.search(r"\b(SELECT|INSERT|UPDATE|DELETE|JOIN|WHERE|FROM|query|sql)", text, re.IGNORECASE):
        t.append("db")
    if re.search(r"\b(api|http|fetch|axios|request|response|endpoint|REST)", text, re.IGNORECASE):
        t.append("net")
    if re.search(r"\b(file|fs|stream|read|write|upload|download|path)", text, re.IGNORECASE):
        t.append("io")
    return sorted(set(t))

# ===== Indexer =====
def should_index(p: pathlib.Path, exclude_langs: set) -> bool:
    if p.is_dir(): return False
    if any(part.startswith(".") for part in p.parts): return False
    lang = detect_lang(p)
    if lang in exclude_langs:
        return False
    if lang == "other":
        important_ext = {".xml", ".json", ".yaml", ".yml", ".toml", ".ini", ".config", ".md", ".txt", ".html", ".htm", ".css"}
        if p.suffix.lower() in important_ext: return True
        if p.name in {"Dockerfile", "Makefile", "Rakefile", "Gemfile", ".env", ".env.example"}: return True
        return False
    return True

def normalize_rel_path(repo: pathlib.Path, path: pathlib.Path) -> str:
    """Normalize a file path relative to repo for pattern matching"""
    try:
        rel = path.relative_to(repo)
    except ValueError:
        rel = path
    return str(rel).replace(os.sep, "/")

def match_patterns(rel_path: str, patterns: Optional[List[str]]) -> bool:
    """Check if a relative path matches any of the glob patterns"""
    if not patterns:
        return False
    return any(fnmatch(rel_path, pat) for pat in patterns)

def cmd_index(
    repo: pathlib.Path,
    index_path: pathlib.Path,
    exclude_langs: set = None,
    max_file_bytes: int = None,
    *,
    profile: bool = False,
    profile_output: pathlib.Path | None = None,
    batch_size: int | None = None,
    max_files: int | None = None,
    max_seconds: float | None = None,
    include_patterns: Optional[List[str]] = None,
    exclude_patterns: Optional[List[str]] = None,
):
    if exclude_langs is None:
        exclude_langs = set()
    if max_file_bytes is None:
        max_file_bytes = DEFAULT_MAX_FILE_BYTES

    repo = repo.resolve()
    paths = []
    large_files = []
    stats = IndexStats(enabled=profile)
    norm_include = include_patterns or []
    norm_exclude = exclude_patterns or []
    start_time = time.perf_counter()
    batch_size = batch_size if batch_size and batch_size > 0 else None
    batch_buffer: List[str] = []
    count = 0

    for root, dirs, files in os.walk(repo):
        dirs[:] = [d for d in dirs if d not in IGNORE_DIRS]
        root_path = pathlib.Path(root)
        for file in files:
            p = root_path / file
            stats.bump("seen")
            rel_norm = normalize_rel_path(repo, p)
            if norm_include and not match_patterns(rel_norm, norm_include):
                stats.bump("skipped_filter")
                continue
            if match_patterns(rel_norm, norm_exclude):
                stats.bump("skipped_filter")
                continue
            try:
                stat_start = time.perf_counter()
                file_size = p.stat().st_size
                stats.add_time("stat", time.perf_counter() - stat_start)
            except (OSError, PermissionError):
                stats.bump("skipped_errors")
                continue
            if file_size > max_file_bytes:
                large_files.append((str(p.relative_to(repo)), file_size))
                stats.bump("skipped_large")
                continue
            if not should_index(p, exclude_langs):
                continue
            if not is_text_file(p):
                stats.bump("skipped_errors")
                continue
            paths.append(p)

    total_files = len(paths)
    print(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {total_files}")

    with open(index_path, "w", encoding="utf-8") as w:
        for idx, p in enumerate(sorted(paths)):
            if idx % 100 == 0:
                print(f"å‡¦ç†ä¸­... {idx}/{total_files} ãƒ•ã‚¡ã‚¤ãƒ«")
            lang = detect_lang(p)
            try:
                read_start = time.perf_counter()
                txt, encoding = read_file_with_encoding(p)
                stats.add_time("read", time.perf_counter() - read_start)
                if not txt:
                    continue
                tags = make_tags(txt)
                rel_path = str(p.relative_to(repo))
                encoded = txt.encode("utf-8", errors="ignore")
                doc = Doc(
                    path=rel_path,
                    lang=lang,
                    size=len(encoded),
                    sha1=sha1_bytes(encoded),
                    tags=tags,
                    summary=make_summary(txt),
                    text=txt,
                    encoding=encoding
                )
                batch_buffer.append(json.dumps(asdict(doc), ensure_ascii=False) + "\n")
                stats.bump("indexed")
                count += 1
                if batch_size and len(batch_buffer) >= batch_size:
                    write_start = time.perf_counter()
                    w.writelines(batch_buffer)
                    stats.add_time("write", time.perf_counter() - write_start)
                    batch_buffer.clear()
                    print(f"[INFO] Indexed {count} files...")
                if max_files and count >= max_files:
                    stats.bump("limit_stop")
                    break
                if max_seconds and (time.perf_counter() - start_time) >= max_seconds:
                    stats.bump("timeout_stop")
                    break
            except Exception as e:
                print(f"[ERROR] Failed to process {p}: {e}")
                stats.bump("skipped_errors")
                continue
        if batch_buffer:
            write_start = time.perf_counter()
            w.writelines(batch_buffer)
            stats.add_time("write", time.perf_counter() - write_start)
            batch_buffer.clear()

    print(f"[OK] Indexed {count} files -> {index_path}")
    if large_files:
        reports_dir = index_path.parent / "reports"
        reports_dir.mkdir(parents=True, exist_ok=True)
        log_path = reports_dir / "large_files_over_limit.log"
        with open(log_path, "w", encoding="utf-8") as f:
            f.write(f"Large files exceeding limit ({max_file_bytes:,} bytes):\n\n")
            for file_path, size in sorted(large_files, key=lambda x: x[1], reverse=True):
                f.write(f"{size:12,} bytes  {file_path}\n")
            f.write(f"\nTotal: {len(large_files)} files\n")
        threshold_mb = max_file_bytes / 1_000_000
        print(f"[WARNING] Skipped {len(large_files)} files exceeding ~{threshold_mb:.1f} MB. Details: {log_path}")
    if stats.counts.get("limit_stop"):
        print("[INFO] Stopped due to --max-files limit")
    if stats.counts.get("timeout_stop"):
        print("[WARNING] Stopped due to --max-seconds timeout")

    summary = stats.render_summary()
    if summary:
        print(summary)
        if profile_output:
            stats.export(profile_output)

# ===== Retrieval =====
def load_index(index_path: pathlib.Path) -> List[Dict[str,Any]]:
    arr: List[Dict[str,Any]] = []
    with open(index_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line:
                arr.append(json.loads(line))
    return arr

def retrieve(query: str, index: List[Dict[str,Any]], topk: int = 10) -> List[Dict[str,Any]]:
    if SKLEARN_OK:
        vec_p = pathlib.Path(VEC_PATH)
        mat_p = pathlib.Path(MATRIX_PATH)
        if vec_p.exists() and mat_p.exists():
            return retrieve_tfidf(query, index, vec_p, mat_p, topk)
    return retrieve_naive(query, index, topk)

def retrieve_naive(query: str, index: List[Dict[str,Any]], topk: int) -> List[Dict[str,Any]]:
    q_words = set(w.lower() for w in re.split(r"\W+", query) if w)
    for doc in index:
        txt_words = set(w.lower() for w in re.split(r"\W+", doc.get("text","")[:3000]) if w)
        doc["_score"] = len(q_words & txt_words) / (len(q_words) + 0.01)
    return sorted(index, key=lambda x: x.get("_score",0), reverse=True)[:topk]

def retrieve_tfidf(query: str, index: List[Dict[str,Any]], vec_path: pathlib.Path, mat_path: pathlib.Path, topk: int) -> List[Dict[str,Any]]:
    from sklearn.metrics.pairwise import cosine_similarity
    vec = joblib.load(vec_path)
    mat = joblib.load(mat_path)
    q_vec = vec.transform([query])
    scores = cosine_similarity(q_vec, mat).flatten()
    for i, doc in enumerate(index):
        if i < len(scores):
            doc["_score"] = float(scores[i])
    return sorted(index, key=lambda x: x.get("_score",0), reverse=True)[:topk]

# ===== Vectorization =====
def cmd_vectorize(index_path: pathlib.Path):
    if not SKLEARN_OK:
        print("scikit-learnæœªå°å…¥ã€‚pip install scikit-learn joblib")
        return
    index = load_index(index_path)
    texts = [d.get("text","")[:10000] for d in index]
    vec = TfidfVectorizer(max_features=5000, ngram_range=(1,2), stop_words=None)
    mat = vec.fit_transform(texts)
    joblib.dump(vec, VEC_PATH)
    joblib.dump(mat, MATRIX_PATH)
    print(f"[OK] Vectorized -> {VEC_PATH}, {MATRIX_PATH}")

# ===== AI Analysis (1 file at a time) =====
def analyze_single_file_with_ai(file_path: str, code_text: str, issues: List[str], retry_count: int = 0) -> AIResult:
    """å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’AIã§è©³ç´°è§£æ"""
    if not OPENAI_OK:
        return AIResult(path=file_path, full_path=file_path, analysis="", error="OpenAIæœªå°å…¥")

    api_key = os.environ.get("OPENAI_API_KEY")
    if not api_key:
        return AIResult(path=file_path, full_path=file_path, analysis="", error="API keyæœªè¨­å®š")

    # é€²æ—è¡¨ç¤º
    print(f"  è§£æä¸­: {file_path}")
    print(f"  æ¤œå‡ºå•é¡Œ: {', '.join(issues[:3])}")

    client = OpenAI(api_key=api_key)
    model = "gpt-4o"  # GPT-5ãŒç©ºå¿œç­”ã®ãŸã‚gpt-4oä½¿ç”¨

    # ã‚³ãƒ¼ãƒ‰ã®ä¸€éƒ¨ã‚’æŠ½å‡ºï¼ˆæœ€å¤§5000æ–‡å­—ï¼‰
    code_snippet = code_text[:5000]

    prompt = f"""
ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è©³ç´°ã«åˆ†æã—ã€å…·ä½“çš„ãªæ”¹å–„æ¡ˆã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚

ãƒ•ã‚¡ã‚¤ãƒ«: {file_path}
æ¤œå‡ºã•ã‚ŒãŸå•é¡Œ: {json.dumps(issues, ensure_ascii=False)}

ã‚³ãƒ¼ãƒ‰:
```
{code_snippet}
```

ä»¥ä¸‹ã®å½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ï¼š

## å•é¡Œç®‡æ‰€ã®ç‰¹å®š
- è¡Œç•ªå·ã¾ãŸã¯é–¢æ•°å
- å•é¡Œã®è©³ç´°èª¬æ˜

## æ”¹å–„æ¡ˆ
### å•é¡Œ1: [å•é¡Œå]
**Before:**
```
[ç¾åœ¨ã®ã‚³ãƒ¼ãƒ‰]
```

**After:**
```
[æ”¹å–„ã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰]
```

**èª¬æ˜:** ãªãœã“ã®æ”¹å–„ãŒå¿…è¦ã‹

é‡è¦: å®Ÿéš›ã«å‹•ä½œã™ã‚‹å…·ä½“çš„ãªã‚³ãƒ¼ãƒ‰ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
"""

    try:
        resp = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "You are an expert code reviewer. Provide specific improvements in Japanese."},
                {"role": "user", "content": prompt}
            ],
            temperature=1,
            max_completion_tokens=2000,
            timeout=AI_TIMEOUT
        )

        result = resp.choices[0].message.content
        if not result or result.strip() == "":
            if retry_count < AI_MAX_RETRIES:
                print(f"  [RETRY] ç©ºã®å¿œç­”ã®ãŸã‚å†è©¦è¡Œ ({retry_count + 1}/{AI_MAX_RETRIES})")
                time.sleep(2)
                return analyze_single_file_with_ai(file_path, code_text, issues, retry_count + 1)
            else:
                return AIResult(path=file_path, full_path=file_path, analysis="", error="ç©ºã®å¿œç­”")

        print(f"  [OK] AIè§£æå®Œäº† ({len(result)} æ–‡å­—)")
        return AIResult(path=file_path, full_path=file_path, analysis=result)

    except Exception as e:
        error_msg = str(e)[:200]
        if retry_count < AI_MAX_RETRIES:
            print(f"  [RETRY] ã‚¨ãƒ©ãƒ¼: {error_msg} ({retry_count + 1}/{AI_MAX_RETRIES})")
            time.sleep(2)
            return analyze_single_file_with_ai(file_path, code_text, issues, retry_count + 1)
        else:
            print(f"  [ERROR] AIè§£æå¤±æ•—: {error_msg}")
            return AIResult(path=file_path, full_path=file_path, analysis="", error=error_msg)

# ===== Two-Phase Analysis System =====
def analyze_all_files(index: List[Dict[str, Any]], query: str, topk: int) -> Tuple[List[RuleResult], List[AIResult]]:
    """å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã§è§£æã—ã€é‡è¦ãªã‚‚ã®ã‚’AIã§è©³ç´°è§£æ"""

    print("\n" + "="*60)
    print("ã€Phase 1ã€‘ ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹è§£æé–‹å§‹")
    print("="*60)

    # Phase 1: ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹è§£æ
    rule_results = []
    for i, doc in enumerate(index[:topk]):
        print(f"\r[{i+1}/{min(topk, len(index))}] {doc['path']}", end="")

        text = doc.get("text", "")
        issues_with_samples = analyze_with_rules(text)

        # å•é¡Œã®ã¿ã‚’æŠ½å‡º
        issues = [issue for issue, _, _ in issues_with_samples]
        severity_score = sum(SEVERITY_SCORES.get(issue, 1) for issue in issues)

        # ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ã¨æ”¹å–„æ¡ˆã‚’ã¾ã¨ã‚ã‚‹
        sample_codes = []
        suggested_fixes = []
        for issue, sample, fix in issues_with_samples[:3]:  # ä¸Šä½3ä»¶
            sample_codes.append(f"ã€{issue}ã€‘\n{sample}")
            suggested_fixes.append(f"ã€{issue}ã€‘\n{fix}")

        result = RuleResult(
            path=doc["path"],
            full_path=doc["path"],
            lang=doc.get("lang", "unknown"),
            encoding=doc.get("encoding", "unknown"),
            severity_score=severity_score,
            severity_level=get_severity_level(severity_score),
            issues=issues,
            tags=doc.get("tags", []),
            sample_code="\n\n".join(sample_codes) if sample_codes else "",
            suggested_fix="\n\n".join(suggested_fixes) if suggested_fixes else ""
        )
        rule_results.append(result)

    print(f"\n[OK] ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹è§£æå®Œäº†: {len(rule_results)}ãƒ•ã‚¡ã‚¤ãƒ«")

    # Phase 2: AIè©³ç´°è§£æï¼ˆé‡è¦åº¦ã®é«˜ã„ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ï¼‰
    print("\n" + "="*60)
    print("ã€Phase 2ã€‘ AIè©³ç´°è§£æé–‹å§‹")
    print("="*60)

    # é‡è¦åº¦ã§ã‚½ãƒ¼ãƒˆã—ã¦AIè§£æå¯¾è±¡ã‚’é¸å®š
    high_severity_results = [r for r in rule_results if r.severity_score >= AI_MIN_SEVERITY]
    high_severity_results.sort(key=lambda x: x.severity_score, reverse=True)
    ai_targets = high_severity_results[:AI_MAX_FILES]

    print(f"AIè§£æå¯¾è±¡: {len(ai_targets)}ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆé‡è¦åº¦{AI_MIN_SEVERITY}ä»¥ä¸Šï¼‰")

    ai_results = []
    for i, rule_result in enumerate(ai_targets):
        print(f"\nã€{i+1}/{len(ai_targets)}ã€‘ AIè§£æ")

        # å¯¾å¿œã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è¦‹ã¤ã‘ã‚‹
        doc = next((d for d in index if d["path"] == rule_result.path), None)
        if doc:
            ai_result = analyze_single_file_with_ai(
                rule_result.full_path,
                doc.get("text", ""),
                rule_result.issues
            )
            ai_results.append(ai_result)
        else:
            print(f"  [SKIP] ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

    print(f"\n[OK] AIè§£æå®Œäº†: {len(ai_results)}ãƒ•ã‚¡ã‚¤ãƒ«")

    return rule_results, ai_results

def get_severity_level(score: int) -> str:
    """ã‚¹ã‚³ã‚¢ã‹ã‚‰é‡è¦åº¦ãƒ¬ãƒ™ãƒ«ã‚’åˆ¤å®š"""
    if score >= 10:
        return "ğŸ”´ ç·Šæ€¥"
    elif score >= 7:
        return "ğŸŸ  é«˜"
    elif score >= 4:
        return "ğŸŸ¡ ä¸­"
    elif score >= 1:
        return "ğŸ”µ ä½"
    else:
        return "âšª å•é¡Œãªã—"

# ===== Report Generation =====
def generate_rule_based_report(query: str, rule_results: List[RuleResult]) -> str:
    """ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆæ”¹å–„æ¡ˆä»˜ãï¼‰"""
    lines = [
        f"# ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹è§£æãƒ¬ãƒãƒ¼ãƒˆ: {query}",
        "",
        f"ç”Ÿæˆ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        f"è§£æãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(rule_results)}",
        "",
        "## ğŸ“Š å•é¡Œã®åˆ†å¸ƒ",
    ]

    # é‡è¦åº¦åˆ¥ã«é›†è¨ˆ
    severity_groups = {
        "ğŸ”´ ç·Šæ€¥": [],
        "ğŸŸ  é«˜": [],
        "ğŸŸ¡ ä¸­": [],
        "ğŸ”µ ä½": [],
        "âšª å•é¡Œãªã—": []
    }

    for r in rule_results:
        severity_groups[r.severity_level].append(r)

    for level, items in severity_groups.items():
        lines.append(f"- {level}: {len(items)}ä»¶")

    lines.extend(["", "---", ""])

    # é‡è¦åº¦é †ã«å‡ºåŠ›
    sorted_results = sorted(rule_results, key=lambda x: x.severity_score, reverse=True)

    for i, result in enumerate(sorted_results, 1):
        lines.extend([
            f"## {i}. {result.path}",
            f"- **è¨€èª**: {result.lang}",
            f"- **ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°**: {result.encoding}",
            f"- **é‡è¦åº¦**: {result.severity_level} (ã‚¹ã‚³ã‚¢: {result.severity_score})",
            f"- **ã‚¿ã‚°**: {', '.join(result.tags) if result.tags else 'ãªã—'}",
            "",
            "### æ¤œå‡ºã•ã‚ŒãŸå•é¡Œ:",
        ])

        if result.issues:
            for issue in result.issues:
                severity = SEVERITY_SCORES.get(issue, 1)
                emoji = "ğŸ”´" if severity >= 8 else "ğŸŸ " if severity >= 5 else "ğŸŸ¡" if severity >= 3 else "ğŸ”µ"
                lines.append(f"- {emoji} {issue}")
        else:
            lines.append("- ãªã—")

        if result.sample_code:
            lines.extend([
                "",
                "### å•é¡Œç®‡æ‰€ã®ã‚³ãƒ¼ãƒ‰ä¾‹:",
                "```",
                result.sample_code,
                "```"
            ])

        if result.suggested_fix:
            lines.extend([
                "",
                "### ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹æ”¹å–„æ¡ˆ:",
                "```",
                result.suggested_fix,
                "```"
            ])

        lines.extend(["", "---", ""])

    return "\n".join(lines)

def generate_ai_enhanced_report(query: str, rule_results: List[RuleResult], ai_results: List[AIResult]) -> str:
    """AIæ”¹å–„æ¡ˆä»˜ããƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
    lines = [
        f"# AIæ”¹å–„æ¡ˆä»˜ãè§£æãƒ¬ãƒãƒ¼ãƒˆ: {query}",
        "",
        f"ç”Ÿæˆ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        f"ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹è§£æ: {len(rule_results)}ãƒ•ã‚¡ã‚¤ãƒ«",
        f"AIè©³ç´°è§£æ: {len(ai_results)}ãƒ•ã‚¡ã‚¤ãƒ«",
        "",
        "## ğŸ¤– AIè©³ç´°è§£æçµæœ",
        ""
    ]

    # AIè§£æçµæœã‚’å‡ºåŠ›
    for i, ai_result in enumerate(ai_results, 1):
        lines.extend([
            f"### [{i}/{len(ai_results)}] {ai_result.path}",
            ""
        ])

        if ai_result.error:
            lines.append(f"âš ï¸ **ã‚¨ãƒ©ãƒ¼**: {ai_result.error}")
        elif ai_result.analysis:
            lines.append(ai_result.analysis)
        else:
            lines.append("*è§£æçµæœãªã—*")

        lines.extend(["", "---", ""])

    # ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹çµæœã®ã‚µãƒãƒªãƒ¼
    lines.extend([
        "## ğŸ“‹ ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹è§£æã‚µãƒãƒªãƒ¼",
        ""
    ])

    # AIè§£æã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒ¼ã‚¯
    ai_analyzed_paths = {ar.path for ar in ai_results}

    high_severity = [r for r in rule_results if r.severity_score >= AI_MIN_SEVERITY]
    other = [r for r in rule_results if r.severity_score < AI_MIN_SEVERITY]

    lines.append("### ğŸ”¥ é«˜é‡è¦åº¦ï¼ˆAIè§£ææ¸ˆã¿ï¼‰")
    for r in high_severity[:AI_MAX_FILES]:
        mark = "âœ…" if r.path in ai_analyzed_paths else "âŒ"
        lines.append(f"- {mark} {r.path} ({r.severity_level}, ã‚¹ã‚³ã‚¢: {r.severity_score})")

    if other:
        lines.append("\n### ğŸ“ ãã®ä»–ã®ãƒ•ã‚¡ã‚¤ãƒ«")
        for r in other[:10]:  # æœ€å¤§10ä»¶
            lines.append(f"- {r.path} ({r.severity_level}, ã‚¹ã‚³ã‚¢: {r.severity_score})")

    return "\n".join(lines)

# ===== Commands =====
def cmd_query(query: str, topk: int, mode: str, index_path: pathlib.Path, out_base: Optional[str]):
    """æ¤œç´¢ã¨è§£æã‚’å®Ÿè¡Œ"""
    index = load_index(index_path)
    cands = retrieve(query, index, topk=topk)

    # 2æ®µéšè§£æã‚’å®Ÿè¡Œ
    rule_results, ai_results = analyze_all_files(cands, query, topk)

    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    rule_report = generate_rule_based_report(query, rule_results)
    ai_report = generate_ai_enhanced_report(query, rule_results, ai_results)

    # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
    if out_base:
        base_path = pathlib.Path(out_base)
        base_path.parent.mkdir(parents=True, exist_ok=True)

        # ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ãƒ¬ãƒãƒ¼ãƒˆ
        rule_path = base_path.parent / f"{base_path.stem}_rules.md"
        rule_path.write_text(rule_report, encoding="utf-8")
        print(f"\n[OK] ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ãƒ¬ãƒãƒ¼ãƒˆ: {rule_path}")

        # AIæ”¹å–„æ¡ˆä»˜ããƒ¬ãƒãƒ¼ãƒˆ
        ai_path = base_path.parent / f"{base_path.stem}_ai.md"
        ai_path.write_text(ai_report, encoding="utf-8")
        print(f"[OK] AIæ”¹å–„æ¡ˆä»˜ããƒ¬ãƒãƒ¼ãƒˆ: {ai_path}")
    else:
        print("\n" + "="*60)
        print("ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ãƒ¬ãƒãƒ¼ãƒˆ")
        print("="*60)
        print(rule_report[:2000] + "...")
        print("\n" + "="*60)
        print("AIæ”¹å–„æ¡ˆä»˜ããƒ¬ãƒãƒ¼ãƒˆ")
        print("="*60)
        print(ai_report[:2000] + "...")

def cmd_advise(topk: int, mode: str, index_path: pathlib.Path, out_base: Optional[str]):
    """å…¨ä½“åŠ©è¨€ã‚’å®Ÿè¡Œ"""
    seed_query = "é‡‘é¡ ä¸æ•´åˆ ç¨ å°æ•° å°åˆ· Print UI UX DB N+1 JOIN"
    cmd_query(seed_query, topk, mode, index_path, out_base)

# ===== CLI =====
if __name__ == "__main__":
    ap = argparse.ArgumentParser(description="ç©¶æ¥µç‰ˆã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ„ãƒ¼ãƒ«ï¼ˆ2æ®µéšè§£æï¼‰")
    sub = ap.add_subparsers(dest="cmd", required=True)

    ap_idx = sub.add_parser("index", help="ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–")
    ap_idx.add_argument("repo", type=str)
    ap_idx.add_argument("--exclude-langs", type=str, nargs="*", help="é™¤å¤–ã™ã‚‹è¨€èª")
    ap_idx.add_argument("--max-file-mb", type=float, default=4.0, help="æœ€å¤§ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º(MB)")
    ap_idx.add_argument("--profile-index", action="store_true", help="ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‡¦ç†ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’å‡ºåŠ›")
    ap_idx.add_argument("--profile-output", type=str, default=None, help="ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«çµæœã‚’æ›¸ãå‡ºã™ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ.csv / .jsonlï¼‰")

    ap_idx.add_argument("--batch-size", type=int, default=BATCH_SIZE_DEFAULT, help="ãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãå‡ºã—ã®ãƒãƒƒãƒä»¶æ•°ï¼ˆæ—¢å®š500ã€0ã§ç„¡åŠ¹ï¼‰")
    ap_idx.add_argument("--max-files", type=int, default=None, help="å‡¦ç†ã™ã‚‹æœ€å¤§ãƒ•ã‚¡ã‚¤ãƒ«æ•°")
    ap_idx.add_argument("--max-seconds", type=float, default=None, help="å‡¦ç†ã‚’æ‰“ã¡åˆ‡ã‚‹æœ€å¤§ç§’æ•°")
    ap_idx.add_argument("--include", nargs="*", help="ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å¯¾è±¡ã¨ã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆglobï¼‰")
    ap_idx.add_argument("--exclude", nargs="*", help="ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‹ã‚‰é™¤å¤–ã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆglobï¼‰")
    ap_vec = sub.add_parser("vectorize", help="TF-IDFãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆ")
    ap_vec.add_argument("--index", type=str, default=INDEX_PATH)

    ap_q = sub.add_parser("query", help="æ¤œç´¢ã¨2æ®µéšè§£æ")
    ap_q.add_argument("query", type=str, help="æ¤œç´¢ã‚¯ã‚¨ãƒª")
    ap_q.add_argument("--topk", type=int, default=50, help="è§£æå¯¾è±¡ä¸Šä½Nä»¶")
    ap_q.add_argument("--mode", choices=["rules","ai","hybrid"], default="hybrid")
    ap_q.add_argument("--index", type=str, default=INDEX_PATH)
    ap_q.add_argument("--out", type=str, help="å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹åï¼ˆ_rules.md ã¨ _ai.md ãŒç”Ÿæˆã•ã‚Œã‚‹ï¼‰")

    ap_adv = sub.add_parser("advise", help="å…¨ä½“åŠ©è¨€ï¼ˆ2æ®µéšè§£æï¼‰")
    ap_adv.add_argument("--topk", type=int, default=100, help="è§£æå¯¾è±¡ä¸Šä½Nä»¶")
    ap_adv.add_argument("--mode", choices=["rules","ai","hybrid"], default="hybrid")
    ap_adv.add_argument("--index", type=str, default=INDEX_PATH)
    ap_adv.add_argument("--out", type=str, help="å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹å")

    args = ap.parse_args()

    if args.cmd == "index":
        repo = pathlib.Path(args.repo)
        exclude_langs = set(args.exclude_langs) if args.exclude_langs else set()
        max_file_bytes = int(args.max_file_mb * 1_000_000)
        profile_output = pathlib.Path(args.profile_output) if args.profile_output else None
        cmd_index(
            repo,
            pathlib.Path(INDEX_PATH),
            exclude_langs,
            max_file_bytes,
            profile=args.profile_index,
            profile_output=profile_output,
            batch_size=args.batch_size,
            max_files=args.max_files,
            max_seconds=args.max_seconds,
            include_patterns=args.include,
            exclude_patterns=args.exclude,
        )

    elif args.cmd == "vectorize":
        cmd_vectorize(pathlib.Path(args.index))

    elif args.cmd == "query":
        cmd_query(args.query, args.topk, args.mode, pathlib.Path(args.index), args.out)

    elif args.cmd == "advise":
        cmd_advise(args.topk, args.mode, pathlib.Path(args.index), args.out)
