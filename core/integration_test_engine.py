#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
çµ±åˆãƒ†ã‚¹ãƒˆã‚¨ãƒ³ã‚¸ãƒ³ - ãƒ•ãƒ«å®Ÿè¡Œä¸€è²«æ€§ãƒ†ã‚¹ãƒˆ

BugSearch2ã®å…¨å·¥ç¨‹ã‚’è‡ªå‹•å®Ÿè¡Œãƒ»æ¤œè¨¼ã—ã¾ã™ï¼š
1. ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
2. Context7çµ±åˆåˆ†æï¼ˆYAMLç”Ÿæˆã€è¤‡æ•°ãƒˆãƒ”ãƒƒã‚¯å¯¾å¿œï¼‰
3. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
4. AIåˆ†æå®Ÿè¡Œ
5. æ”¹å–„ã‚³ãƒ¼ãƒ‰é©ç”¨
6. çµæœæ¤œè¨¼ãƒ»ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ

Version: v1.0.0
Author: BugSearch2 Team
"""

import os
import sys
import time
import json
import subprocess
import shutil
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from datetime import datetime
from dataclasses import dataclass, asdict


@dataclass
class IntegrationTestConfig:
    """çµ±åˆãƒ†ã‚¹ãƒˆè¨­å®š"""
    project_type: str  # react, angular, express, djangoç­‰
    topics: List[str]  # åˆ†æãƒˆãƒ”ãƒƒã‚¯ï¼ˆè¤‡æ•°å¯ï¼‰
    include_examples: bool = True
    test_dir: Path = Path("test_integration")
    timeout_seconds: int = 1800  # 30åˆ†
    max_file_mb: int = 4  # æœ€å¤§ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º (MB)
    worker_count: int = 4  # ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°


@dataclass
class StepResult:
    """å„ã‚¹ãƒ†ãƒƒãƒ—ã®å®Ÿè¡Œçµæœ"""
    step_name: str
    success: bool
    duration_seconds: float
    output: str = ""
    error: str = ""
    metrics: Dict[str, Any] = None

    def __post_init__(self):
        if self.metrics is None:
            self.metrics = {}


@dataclass
class IntegrationTestResult:
    """çµ±åˆãƒ†ã‚¹ãƒˆå…¨ä½“ã®çµæœ"""
    config: IntegrationTestConfig
    start_time: str
    end_time: str
    total_duration_seconds: float
    overall_success: bool
    steps: List[StepResult]
    summary: Dict[str, Any]


class IntegrationTestEngine:
    """çµ±åˆãƒ†ã‚¹ãƒˆã‚¨ãƒ³ã‚¸ãƒ³"""

    # ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼ˆæ„å›³çš„ãªãƒã‚°å«ã‚€ï¼‰
    SAMPLE_CODE_TEMPLATES = {
        "react": {
            "src/App.tsx": """
import React, { useState, useEffect } from 'react';

// æ„å›³çš„ãªå•é¡Œã‚³ãƒ¼ãƒ‰ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
function App() {
  const [data, setData] = useState<any[]>([]); // anyä½¿ç”¨ï¼ˆå‹å®‰å…¨æ€§å•é¡Œï¼‰

  useEffect(() => {
    // ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯: cleanupãªã—
    const interval = setInterval(() => {
      console.log('Polling...');
    }, 1000);
  }, []);

  // XSSè„†å¼±æ€§
  const renderHTML = (html: string) => {
    return <div dangerouslySetInnerHTML={{ __html: html }} />;
  };

  // ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å•é¡Œ: useCallbackãªã—
  const handleClick = () => {
    console.log('Clicked');
  };

  return (
    <div>
      <h1>Test App</h1>
      <button onClick={handleClick}>Click Me</button>
      {data.map((item, index) => (
        <div key={index}>{item.name}</div>
      ))}
    </div>
  );
}

export default App;
""",
            "src/api.ts": """
// ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å•é¡Œ: ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä¸è¶³
export async function fetchData(url: string) {
  const response = await fetch(url);
  return response.json(); // ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯ãªã—
}

// ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å•é¡Œ: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—
export async function loadUserData(userId: string) {
  const data = await fetchData(`/api/users/${userId}`);
  return data;
}
""",
            "package.json": """{
  "name": "test-react-app",
  "version": "1.0.0",
  "dependencies": {
    "react": "^18.0.0",
    "react-dom": "^18.0.0"
  },
  "devDependencies": {
    "typescript": "^4.9.0"
  }
}
"""
        },
        "angular": {
            "src/app/app.component.ts": """
import { Component, OnInit } from '@angular/core';
import { HttpClient } from '@angular/common/http';

// æ„å›³çš„ãªå•é¡Œã‚³ãƒ¼ãƒ‰ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
@Component({
  selector: 'app-root',
  template: '<div [innerHTML]="content"></div>' // XSSè„†å¼±æ€§
})
export class AppComponent implements OnInit {
  content: any; // anyä½¿ç”¨

  constructor(private http: HttpClient) {}

  ngOnInit() {
    // Subscriptionãƒªãƒ¼ã‚¯: unsubscribeãªã—
    this.http.get('/api/data').subscribe(data => {
      this.content = data;
    });

    // ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯: setInterval cleanupãªã—
    setInterval(() => {
      console.log('Polling...');
    }, 1000);
  }

  // ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å•é¡Œ: ChangeDetectionStrategyæœªæŒ‡å®š
  loadData() {
    this.http.get('/api/data').subscribe();
  }
}
""",
            "package.json": """{
  "name": "test-angular-app",
  "version": "1.0.0",
  "dependencies": {
    "@angular/core": "^15.0.0",
    "@angular/common": "^15.0.0"
  }
}
"""
        }
    }

    def __init__(self, config: IntegrationTestConfig):
        """
        çµ±åˆãƒ†ã‚¹ãƒˆã‚¨ãƒ³ã‚¸ãƒ³ã®åˆæœŸåŒ–

        Args:
            config: ãƒ†ã‚¹ãƒˆè¨­å®š
        """
        self.config = config
        self.test_dir = config.test_dir.resolve()
        self.steps: List[StepResult] = []
        self.start_time = None
        self.end_time = None

    def run(self) -> IntegrationTestResult:
        """
        çµ±åˆãƒ†ã‚¹ãƒˆå…¨ä½“ã‚’å®Ÿè¡Œ

        Returns:
            IntegrationTestResult: ãƒ†ã‚¹ãƒˆçµæœ
        """
        self.start_time = datetime.now()
        print("=" * 80)
        print("ğŸš€ BugSearch2 çµ±åˆãƒ†ã‚¹ãƒˆé–‹å§‹")
        print("=" * 80)
        print(f"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚¿ã‚¤ãƒ—: {self.config.project_type}")
        print(f"åˆ†æãƒˆãƒ”ãƒƒã‚¯: {', '.join(self.config.topics)}")
        print(f"ãƒ†ã‚¹ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.test_dir}")
        print()

        overall_success = True

        try:
            # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
            result = self._run_step("ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—", self._setup_test_project)
            self.steps.append(result)
            if not result.success:
                overall_success = False
                raise Exception(f"ã‚¹ãƒ†ãƒƒãƒ—å¤±æ•—: {result.step_name}")

            # ã‚¹ãƒ†ãƒƒãƒ—2: Context7çµ±åˆåˆ†æï¼ˆYAMLç”Ÿæˆï¼‰
            result = self._run_step("Context7çµ±åˆåˆ†æ", self._run_context7_analysis)
            self.steps.append(result)
            if not result.success:
                overall_success = False
                raise Exception(f"ã‚¹ãƒ†ãƒƒãƒ—å¤±æ•—: {result.step_name}")

            # ã‚¹ãƒ†ãƒƒãƒ—3: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
            result = self._run_step("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ", self._run_index_creation)
            self.steps.append(result)
            if not result.success:
                overall_success = False
                raise Exception(f"ã‚¹ãƒ†ãƒƒãƒ—å¤±æ•—: {result.step_name}")

            # ã‚¹ãƒ†ãƒƒãƒ—4: AIåˆ†æå®Ÿè¡Œ
            result = self._run_step("AIåˆ†æå®Ÿè¡Œ", self._run_ai_analysis)
            self.steps.append(result)
            if not result.success:
                overall_success = False
                raise Exception(f"ã‚¹ãƒ†ãƒƒãƒ—å¤±æ•—: {result.step_name}")

            # ã‚¹ãƒ†ãƒƒãƒ—5: æ”¹å–„ã‚³ãƒ¼ãƒ‰é©ç”¨
            result = self._run_step("æ”¹å–„ã‚³ãƒ¼ãƒ‰é©ç”¨", self._run_code_improvement)
            self.steps.append(result)
            if not result.success:
                print(f"âš ï¸  è­¦å‘Š: {result.step_name}ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸãŒã€ãƒ†ã‚¹ãƒˆã¯ç¶™ç¶šã—ã¾ã™")

            # ã‚¹ãƒ†ãƒƒãƒ—6: çµæœæ¤œè¨¼
            result = self._run_step("çµæœæ¤œè¨¼", self._verify_results)
            self.steps.append(result)
            if not result.success:
                overall_success = False

        except Exception as e:
            print(f"\nâŒ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            overall_success = False

        self.end_time = datetime.now()
        total_duration = (self.end_time - self.start_time).total_seconds()

        # ã‚µãƒãƒªãƒ¼ç”Ÿæˆ
        summary = self._generate_summary()

        result = IntegrationTestResult(
            config=self.config,
            start_time=self.start_time.isoformat(),
            end_time=self.end_time.isoformat(),
            total_duration_seconds=total_duration,
            overall_success=overall_success,
            steps=self.steps,
            summary=summary
        )

        # ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›
        self._print_report(result)
        self._save_report(result)

        return result

    def _run_step(self, step_name: str, func) -> StepResult:
        """
        å€‹åˆ¥ã‚¹ãƒ†ãƒƒãƒ—ã®å®Ÿè¡Œ

        Args:
            step_name: ã‚¹ãƒ†ãƒƒãƒ—å
            func: å®Ÿè¡Œã™ã‚‹é–¢æ•°

        Returns:
            StepResult: ã‚¹ãƒ†ãƒƒãƒ—çµæœ
        """
        print(f"\n{'='*80}")
        print(f"ğŸ“‹ ã‚¹ãƒ†ãƒƒãƒ—: {step_name}")
        print(f"{'='*80}")

        start = time.time()
        try:
            output, metrics = func()
            duration = time.time() - start

            result = StepResult(
                step_name=step_name,
                success=True,
                duration_seconds=duration,
                output=output,
                metrics=metrics
            )

            print(f"âœ… {step_name} å®Œäº† ({duration:.2f}ç§’)")
            return result

        except Exception as e:
            duration = time.time() - start
            error_msg = str(e)

            result = StepResult(
                step_name=step_name,
                success=False,
                duration_seconds=duration,
                error=error_msg
            )

            print(f"âŒ {step_name} å¤±æ•— ({duration:.2f}ç§’)")
            print(f"   ã‚¨ãƒ©ãƒ¼: {error_msg}")
            return result

    def _setup_test_project(self) -> Tuple[str, Dict]:
        """ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        # æ—¢å­˜ã®ãƒ†ã‚¹ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        if self.test_dir.exists():
            shutil.rmtree(self.test_dir)

        self.test_dir.mkdir(parents=True)

        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ç”Ÿæˆ
        if self.config.project_type not in self.SAMPLE_CODE_TEMPLATES:
            raise ValueError(f"æœªå¯¾å¿œã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚¿ã‚¤ãƒ—: {self.config.project_type}")

        templates = self.SAMPLE_CODE_TEMPLATES[self.config.project_type]
        files_created = []

        for file_path, content in templates.items():
            full_path = self.test_dir / file_path
            full_path.parent.mkdir(parents=True, exist_ok=True)
            full_path.write_text(content, encoding='utf-8')
            files_created.append(str(file_path))

        # .bugsearch.ymlç”Ÿæˆ
        bugsearch_config = f"""
tech_stack:
  frontend:
    - {self.config.project_type}
  language:
    - typescript
    - javascript

analysis:
  topics: {self.config.topics}
  include_examples: {self.config.include_examples}
"""
        (self.test_dir / ".bugsearch.yml").write_text(bugsearch_config, encoding='utf-8')
        files_created.append(".bugsearch.yml")

        output = f"ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä½œæˆå®Œäº†\nä½œæˆãƒ•ã‚¡ã‚¤ãƒ«: {len(files_created)}å€‹"
        metrics = {
            "files_created": len(files_created),
            "project_type": self.config.project_type
        }

        return output, metrics

    def _run_context7_analysis(self) -> Tuple[str, Dict]:
        """Context7çµ±åˆåˆ†æå®Ÿè¡Œ"""
        # generate_tech_config.pyã‚’å®Ÿè¡Œ
        cmd = [
            sys.executable,
            "generate_tech_config.py",
            "--tech", self.config.project_type,
            "--no-examples" if not self.config.include_examples else ""
        ]

        # è¤‡æ•°ãƒˆãƒ”ãƒƒã‚¯å¯¾å¿œï¼ˆPhase 3ï¼‰
        for topic in self.config.topics:
            cmd.extend(["--topic", topic])

        # ç©ºæ–‡å­—åˆ—ã‚’é™¤å»
        cmd = [c for c in cmd if c]

        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=300,
            cwd=Path.cwd()
        )

        if result.returncode != 0:
            raise Exception(f"Context7åˆ†æå¤±æ•—: {result.stderr}")

        # ç”Ÿæˆã•ã‚ŒãŸYAMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª
        yaml_file = Path(f"config/{self.config.project_type}-rules.yml")
        if not yaml_file.exists():
            raise Exception(f"YAMLãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ: {yaml_file}")

        output = f"YAMLè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ: {yaml_file}"
        metrics = {
            "yaml_file": str(yaml_file),
            "topics_analyzed": len(self.config.topics)
        }

        return output, metrics

    def _run_index_creation(self) -> Tuple[str, Dict]:
        """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆå®Ÿè¡Œ"""
        cmd = [
            sys.executable,
            "codex_review_severity.py",
            "index",
            "--src-dir", str(self.test_dir),
            "--max-file-mb", str(self.config.max_file_mb),
            "--worker-count", str(self.config.worker_count)
        ]

        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=300
        )

        if result.returncode != 0:
            raise Exception(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆå¤±æ•—: {result.stderr}")

        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª
        index_file = Path(".advice_index.jsonl")
        if not index_file.exists():
            raise Exception("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")

        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®è¡Œæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        with open(index_file, 'r', encoding='utf-8') as f:
            file_count = sum(1 for _ in f)

        output = f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆå®Œäº†: {file_count}ãƒ•ã‚¡ã‚¤ãƒ«"
        metrics = {
            "indexed_files": file_count
        }

        return output, metrics

    def _run_ai_analysis(self) -> Tuple[str, Dict]:
        """AIåˆ†æå®Ÿè¡Œ"""
        report_path = self.test_dir / "integration_test_report"

        cmd = [
            sys.executable,
            "codex_review_severity.py",
            "advise",
            "--all",
            "--complete-report",
            "--max-complete-items", "50",
            "--out", str(report_path)
        ]

        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=600  # 10åˆ†
        )

        if result.returncode != 0:
            raise Exception(f"AIåˆ†æå¤±æ•—: {result.stderr}")

        # ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª
        report_file = Path(f"{report_path}.md")
        if not report_file.exists():
            raise Exception("åˆ†æãƒ¬ãƒãƒ¼ãƒˆãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")

        # ãƒ¬ãƒãƒ¼ãƒˆå†…å®¹ã‚’è§£æ
        content = report_file.read_text(encoding='utf-8')
        issues_count = content.count("##")  # å•é¡Œã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ã‚«ã‚¦ãƒ³ãƒˆ

        output = f"AIåˆ†æå®Œäº†: {issues_count}å€‹ã®å•é¡Œã‚’æ¤œå‡º"
        metrics = {
            "issues_detected": issues_count,
            "report_file": str(report_file)
        }

        return output, metrics

    def _run_code_improvement(self) -> Tuple[str, Dict]:
        """æ”¹å–„ã‚³ãƒ¼ãƒ‰é©ç”¨"""
        report_file = self.test_dir / "integration_test_report.md"

        if not report_file.exists():
            raise Exception("åˆ†æãƒ¬ãƒãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

        cmd = [
            sys.executable,
            "apply_improvements_from_report.py",
            str(report_file),
            "--dry-run"  # ãƒ†ã‚¹ãƒˆãªã®ã§dry-runã®ã¿
        ]

        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=300
        )

        if result.returncode != 0:
            # æ”¹å–„ã‚³ãƒ¼ãƒ‰ãŒãªã„å ´åˆã‚‚ã‚ã‚‹ã®ã§warningã¨ã—ã¦æ‰±ã†
            output = "æ”¹å–„ã‚³ãƒ¼ãƒ‰é©ç”¨ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæ”¹å–„ææ¡ˆãªã—ï¼‰"
            metrics = {"improvements_applied": 0}
            return output, metrics

        # é©ç”¨äºˆå®šã®æ”¹å–„æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        improvements = result.stdout.count("Would apply")

        output = f"æ”¹å–„ã‚³ãƒ¼ãƒ‰ç¢ºèªå®Œäº†: {improvements}å€‹ã®æ”¹å–„ã‚’æ¤œå‡º"
        metrics = {
            "improvements_available": improvements
        }

        return output, metrics

    def _verify_results(self) -> Tuple[str, Dict]:
        """çµæœæ¤œè¨¼"""
        checks = []

        # 1. YAMLãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆç¢ºèª
        yaml_file = Path(f"config/{self.config.project_type}-rules.yml")
        yaml_exists = yaml_file.exists()
        checks.append(("YAMLç”Ÿæˆ", yaml_exists))

        # 2. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
        index_exists = Path(".advice_index.jsonl").exists()
        checks.append(("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ", index_exists))

        # 3. ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
        report_exists = (self.test_dir / "integration_test_report.md").exists()
        checks.append(("åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ", report_exists))

        # 4. ãƒ†ã‚¹ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç¢ºèª
        test_dir_exists = self.test_dir.exists()
        checks.append(("ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ", test_dir_exists))

        success_count = sum(1 for _, result in checks if result)
        total_count = len(checks)
        all_passed = success_count == total_count

        output = f"æ¤œè¨¼çµæœ: {success_count}/{total_count} åˆæ ¼"
        for check_name, result in checks:
            status = "âœ…" if result else "âŒ"
            output += f"\n  {status} {check_name}"

        metrics = {
            "checks_passed": success_count,
            "checks_total": total_count,
            "all_passed": all_passed
        }

        if not all_passed:
            raise Exception("ä¸€éƒ¨ã®æ¤œè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸ")

        return output, metrics

    def _generate_summary(self) -> Dict[str, Any]:
        """ã‚µãƒãƒªãƒ¼ç”Ÿæˆ"""
        total_steps = len(self.steps)
        successful_steps = sum(1 for step in self.steps if step.success)

        # å„ã‚¹ãƒ†ãƒƒãƒ—ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’é›†ç´„
        files_created = 0
        indexed_files = 0
        issues_detected = 0
        improvements_available = 0

        for step in self.steps:
            if step.metrics:
                files_created += step.metrics.get("files_created", 0)
                indexed_files += step.metrics.get("indexed_files", 0)
                issues_detected += step.metrics.get("issues_detected", 0)
                improvements_available += step.metrics.get("improvements_available", 0)

        return {
            "total_steps": total_steps,
            "successful_steps": successful_steps,
            "success_rate": (successful_steps / total_steps * 100) if total_steps > 0 else 0,
            "files_created": files_created,
            "indexed_files": indexed_files,
            "issues_detected": issues_detected,
            "improvements_available": improvements_available,
            "project_type": self.config.project_type,
            "topics_analyzed": self.config.topics
        }

    def _print_report(self, result: IntegrationTestResult):
        """ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›ï¼ˆã‚³ãƒ³ã‚½ãƒ¼ãƒ«ï¼‰"""
        print("\n")
        print("=" * 80)
        print("ğŸ“Š çµ±åˆãƒ†ã‚¹ãƒˆçµæœãƒ¬ãƒãƒ¼ãƒˆ")
        print("=" * 80)
        print()

        # å…¨ä½“çµæœ
        status_emoji = "âœ…" if result.overall_success else "âŒ"
        print(f"{status_emoji} å…¨ä½“çµæœ: {'æˆåŠŸ' if result.overall_success else 'å¤±æ•—'}")
        print(f"â±ï¸  å®Ÿè¡Œæ™‚é–“: {result.total_duration_seconds:.2f}ç§’")
        print()

        # ã‚¹ãƒ†ãƒƒãƒ—åˆ¥çµæœ
        print("ğŸ“‹ ã‚¹ãƒ†ãƒƒãƒ—åˆ¥å®Ÿè¡Œçµæœ:")
        for i, step in enumerate(result.steps, 1):
            status = "âœ…" if step.success else "âŒ"
            print(f"  {status} {i}. {step.step_name} ({step.duration_seconds:.2f}ç§’)")
            if step.error:
                print(f"      ã‚¨ãƒ©ãƒ¼: {step.error}")
        print()

        # ã‚µãƒãƒªãƒ¼
        summary = result.summary
        print("ğŸ“ˆ çµ±è¨ˆæƒ…å ±:")
        print(f"  ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚¿ã‚¤ãƒ—: {summary['project_type']}")
        print(f"  åˆ†æãƒˆãƒ”ãƒƒã‚¯: {', '.join(summary['topics_analyzed'])}")
        print(f"  ä½œæˆãƒ•ã‚¡ã‚¤ãƒ«æ•°: {summary['files_created']}")
        print(f"  ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {summary['indexed_files']}")
        print(f"  æ¤œå‡ºå•é¡Œæ•°: {summary['issues_detected']}")
        print(f"  æ”¹å–„ææ¡ˆæ•°: {summary['improvements_available']}")
        print(f"  æˆåŠŸç‡: {summary['success_rate']:.1f}%")
        print()

        print("=" * 80)
        if result.overall_success:
            print("ğŸ‰ çµ±åˆãƒ†ã‚¹ãƒˆå®Œäº†: å…¨ã‚¹ãƒ†ãƒƒãƒ—æˆåŠŸ")
        else:
            print("âš ï¸  çµ±åˆãƒ†ã‚¹ãƒˆå®Œäº†: ä¸€éƒ¨ã‚¹ãƒ†ãƒƒãƒ—ã§å•é¡ŒãŒç™ºç”Ÿ")
        print("=" * 80)

    def _save_report(self, result: IntegrationTestResult):
        """ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜ï¼ˆJSONï¼‰"""
        report_dir = Path("reports")
        report_dir.mkdir(exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = report_dir / f"integration_test_{timestamp}.json"

        # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹ã‚’dictã«å¤‰æ›
        report_data = {
            "config": asdict(result.config),
            "start_time": result.start_time,
            "end_time": result.end_time,
            "total_duration_seconds": result.total_duration_seconds,
            "overall_success": result.overall_success,
            "steps": [asdict(step) for step in result.steps],
            "summary": result.summary
        }

        # Path objectsã‚’strã«å¤‰æ›
        def convert_paths(obj):
            if isinstance(obj, dict):
                return {k: convert_paths(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_paths(item) for item in obj]
            elif isinstance(obj, Path):
                return str(obj)
            return obj

        report_data = convert_paths(report_data)

        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)

        print(f"\nğŸ’¾ ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_file}")


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°ï¼ˆCLIå®Ÿè¡Œç”¨ï¼‰ - Phase 8.4: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šçµ±åˆ"""
    import argparse

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ (Phase 8.4)
    try:
        from core.integration_test_config import get_config_manager
        config_manager = get_config_manager()
        use_defaults = True
    except ImportError:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚¤ãƒ³ãƒãƒ¼ãƒˆã§ããªã„å ´åˆã¯ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰
        config_manager = None
        use_defaults = False

    parser = argparse.ArgumentParser(
        description="BugSearch2 çµ±åˆãƒ†ã‚¹ãƒˆ - å…¨å·¥ç¨‹ä¸€è²«æ€§ãƒ†ã‚¹ãƒˆ"
    )
    parser.add_argument(
        "--project-type",
        required=False,  # Phase 8.4: ã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ã«å¤‰æ›´ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šä½¿ç”¨å¯ï¼‰
        choices=["react", "angular", "vue", "express", "django", "spring-boot", "flask", "nestjs", "mysql", "postgresql", "sqlserver", "oracle", "memcached"],
        help="ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚¿ã‚¤ãƒ—ï¼ˆçœç•¥æ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‹ã‚‰å–å¾—ï¼‰"
    )
    parser.add_argument(
        "--topics",
        nargs="+",
        help="åˆ†æãƒˆãƒ”ãƒƒã‚¯ï¼ˆè¤‡æ•°æŒ‡å®šå¯ã€çœç•¥æ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‹ã‚‰å–å¾—ï¼‰"
    )
    parser.add_argument(
        "--no-examples",
        action="store_true",
        help="ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ã‚’å«ã‚ãªã„"
    )
    parser.add_argument(
        "--test-dir",
        type=Path,
        default=Path("test_integration"),
        help="ãƒ†ã‚¹ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª"
    )
    parser.add_argument(
        "--max-file-mb",
        type=int,
        help="æœ€å¤§ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º (MBã€çœç•¥æ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‹ã‚‰å–å¾—)"
    )
    parser.add_argument(
        "--worker-count",
        type=int,
        help="ä¸¦åˆ—ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ï¼ˆçœç•¥æ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‹ã‚‰å–å¾—ï¼‰"
    )

    args = parser.parse_args()

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã®é©ç”¨ (Phase 8.4)
    if use_defaults and config_manager:
        # project-type
        if not args.project_type:
            default_projects = config_manager.get_integration_test_default_project_types()
            if default_projects:
                args.project_type = default_projects[0]  # æœ€åˆã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚¿ã‚¤ãƒ—ã‚’ä½¿ç”¨
                print(f"[INFO] ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‹ã‚‰project-typeå–å¾—: {args.project_type}")
            else:
                print("[ERROR] project-typeãŒæŒ‡å®šã•ã‚Œã¦ãŠã‚‰ãšã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚‚ç©ºã§ã™")
                sys.exit(1)

        # topics
        if not args.topics:
            args.topics = config_manager.get_integration_test_default_topics()
            if args.topics:
                print(f"[INFO] ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‹ã‚‰topicså–å¾—: {', '.join(args.topics)}")
            else:
                args.topics = ["security", "performance"]  # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

        # max-file-mb
        if args.max_file_mb is None:
            args.max_file_mb = config_manager.get_integration_test_default_max_file_mb()
            print(f"[INFO] ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‹ã‚‰max-file-mbå–å¾—: {args.max_file_mb}")

        # worker-count
        if args.worker_count is None:
            args.worker_count = config_manager.get_integration_test_default_worker_count()
            print(f"[INFO] ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‹ã‚‰worker-countå–å¾—: {args.worker_count}")
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ãŒä½¿ãˆãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if not args.project_type:
            print("[ERROR] --project-type ã¯å¿…é ˆã§ã™ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ï¼‰")
            sys.exit(1)
        if not args.topics:
            args.topics = ["security", "performance"]
        if args.max_file_mb is None:
            args.max_file_mb = 4
        if args.worker_count is None:
            args.worker_count = 4

    config = IntegrationTestConfig(
        project_type=args.project_type,
        topics=args.topics,
        include_examples=not args.no_examples,
        test_dir=args.test_dir,
        max_file_mb=args.max_file_mb,
        worker_count=args.worker_count
    )

    engine = IntegrationTestEngine(config)
    result = engine.run()

    # çµ‚äº†ã‚³ãƒ¼ãƒ‰
    sys.exit(0 if result.overall_success else 1)


if __name__ == "__main__":
    main()
