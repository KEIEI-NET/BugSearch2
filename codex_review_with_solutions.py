#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
codex_review_with_solutions.py â€” æ”¹å–„æ¡ˆã¨ã‚³ãƒ¼ãƒ‰ä¾‹ã‚’å«ã‚€ãƒãƒ¼ã‚¸ãƒ§ãƒ³
GPT-5ã«ã‚ˆã‚‹å…·ä½“çš„ãªæ”¹å–„æ¡ˆã¨ä¿®æ­£ã‚³ãƒ¼ãƒ‰ä¾‹ã‚’æä¾›

é‡ç‚¹ã‚«ãƒ†ã‚´ãƒª:
  1) é‡‘é¡é¢ã®ä¸æ•´åˆï¼ˆä¸¸ã‚/ç¨/å°æ•°/é€šè²¨å‹ï¼‰
  2) å°åˆ·ç³»ï¼ˆé€”ä¸­åœæ­¢/ãƒšãƒ¼ã‚¸æ¬ ã‘/ãƒ¬ãƒãƒ¼ãƒˆç³»ï¼‰
  3) UI/UXï¼ˆæœªæ¤œè¨¼å…¥åŠ›/å°ç·š/ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£ï¼‰
  4) DBè² è·ï¼ˆN+1/å…¨ä»¶/æœªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹/å¤šé‡JOINï¼‰

pip install chromadb openai scikit-learn joblib regex
ç’°å¢ƒå¤‰æ•°: OPENAI_API_KEYï¼ˆå¿…é ˆ: AIãƒ¢ãƒ¼ãƒ‰ï¼‰, OPENAI_MODELï¼ˆä»»æ„: æ—¢å®š 'gpt-5'ï¼‰

ä¾‹:
  python codex_review_with_solutions.py index /path/to/repo
  python codex_review_with_solutions.py query "å°åˆ·ãŒé€”ä¸­ã§æ­¢ã¾ã‚‹" --mode hybrid --topk 30 --out reports/print.md
  python codex_review_with_solutions.py advise --mode hybrid --topk 80 --out reports/advise.md
"""
from __future__ import annotations
import argparse, hashlib, json, os, pathlib, re, sys, time
from dataclasses import dataclass, asdict
from typing import Any, Dict, List, Tuple

# ===== Config =====
IGNORE_DIRS = {".git","node_modules","dist","build","out","bin","obj",".idea",".vscode",".next","coverage","target"}
MAX_FILE_BYTES = 3_000_000
ENV_FILE = ".env"
INDEX_PATH = ".advice_index.jsonl"
VEC_PATH = ".advice_tfidf.pkl"
MATRIX_PATH = ".advice_matrix.pkl"
BATCH_SIZE = 5  # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å°ã•ãã—ã¦è©³ç´°ãªåˆ†æã‚’å¯èƒ½ã«
MAX_PROMPT_SIZE = 8000  # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚µã‚¤ã‚º
AI_TIMEOUT = 240  # 240ç§’ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ

# ===== Optional deps =====
SKLEARN_OK = False
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    import joblib
    SKLEARN_OK = True
except Exception:
    pass

OPENAI_OK = False
try:
    from openai import OpenAI
    OPENAI_OK = True
except Exception:
    pass

# ===== Utils =====
if ENV_FILE and pathlib.Path(ENV_FILE).exists():
    with open(ENV_FILE, "r", encoding="utf-8") as f:
        for line in f:
            if "=" in line: key, val = line.strip().split("=", 1); os.environ[key] = val

@dataclass
class Doc:
    path: str; lang: str; size: int; sha1: str; tags: List[str]; summary: str; text: str

def sha1_bytes(data: bytes) -> str:
    return hashlib.sha1(data).hexdigest()[:10]

def detect_lang(p: pathlib.Path) -> str:
    ext = p.suffix.lower()
    if ext in (".py",): return "python"
    if ext in (".js",".mjs",".cjs",".jsx"): return "javascript"
    if ext in (".ts",".tsx"): return "typescript"
    if ext in (".cs",".csx"): return "csharp"
    if ext in (".java",): return "java"
    if ext in (".pas",".dfm",".dpr"): return "delphi"
    if ext in (".rb",): return "ruby"
    if ext in (".go",): return "go"
    if ext in (".rs",): return "rust"
    if ext in (".php",): return "php"
    if ext in (".c",".h"): return "c"
    if ext in (".cpp",".hpp",".cc",".hh",".cxx",".hxx"): return "cpp"
    return "other"

def make_summary(text: str) -> str:
    lines = text.splitlines()[:10]; return " / ".join(l.strip()[:80] for l in lines if l.strip())[:400]

def write_large_file_log(large_files: List[Tuple[str, int]], output_dir: pathlib.Path) -> pathlib.Path|None:
    if not large_files: return None
    reports_dir = output_dir / "reports"; reports_dir.mkdir(parents=True, exist_ok=True)
    log_path = reports_dir / "large_files_over_limit.log"
    with open(log_path, "w", encoding="utf-8") as f:
        f.write(f"Large files exceeding limit ({MAX_FILE_BYTES} bytes):\n\n")
        for file_path, size in sorted(large_files, key=lambda x: x[1], reverse=True):
            f.write(f"{size:12,} bytes  {file_path}\n")
        f.write(f"\nTotal: {len(large_files)} files\n")
    return log_path

# ===== Rule engine =====
def scan_money(text: str) -> List[str]:
    m: List[str] = []
    if re.search(r"\b(float|double|real)\b.*\b(price|cost|money|amount|tax|total|sum|charge|fee|pay)", text, re.IGNORECASE):
        m.append("é‡‘é¡: æµ®å‹•å°æ•°ã§é‡‘é¡â†’èª¤å·®ã€‚Decimal/é€šè²¨å‹ã¸")
    if re.search(r"\b(total|sum|amount)\s*=.*\+.*tax", text, re.IGNORECASE) and re.search(r"(Math\.(floor|round)|parseInt|truncate)", text, re.IGNORECASE):
        m.append("é‡‘é¡: ç¨è¾¼/ç¨æŠœãƒ»ç«¯æ•°å‡¦ç†ã®æ··åœ¨æ³¨æ„")
    return m

def scan_print(text: str) -> List[str]:
    m: List[str] = []
    if re.search(r"(window\.print|PrintDocument|Printer|PrintPage|HasMorePages|NewPage|PrintPreview)", text, re.IGNORECASE) and not re.search(r"(try|catch|error|exception)", text, re.IGNORECASE):
        m.append("å°åˆ·: ã‚¨ãƒ©ãƒ¼å‡¦ç†ãªã—â†’é€”ä¸­åœæ­¢ãƒªã‚¹ã‚¯")
    if re.search(r"\.HasMorePages\s*=\s*true", text, re.IGNORECASE) and not re.search(r"(yPos|currentY|pageHeight|margin)", text, re.IGNORECASE):
        m.append("å°åˆ·: æ”¹ãƒšãƒ¼ã‚¸åˆ¤å®šãŒä¸ååˆ†")
    return m

def scan_ui(text: str) -> List[str]:
    m: List[str] = []
    if re.search(r"(button|submit|click|tap|press)", text, re.IGNORECASE) and not re.search(r"(disabled|loading|spinner|prevent|debounce|throttle)", text, re.IGNORECASE):
        m.append("UI: å¤šé‡ã‚¯ãƒªãƒƒã‚¯é˜²æ­¢ãªã—")
    if re.search(r"(innerHTML\s*=|dangerouslySetInnerHTML|v-html)", text) and not re.search(r"(sanitize|escape|DOMPurify|textContent)", text, re.IGNORECASE):
        m.append("UI: XSSè„†å¼±æ€§ã®ç–‘ã„ï¼ˆæœªã‚µãƒ‹ã‚¿ã‚¤ã‚ºHTMLï¼‰")
    if re.search(r"(input|textarea|select)", text, re.IGNORECASE) and not re.search(r"(validate|pattern|required|maxlength|regex|test|match)", text, re.IGNORECASE):
        m.append("UI: å…¥åŠ›æ¤œè¨¼ãŒä¸ååˆ†")
    return m

def scan_db(text: str) -> List[str]:
    m: List[str] = []
    if re.search(r"SELECT\s+\*\s+FROM", text, re.IGNORECASE):
        m.append("DB: SELECT * â†’è² è·å¢—ã€‚åˆ—é™å®š")
    if re.search(r"(for|while|foreach|map).*\n.*\n.*\n.*(SELECT|INSERT|UPDATE|DELETE)", text, re.IGNORECASE):
        m.append("DB: ãƒ«ãƒ¼ãƒ—å†…SELECT (N+1) ç–‘ã„")
    if re.search(r"(JOIN.*){3,}", text, re.IGNORECASE):
        m.append("DB: å¤šé‡JOINâ†’é…å»¶ã®æã‚Œã€‚è¨ˆç”»/IDXç¢ºèª")
    if re.search(r"(OFFSET|LIMIT)\s+\d{4,}", text, re.IGNORECASE):
        m.append("DB: å¤§OFFSETâ†’é…å»¶ã€‚IDã‚«ãƒ¼ã‚½ãƒ«æ¨å¥¨")
    return m

def make_tags(text: str) -> List[str]:
    t = []
    if re.search(r"\b(price|cost|money|amount|tax|total|sum|charge|fee|pay|invoice|billing|currency|decimal|round)", text, re.IGNORECASE): t.append("money")
    if re.search(r"\b(print|printer|page|paper|report|pdf|export|PrintDocument|PrintPage|HasMorePages)", text, re.IGNORECASE): t.append("print")
    if re.search(r"\b(button|click|submit|form|input|select|modal|dialog|toast|spinner|loading|disabled)", text, re.IGNORECASE): t.append("uiux")
    if re.search(r"\b(SELECT|INSERT|UPDATE|DELETE|JOIN|WHERE|FROM|query|sql|database|transaction|connection)", text, re.IGNORECASE): t.append("db")
    if re.search(r"\b(api|http|fetch|axios|request|response|endpoint|REST|webhook|curl)", text, re.IGNORECASE): t.append("net")
    if re.search(r"\b(file|fs|stream|read|write|upload|download|path|directory|folder)", text, re.IGNORECASE): t.append("io")
    return sorted(set(t))

# ===== Indexer =====
def should_index(p: pathlib.Path) -> bool:
    if p.is_dir(): return False
    if any(part.startswith(".") for part in p.parts): return False
    lang = detect_lang(p)
    if lang == "other":
        important_ext = {".xml", ".json", ".yaml", ".yml", ".toml", ".ini", ".config", ".md", ".txt", ".html", ".htm", ".css"}
        if p.suffix.lower() in important_ext: return True
        if p.name in {"Dockerfile", "Makefile", "Rakefile", "Gemfile", ".env", ".env.example"}: return True
        return False
    return True

def cmd_index(repo: pathlib.Path, index_path: pathlib.Path):
    paths = []; large_files = []
    for p in repo.rglob("*"):
        if p.is_file() and not any(d in p.parts for d in IGNORE_DIRS):
            file_size = p.stat().st_size
            if file_size > MAX_FILE_BYTES:
                large_files.append((str(p.relative_to(repo)), file_size))
                continue
            if should_index(p): paths.append(p)
    count = 0
    with open(index_path, "w", encoding="utf-8") as w:
        for p in sorted(paths):
            lang = detect_lang(p)
            try: txt = p.read_text(encoding="utf-8", errors="ignore")
            except Exception: continue
            tags = make_tags(txt)
            rel_path = str(p.relative_to(repo))
            encoded = txt.encode("utf-8", errors="ignore")
            doc = Doc(path=rel_path, lang=lang, size=len(encoded), sha1=sha1_bytes(encoded), tags=tags, summary=make_summary(txt), text=txt)
            w.write(json.dumps(asdict(doc), ensure_ascii=False) + "\n"); count += 1
    print(f"[OK] Indexed {count} files -> {index_path}")
    log_path = write_large_file_log(large_files, index_path.parent.resolve())
    if log_path:
        threshold_mb = MAX_FILE_BYTES / 1_000_000
        try:
            rel_log = log_path.relative_to(index_path.parent.resolve())
        except ValueError:
            rel_log = log_path
        print(f"[WARNING] Skipped {len(large_files)} files exceeding ~{threshold_mb:.1f} MB. Details: {rel_log}")

# ===== Retrieval =====
def load_index(index_path: pathlib.Path) -> List[Dict[str,Any]]:
    arr: List[Dict[str,Any]] = []
    with open(index_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line: arr.append(json.loads(line))
    return arr

def retrieve(query: str, index: List[Dict[str,Any]], topk: int = 10) -> List[Dict[str,Any]]:
    if SKLEARN_OK:
        vec_p = pathlib.Path(VEC_PATH); mat_p = pathlib.Path(MATRIX_PATH)
        if vec_p.exists() and mat_p.exists():
            return retrieve_tfidf(query, index, vec_p, mat_p, topk)
    return retrieve_naive(query, index, topk)

def retrieve_naive(query: str, index: List[Dict[str,Any]], topk: int) -> List[Dict[str,Any]]:
    q_words = set(w.lower() for w in re.split(r"\W+", query) if w)
    for doc in index:
        txt_words = set(w.lower() for w in re.split(r"\W+", doc.get("text","")[:3000]) if w)
        doc["_score"] = len(q_words & txt_words) / (len(q_words) + 0.01)
    return sorted(index, key=lambda x: x.get("_score",0), reverse=True)[:topk]

def retrieve_tfidf(query: str, index: List[Dict[str,Any]], vec_path: pathlib.Path, mat_path: pathlib.Path, topk: int) -> List[Dict[str,Any]]:
    from sklearn.metrics.pairwise import cosine_similarity
    vec = joblib.load(vec_path); mat = joblib.load(mat_path)
    q_vec = vec.transform([query])
    scores = cosine_similarity(q_vec, mat).flatten()
    for i, doc in enumerate(index):
        if i < len(scores):
            doc["_score"] = float(scores[i])
    return sorted(index, key=lambda x: x.get("_score",0), reverse=True)[:topk]

def cmd_vectorize(index_path: pathlib.Path):
    if not SKLEARN_OK:
        print("scikit-learnæœªå°å…¥ã€‚pip install scikit-learn joblib")
        return
    index = load_index(index_path)
    texts = [d.get("text","")[:10000] for d in index]
    vec = TfidfVectorizer(max_features=5000, ngram_range=(1,2), stop_words=None)
    mat = vec.fit_transform(texts)
    joblib.dump(vec, VEC_PATH); joblib.dump(mat, MATRIX_PATH)
    print(f"[OK] Vectorized -> {VEC_PATH}, {MATRIX_PATH}")

# ===== Advice rules =====
def rule_advices(d: Dict[str,Any]) -> List[str]:
    text = d.get("text", "")[:20000]
    out: List[str] = []
    out += scan_money(text)
    out += scan_print(text)
    out += scan_ui(text)
    out += scan_db(text)
    return out[:8]

# ===== GPT-5 æ”¹å–„æ¡ˆã¨ã‚³ãƒ¼ãƒ‰ä¾‹ã‚’å«ã‚€ãƒãƒ¼ã‚¸ãƒ§ãƒ³ =====
def ai_review_with_solutions(query: str, all_snippets: List[str]) -> str:
    """æ”¹å–„æ¡ˆã¨ä¿®æ­£ã‚³ãƒ¼ãƒ‰ä¾‹ã‚’å«ã‚€AIãƒ¬ãƒ“ãƒ¥ãƒ¼"""
    if not OPENAI_OK: return "(AIæœªæœ‰åŠ¹: openaiæœªå°å…¥)"
    api_key = os.environ.get("OPENAI_API_KEY")
    if not api_key: return "(AIæœªæœ‰åŠ¹: OPENAI_API_KEY æœªè¨­å®š)"
    # GPT-5ãŒç©ºã®å¿œç­”ã‚’è¿”ã™ãŸã‚ã€gpt-4oã‚’ä½¿ç”¨
    model = os.environ.get("OPENAI_MODEL", "gpt-4o")

    # APIã‚­ãƒ¼ãŒæ–°ã—ã„ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‹ç¢ºèª
    if api_key.startswith("sk-proj-"):
        print("[INFO] Using new API key format")

    client = OpenAI(api_key=api_key)

    if not all_snippets:
        return "(ãƒ¬ãƒ“ãƒ¥ãƒ¼å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãªã—)"

    reviews = []
    total_batches = (len(all_snippets) + BATCH_SIZE - 1) // BATCH_SIZE
    print(f"[INFO] Processing {len(all_snippets)} files in {total_batches} batches...")

    for i in range(0, len(all_snippets), BATCH_SIZE):
        batch_num = i // BATCH_SIZE + 1
        batch = all_snippets[i:i+BATCH_SIZE]
        batch_json = json.dumps(batch, ensure_ascii=False)

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚µã‚¤ã‚ºåˆ¶é™
        if len(batch_json) > MAX_PROMPT_SIZE:
            batch_json = batch_json[:MAX_PROMPT_SIZE] + "...(truncated)"

        prompt = (
            "ã‚ãªãŸã¯ã‚·ãƒ‹ã‚¢ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼ã§ã™ã€‚\n" +
            f"ãƒ¦ãƒ¼ã‚¶ãƒ¼èª²é¡Œ: {json.dumps(query, ensure_ascii=False)}\n\n" +
            "æ¬¡ã®ã‚³ãƒ¼ãƒ‰ã‚’åˆ†æã—ã€ä»¥ä¸‹ã®å½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ï¼š\n\n" +
            "## æ¤œå‡ºã•ã‚ŒãŸå•é¡Œ\n" +
            "1. **é‡‘é¡å‡¦ç†**: æµ®å‹•å°æ•°ç‚¹ã€ç¨è¨ˆç®—ã€ç«¯æ•°å‡¦ç†ã®å•é¡Œ\n" +
            "2. **å°åˆ·å‡¦ç†**: ã‚¨ãƒ©ãƒ¼å‡¦ç†ã€æ”¹ãƒšãƒ¼ã‚¸ã€ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã®å•é¡Œ\n" +
            "3. **UI/UX**: å…¥åŠ›æ¤œè¨¼ã€å¤šé‡ã‚¯ãƒªãƒƒã‚¯ã€XSSè„†å¼±æ€§\n" +
            "4. **DBè² è·**: N+1å•é¡Œã€SELECT *ã€å¤šé‡JOIN\n\n" +
            "## æ”¹å–„æ¡ˆã¨ä¿®æ­£ã‚³ãƒ¼ãƒ‰\n" +
            "å„å•é¡Œã«å¯¾ã—ã¦ï¼š\n" +
            "- å•é¡Œã®èª¬æ˜\n" +
            "- æ”¹å–„æ¡ˆ\n" +
            "- å…·ä½“çš„ãªä¿®æ­£ã‚³ãƒ¼ãƒ‰ä¾‹ï¼ˆBefore/Afterå½¢å¼ï¼‰\n\n" +
            "é‡è¦: ä¿®æ­£ã‚³ãƒ¼ãƒ‰ã¯å®Ÿéš›ã«å‹•ä½œã™ã‚‹å…·ä½“çš„ãªã‚³ãƒ¼ãƒ‰ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚\n" +
            "è¨€èªã¯å…ƒã®ã‚³ãƒ¼ãƒ‰ã¨åŒã˜ã‚‚ã®ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚\n\n" +
            f"=== åˆ†æå¯¾è±¡ã‚³ãƒ¼ãƒ‰ (ãƒãƒƒãƒ {batch_num}/{total_batches}) ===\n" +
            batch_json
        )

        try:
            print(f"[INFO] Calling AI for batch {batch_num}/{total_batches}...")
            resp = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "You are an expert code reviewer. Provide specific, actionable improvements with working code examples."},
                    {"role": "user", "content": prompt}
                ],
                temperature=1,  # GPT-5ã¯1ã®ã¿ã‚µãƒãƒ¼ãƒˆ
                max_completion_tokens=2000,  # ååˆ†ãªé•·ã•ã®å›ç­”ã‚’å¾—ã‚‹ãŸã‚ï¼ˆæ–°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰
                timeout=AI_TIMEOUT
            )
            review = resp.choices[0].message.content.strip()
            reviews.append(f"### ãƒãƒƒãƒ {batch_num} ({len(batch)}ãƒ•ã‚¡ã‚¤ãƒ«)\n\n{review}")
            print(f"[OK] Batch {batch_num} completed")
        except Exception as e:
            error_msg = f"(ãƒãƒƒãƒ{batch_num} AIã‚¨ãƒ©ãƒ¼: {str(e)[:200]})"
            reviews.append(error_msg)
            print(f"[ERROR] {error_msg}")

    # å…¨ãƒãƒƒãƒã®çµæœã‚’çµ±åˆ
    header = f"## ğŸ”§ AIæ”¹å–„ææ¡ˆãƒ¬ãƒãƒ¼ãƒˆ (å…¨{len(all_snippets)}ãƒ•ã‚¡ã‚¤ãƒ« / {total_batches}ãƒãƒƒãƒå‡¦ç†)\n"
    return header + "\n\n---\n\n".join(reviews)

# ===== Report =====
def render_report(title: str, items: List[Dict[str,Any]], ai_summary: str|None) -> str:
    lines: List[str] = [
        f"# {title}",
        "",
        f"ç”Ÿæˆ: {time.strftime('%Y-%m-%d %H:%M:%S')}",
        f"ãƒ¢ãƒ¼ãƒ‰: {'AIã«ã‚ˆã‚‹æ”¹å–„æ¡ˆä»˜ã' if ai_summary else 'ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®ã¿'}",
        ""
    ]

    if ai_summary:
        lines += [
            "## ğŸ¤– AIã«ã‚ˆã‚‹è©³ç´°åˆ†æã¨æ”¹å–„æ¡ˆ",
            "",
            ai_summary,
            "",
            "---",
            ""
        ]

    # å…¨ä»¶ã®ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’å‡ºåŠ›
    lines.append(f"## ğŸ“‹ æ¤œå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ (å…¨{len(items)}ä»¶)\n")
    for i, it in enumerate(items, 1):
        lines += [f"### {i}. {it['path']}  ({it['lang']})  score={it.get('score',0)}",
                  f"- tags: {', '.join(it.get('tags', [])) or '-'}"]
        susp = it.get("suspicions")
        lines += (["- é‡ç‚¹æŒ‡æ‘˜:"] + [f"  - {m}" for m in susp]) if susp else ["- é‡ç‚¹æŒ‡æ‘˜: ï¼ˆç‰¹ã«å¼·ã„å…†å€™ãªã—ï¼‰"]
        lines += [""]
    return "\n".join(lines)

def make_advice_entry(d: Dict[str,Any]) -> Dict[str,Any]:
    return {"path": d["path"], "lang": d["lang"], "score": round(d.get("_score",0.0),4),
            "tags": d.get("tags",[]), "suspicions": rule_advices(d)}

# ===== Commands =====
def cmd_query(query: str, topk: int, mode: str, index_path: pathlib.Path, out: pathlib.Path|None):
    index = load_index(index_path)
    cands = retrieve(query, index, topk=topk)
    items = [make_advice_entry(d) for d in cands]

    ai_summary = None
    if mode in ("ai","hybrid"):
        print(f"[INFO] Starting AI review with code solutions for {len(cands)} files...")
        ai_summary = ai_review_with_solutions(query, [d["text"] for d in cands])

    rep = render_report(f"æ¤œç´¢ãƒ¬ãƒãƒ¼ãƒˆ: {query}", items, ai_summary)
    if out:
        out.parent.mkdir(parents=True, exist_ok=True)
        out.write_text(rep, encoding="utf-8")
        print(f"[OK] wrote {out}")
    else:
        print(rep)

def cmd_advise(topk: int, mode: str, index_path: pathlib.Path, out: pathlib.Path|None):
    seed_query = "é‡‘é¡ ä¸æ•´åˆ ç¨ å°æ•° å°åˆ· Print HasMorePages NewPage UI UX ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£ DB N+1 JOIN é…ã„"
    index = load_index(index_path)
    cands = retrieve(seed_query, index, topk=topk)
    items = [make_advice_entry(d) for d in cands]

    ai_summary = None
    if mode in ("ai","hybrid"):
        print(f"[INFO] Starting AI review with code solutions for {len(cands)} files...")
        ai_summary = ai_review_with_solutions("æ¨ªæ–­åŠ©è¨€ï¼ˆé‡‘é¡/å°åˆ·/UI/DBï¼‰- æ”¹å–„æ¡ˆã¨ã‚³ãƒ¼ãƒ‰ä¾‹ã‚’æä¾›", [d["text"] for d in cands])

    rep = render_report("å…¨ä½“åŠ©è¨€ï¼ˆæ¨ªæ–­ãƒã‚§ãƒƒã‚¯ï¼‰", items, ai_summary)
    if out:
        out.parent.mkdir(parents=True, exist_ok=True)
        out.write_text(rep, encoding="utf-8")
        print(f"[OK] wrote {out}")
    else:
        print(rep)

# ===== CLI =====
if __name__ == "__main__":
    ap = argparse.ArgumentParser(description="èª­å–å°‚ç”¨ãƒ¬ãƒ“ãƒ¥ãƒ¼ with æ”¹å–„æ¡ˆãƒ»ä¿®æ­£ã‚³ãƒ¼ãƒ‰ä¾‹ï¼ˆGPT-5/GPT-4ï¼‰")
    sub = ap.add_subparsers(dest="cmd", required=True)

    ap_idx = sub.add_parser("index", help="ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ï¼ˆèª­å–ã®ã¿ï¼‰")
    ap_idx.add_argument("repo", type=str)

    ap_vec = sub.add_parser("vectorize", help="(ä»»æ„) TF-IDFãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆ")
    ap_vec.add_argument("--index", type=str, default=INDEX_PATH)

    ap_q = sub.add_parser("query", help="è‡ªç„¶è¨€èªæ¤œç´¢ï¼‹æ”¹å–„æ¡ˆãƒ»ä¿®æ­£ã‚³ãƒ¼ãƒ‰ä¾‹")
    ap_q.add_argument("query", type=str, help="æ¤œç´¢ã‚¯ã‚¨ãƒª")
    ap_q.add_argument("--topk", type=int, default=10, help="æ¤œç´¢ä¸Šä½Nä»¶")
    ap_q.add_argument("--mode", choices=["rules","ai","hybrid"], default="hybrid")
    ap_q.add_argument("--index", type=str, default=INDEX_PATH)
    ap_q.add_argument("--out", type=str, help="å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ« (Markdown)")

    ap_adv = sub.add_parser("advise", help="å…¨ä½“åŠ©è¨€ï¼ˆæ”¹å–„æ¡ˆãƒ»ä¿®æ­£ã‚³ãƒ¼ãƒ‰ä»˜ãï¼‰")
    ap_adv.add_argument("--topk", type=int, default=20, help="åŠ©è¨€å¯¾è±¡ä¸Šä½Nä»¶")
    ap_adv.add_argument("--mode", choices=["rules","ai","hybrid"], default="hybrid")
    ap_adv.add_argument("--index", type=str, default=INDEX_PATH)
    ap_adv.add_argument("--out", type=str, help="å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ« (Markdown)")

    args = ap.parse_args()
    if args.cmd == "index":
        repo = pathlib.Path(args.repo); cmd_index(repo, pathlib.Path(INDEX_PATH))
    elif args.cmd == "vectorize":
        cmd_vectorize(pathlib.Path(args.index))
    elif args.cmd == "query":
        out = pathlib.Path(args.out) if args.out else None
        cmd_query(args.query, args.topk, args.mode, pathlib.Path(args.index), out)
    elif args.cmd == "advise":
        out = pathlib.Path(args.out) if args.out else None
        cmd_advise(args.topk, args.mode, pathlib.Path(args.index), out)