# Elasticsearch Optimization and Performance Rules
# Auto-generated by BugSearch2 - Database-Specific Rules
# Generated: 2025-10-14
# Category: database
# Target: Elasticsearch Search Engine

rule:
  id: ELASTICSEARCH_DEEP_PAGINATION
  category: database
  name: Elasticsearch Deep Pagination
  description: |
    Deep pagination (from + size > 10000) causes severe memory and performance issues.
    Elasticsearch loads ALL results into coordinator node memory before pagination.

    Problems:
    - Coordinator node OOM (loads from * size results)
    - Exponential memory growth with page depth
    - Query timeout on large result sets
    - Result Window exception (default max: 10000)

    Use scroll API or search_after for deep pagination.
  base_severity: 10

  patterns:
    java:
      - pattern: 'from\(\s*\d{5,}\s*\)'
        context: 'Java Elasticsearch client deep pagination (from > 10000)'
      - pattern: 'setFrom\(\s*\d{5,}\s*\)'
        context: 'Java setFrom with large offset'
      - pattern: '\.from\s*=\s*\d{5,}'
        context: 'Java SearchRequest from parameter'

    python:
      - pattern: "[\"'](from|From)[\"']\\s*:\\s*\\d{5,}"
        context: 'Python Elasticsearch deep pagination'
      - pattern: 'from_=\d{5,}'
        context: 'Python elasticsearch-py from parameter'
      - pattern: 'search\(.*from_=.*\d{4,}'
        context: 'Python search with high from value'

    javascript:
      - pattern: 'from:\s*\d{5,}'
        context: 'JavaScript Elasticsearch deep pagination'
      - pattern: 'client\.search\(.*from:\s*\d{4,}'
        context: 'Node.js Elasticsearch client pagination'

    csharp:
      - pattern: 'From\(\s*\d{5,}\s*\)'
        context: 'C# NEST client deep pagination'
      - pattern: '\.From\s*=\s*\d{5,}'
        context: 'C# SearchRequest From property'

    go:
      - pattern: 'From\(\s*\d{5,}\s*\)'
        context: 'Go Elasticsearch client deep pagination'

  fixes:
    description: |
      Use scroll API or search_after for deep pagination:

      // Bad - Deep pagination
      GET /my-index/_search
      {
        "from": 10000,
        "size": 100,
        "query": {...}
      }

      // Good - Scroll API for large result sets
      POST /my-index/_search?scroll=1m
      {
        "size": 1000,
        "query": {...}
      }
      // Then use scroll_id for next pages

      // Better - search_after (recommended for real-time pagination)
      GET /my-index/_search
      {
        "size": 100,
        "query": {...},
        "sort": [{"timestamp": "asc"}],
        "search_after": [1609459200000]  // Last sort value
      }

      Adjust index.max_result_window (use with caution):
      PUT /my-index/_settings
      {
        "index.max_result_window": 50000
      }

    references:
      - 'Pagination guide: https://www.elastic.co/guide/en/elasticsearch/reference/current/paginate-search-results.html'
      - 'search_after: https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-search-after.html'

---

rule:
  id: ELASTICSEARCH_WILDCARD_PREFIX
  category: database
  name: Elasticsearch Leading Wildcard Query
  description: |
    Leading wildcard queries (*term or ?term) force full index scan.
    Cannot use inverted index, resulting in extremely slow queries.

    Problems:
    - Scans ALL documents in ALL shards
    - Query time proportional to index size
    - High CPU usage on data nodes
    - Cannot be cached effectively

    Use ngram tokenizer or suffix analyzer instead.
  base_severity: 10

  patterns:
    java:
      - pattern: 'wildcardQuery\([^,]+,\s*["\'][\*\?]'
        context: 'Java Elasticsearch wildcard query with leading wildcard'
      - pattern: 'query:\s*["\'][*?][^"\']*["\']'
        context: 'Java query with leading wildcard'

    python:
      - pattern: 'wildcard["\']:\s*{.*["\']value["\']\s*:\s*["\'][*?]'
        context: 'Python Elasticsearch wildcard query'
      - pattern: '[\'"]\*\w+["\']|["\']?\w+["\']'
        context: 'Python wildcard pattern'

    javascript:
      - pattern: 'wildcard:\s*{.*value:\s*["\'][*?]'
        context: 'JavaScript Elasticsearch wildcard query'

    csharp:
      - pattern: 'Wildcard\([^,]+,\s*["\'][*?]'
        context: 'C# NEST wildcard query with leading wildcard'

  fixes:
    description: |
      Use ngram or edge ngram tokenizer:

      // Bad - Leading wildcard
      GET /products/_search
      {
        "query": {
          "wildcard": {"name": "*phone"}
        }
      }

      // Good - Ngram tokenizer
      PUT /products
      {
        "settings": {
          "analysis": {
            "tokenizer": {
              "ngram_tokenizer": {
                "type": "ngram",
                "min_gram": 3,
                "max_gram": 4
              }
            },
            "analyzer": {
              "ngram_analyzer": {
                "tokenizer": "ngram_tokenizer"
              }
            }
          }
        },
        "mappings": {
          "properties": {
            "name": {
              "type": "text",
              "analyzer": "ngram_analyzer"
            }
          }
        }
      }

      // Query with match (uses ngram index)
      GET /products/_search
      {
        "query": {"match": {"name": "phone"}}
      }

    references:
      - 'Wildcard performance: https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-wildcard-query.html'
      - 'Ngram tokenizer: https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-ngram-tokenizer.html'

---

rule:
  id: ELASTICSEARCH_MISSING_ROUTING
  category: database
  name: Elasticsearch Missing Routing Parameter
  description: |
    Queries without routing parameter scan ALL shards.
    With routing, query targets only specific shard(s).

    Problems without routing:
    - Query broadcasts to ALL shards (default: 1-5 per index)
    - Increases latency linearly with shard count
    - Unnecessary network overhead
    - Coordinator aggregation overhead

    Use routing parameter for tenant/user-based data.
  base_severity: 8

  patterns:
    java:
      - pattern: 'search\([^)]*\)(?!.*routing)'
        context: 'Java Elasticsearch search without routing'
      - pattern: 'SearchRequest\([^)]*\)(?!.*routing)'
        context: 'Java SearchRequest without routing'

    python:
      - pattern: 'search\([^)]*\)(?!.*routing)'
        context: 'Python elasticsearch-py search without routing'

    javascript:
      - pattern: 'client\.search\(\{[^}]*\}(?!.*routing)'
        context: 'JavaScript Elasticsearch client search without routing'

  fixes:
    description: |
      Add routing parameter for tenant-based queries:

      // Bad - Broadcasts to all shards
      GET /logs/_search
      {
        "query": {
          "term": {"tenant_id": "tenant_123"}
        }
      }

      // Good - Routes to specific shard
      GET /logs/_search?routing=tenant_123
      {
        "query": {
          "term": {"tenant_id": "tenant_123"}
        }
      }

      Define routing in mapping:
      PUT /logs
      {
        "mappings": {
          "_routing": {"required": true},
          "properties": {
            "tenant_id": {"type": "keyword"}
          }
        }
      }

      Index with routing:
      POST /logs/_doc?routing=tenant_123
      {
        "tenant_id": "tenant_123",
        "message": "Log message"
      }

      Performance improvement: 5-10x faster for multi-tenant apps

    references:
      - 'Custom routing: https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-routing-field.html'

---

rule:
  id: ELASTICSEARCH_SCRIPT_NO_CACHE
  category: database
  name: Elasticsearch Script Query Without Caching
  description: |
    Inline scripts without caching are recompiled for EVERY query.
    Compilation is expensive (10-100ms per query).

    Problems:
    - High CPU usage for script compilation
    - Increased query latency
    - Memory overhead for script engines
    - Cannot use query cache

    Use stored scripts or painless script caching.
  base_severity: 8

  patterns:
    java:
      - pattern: 'Script\(["\']inline["\']'
        context: 'Java Elasticsearch inline script'
      - pattern: 'new\s+Script\(["\'][^"\']*["\'](?!.*id)'
        context: 'Java Script without script ID'

    python:
      - pattern: 'script["\']\s*:\s*{.*["\']inline["\']'
        context: 'Python Elasticsearch inline script'
      - pattern: '[\'"](source|inline)["\']\s*:'
        context: 'Python inline script source'

    javascript:
      - pattern: 'script:\s*{.*inline:'
        context: 'JavaScript Elasticsearch inline script'

  fixes:
    description: |
      Use stored scripts with caching:

      // Bad - Inline script (recompiled every time)
      GET /products/_search
      {
        "query": {
          "script_score": {
            "query": {"match_all": {}},
            "script": {
              "source": "doc['price'].value * params.discount",
              "params": {"discount": 0.9}
            }
          }
        }
      }

      // Good - Stored script (compiled once, cached)
      POST _scripts/calculate_discount
      {
        "script": {
          "lang": "painless",
          "source": "doc['price'].value * params.discount"
        }
      }

      GET /products/_search
      {
        "query": {
          "script_score": {
            "query": {"match_all": {}},
            "script": {
              "id": "calculate_discount",
              "params": {"discount": 0.9}
            }
          }
        }
      }

      // Or enable script.cache.max_size (default: 100)
      PUT /_cluster/settings
      {
        "transient": {
          "script.cache.max_size": "200"
        }
      }

    references:
      - 'Script caching: https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-scripting-using.html'
      - 'Stored scripts: https://www.elastic.co/guide/en/elasticsearch/reference/current/script-stored-scripts.html'

---

rule:
  id: ELASTICSEARCH_NESTED_ABUSE
  category: database
  name: Elasticsearch Excessive Nested Documents
  description: |
    Nested documents have severe performance implications.
    Each nested doc is stored as separate Lucene document.

    Problems:
    - Increases index size exponentially
    - Slow indexing (all nested docs must be reindexed on update)
    - Memory overhead for nested queries
    - Limited to 50-10000 nested docs per parent

    Use parent-child relationship or denormalization instead.
  base_severity: 9

  patterns:
    java:
      - pattern: 'nested["\']:\s*{.*["\']path["\']'
        context: 'Java Elasticsearch nested query'
      - pattern: 'type["\']:\s*["\']nested["\']'
        context: 'Java nested field mapping'

    python:
      - pattern: 'nested["\']\s*:\s*{.*["\']path["\']'
        context: 'Python Elasticsearch nested query'

    javascript:
      - pattern: 'nested:\s*{.*path:'
        context: 'JavaScript Elasticsearch nested query'

  fixes:
    description: |
      Denormalize or use parent-child:

      // Bad - Nested documents (1 user = 1000 orders)
      PUT /users/_doc/1
      {
        "name": "John",
        "orders": [
          {"id": 1, "total": 100},
          {"id": 2, "total": 200},
          ...
          {"id": 1000, "total": 50}  // 1000 nested docs!
        ]
      }

      // Good - Separate index with join field
      PUT /users/_doc/1
      {
        "name": "John",
        "user_order": {"name": "user"}
      }

      PUT /users/_doc/1001?routing=1
      {
        "order_id": 1,
        "total": 100,
        "user_order": {
          "name": "order",
          "parent": 1
        }
      }

      // Better - Denormalize if possible
      PUT /orders/_doc/1
      {
        "order_id": 1,
        "user_id": 1,
        "user_name": "John",  // Denormalized
        "total": 100
      }

      Nested document limits:
      - index.mapping.nested_fields.limit: 50 (default)
      - index.mapping.nested_objects.limit: 10000 (default)

    references:
      - 'Nested vs parent-child: https://www.elastic.co/guide/en/elasticsearch/reference/current/nested.html'
      - 'Parent-child: https://www.elastic.co/guide/en/elasticsearch/reference/current/parent-join.html'

---

rule:
  id: ELASTICSEARCH_TOO_MANY_SHARDS
  category: database
  name: Elasticsearch Over-Sharding
  description: |
    Too many shards causes cluster state bloat and poor performance.
    Each shard has fixed overhead (~10MB memory, file handles).

    Problems:
    - Master node memory pressure (cluster state)
    - Slow cluster state updates
    - Increased query latency (aggregation overhead)
    - Wasted resources (empty shards)

    Recommended: 10-50GB per shard, <1000 shards per node.
  base_severity: 8

  patterns:
    java:
      - pattern: 'number_of_shards["\']:\s*\d{3,}'
        context: 'Java Elasticsearch index with too many shards'
      - pattern: 'numberOfShards\(\s*\d{3,}\s*\)'
        context: 'Java index settings with excessive shards'

    python:
      - pattern: '[\'"](number_of_shards|numberOfShards)["\']\s*:\s*\d{3,}'
        context: 'Python Elasticsearch index with too many shards'

  fixes:
    description: |
      Use appropriate shard sizing:

      // Bad - Over-sharding (100 shards for 10GB index)
      PUT /small-index
      {
        "settings": {
          "number_of_shards": 100,
          "number_of_replicas": 1
        }
      }

      // Good - Proper shard sizing (1-2 shards for 10GB)
      PUT /small-index
      {
        "settings": {
          "number_of_shards": 2,
          "number_of_replicas": 1
        }
      }

      Shard sizing guidelines:
      - Target shard size: 10-50GB
      - Max shards per node: 1000 (soft limit)
      - Total shards = (index size / target shard size) * (1 + replicas)

      Example: 100GB index, 2 replicas
      - Shards: 100GB / 20GB = 5 primary shards
      - Total: 5 * 3 = 15 shards

      For time-series data, use ILM with rollover:
      PUT /_ilm/policy/logs-policy
      {
        "policy": {
          "phases": {
            "hot": {
              "actions": {
                "rollover": {
                  "max_size": "50GB",
                  "max_age": "7d"
                }
              }
            }
          }
        }
      }

    references:
      - 'Shard sizing: https://www.elastic.co/guide/en/elasticsearch/reference/current/size-your-shards.html'
      - 'ILM: https://www.elastic.co/guide/en/elasticsearch/reference/current/index-lifecycle-management.html'

---

rule:
  id: ELASTICSEARCH_MAPPING_EXPLOSION
  category: database
  name: Elasticsearch Mapping Explosion
  description: |
    Dynamic mapping with high-cardinality fields causes mapping explosion.
    Cluster state grows unbounded, causing master node OOM.

    Problems:
    - Cluster state size > 100MB (critical threshold)
    - Master election failures
    - Slow cluster state updates
    - Cannot add new mappings

    Disable dynamic mapping for high-cardinality fields.
  base_severity: 9

  patterns:
    java:
      - pattern: 'dynamic["\']:\s*["\']true["\']'
        context: 'Java Elasticsearch dynamic mapping enabled'
      - pattern: 'setDynamic\(["\']true["\']'
        context: 'Java mapping with dynamic=true'

    python:
      - pattern: '[\'"](dynamic|Dynamic)["\']\s*:\s*["\']?true["\']?'
        context: 'Python Elasticsearch dynamic mapping'

  fixes:
    description: |
      Disable dynamic mapping or use strict mode:

      // Bad - Dynamic mapping (unlimited fields)
      PUT /logs
      {
        "mappings": {
          "dynamic": "true",  // Dangerous for logs with arbitrary fields
          "properties": {
            "message": {"type": "text"}
          }
        }
      }

      // Good - Dynamic mapping disabled
      PUT /logs
      {
        "mappings": {
          "dynamic": "strict",  // Reject unknown fields
          "properties": {
            "message": {"type": "text"},
            "timestamp": {"type": "date"},
            "level": {"type": "keyword"}
          }
        }
      }

      // Better - Use flattened datatype for arbitrary fields
      PUT /logs
      {
        "mappings": {
          "dynamic": "strict",
          "properties": {
            "message": {"type": "text"},
            "metadata": {"type": "flattened"}  // Arbitrary key-value pairs
          }
        }
      }

      Monitor mapping count:
      GET /_cluster/stats
      // Check indices.mappings.field_types

      Mapping limits:
      - index.mapping.total_fields.limit: 1000 (default)
      - index.mapping.depth.limit: 20 (default)

    references:
      - 'Dynamic mapping: https://www.elastic.co/guide/en/elasticsearch/reference/current/dynamic-mapping.html'
      - 'Flattened datatype: https://www.elastic.co/guide/en/elasticsearch/reference/current/flattened.html'

---

rule:
  id: ELASTICSEARCH_NO_QUERY_CACHE
  category: database
  name: Elasticsearch Query Cache Not Utilized
  description: |
    Filter queries are cacheable but must use filter context.
    Query context (must) is NOT cached.

    Benefits of filter context:
    - Results cached in query cache (10% heap)
    - Faster subsequent queries (100x speedup)
    - No relevance scoring overhead

    Use bool query with filter clause.
  base_severity: 7

  patterns:
    java:
      - pattern: 'must\([^)]+termQuery'
        context: 'Java Elasticsearch must clause (not cached)'
      - pattern: 'QueryBuilders\.termQuery(?!.*filter)'
        context: 'Java term query in query context'

    python:
      - pattern: '[\'"](must|Must)["\']\s*:\s*\[.*term'
        context: 'Python Elasticsearch must clause'

  fixes:
    description: |
      Use filter context for cacheable queries:

      // Bad - Query context (not cached)
      GET /products/_search
      {
        "query": {
          "bool": {
            "must": [
              {"term": {"status": "active"}},
              {"range": {"price": {"gte": 10}}}
            ]
          }
        }
      }

      // Good - Filter context (cached)
      GET /products/_search
      {
        "query": {
          "bool": {
            "filter": [
              {"term": {"status": "active"}},
              {"range": {"price": {"gte": 10}}}
            ]
          }
        }
      }

      // Best - Combine query and filter
      GET /products/_search
      {
        "query": {
          "bool": {
            "must": [
              {"match": {"description": "laptop"}}  // Scored
            ],
            "filter": [
              {"term": {"status": "active"}},       // Cached
              {"range": {"price": {"gte": 10}}}     // Cached
            ]
          }
        }
      }

      Query cache stats:
      GET /_nodes/stats/indices/query_cache

    references:
      - 'Query and filter context: https://www.elastic.co/guide/en/elasticsearch/reference/current/query-filter-context.html'
      - 'Query cache: https://www.elastic.co/guide/en/elasticsearch/reference/current/query-cache.html'
