# Memcached Optimization and Best Practices
# Auto-generated by BugSearch2 - Database-Specific Rules
# Generated: 2025-10-14
# Category: database
# Target: Memcached Distributed Memory Caching

rule:
  id: MEMCACHED_KEY_TOO_LONG
  category: database
  name: Memcached Key Length Exceeds Limit
  description: |
    Memcached key length is limited to 250 bytes.
    Longer keys are silently rejected or truncated.

    Problems:
    - Key rejected (error or silent failure)
    - Cache miss (key not stored)
    - Truncated keys cause collisions
    - Application errors
    - Debugging difficulty

    Keep keys under 250 bytes, use hashing for long keys.
  base_severity: 9

  patterns:
    java:
      - pattern: 'set\\([\'"][^\'"]{250,}[\'"]|add\\([\'"][^\'"]{250,}[\'"]'
        context: 'Java Memcached set/add with long key (>250 chars)'

    python:
      - pattern: 'set\\([\'"][^\'"]{250,}[\'"]|add\\([\'"][^\'"]{250,}[\'"]'
        context: 'Python Memcached set/add with long key'

    php:
      - pattern: '->set\\([\'"][^\'"]{250,}|->add\\([\'"][^\'"]{250,}'
        context: 'PHP Memcached long key'

    javascript:
      - pattern: 'set\\([\'"][^\'"]{250,}[\'"]'
        context: 'Node.js Memcached long key'

  fixes:
    description: |
      Keep keys short or use hashing:

      // Bad - Key too long (>250 bytes)
      String key = "user:12345:profile:full_details:with_orders:and_preferences:2024";
      client.set(key, 3600, userData);  // May be rejected!

      // Good - Short key
      String key = "user:12345:prof";
      client.set(key, 3600, userData);

      // Better - Hash long keys (MD5/SHA1)
      String longKey = "user:12345:profile:full_details:with_orders:and_preferences:2024";
      String key = "cache:" + md5(longKey);  // Always <250 bytes
      client.set(key, 3600, userData);

      # Python example - Bad
      long_key = f"user:{user_id}:session:{session_id}:preferences:{preferences_json}"
      memcache_client.set(long_key, data)  # May fail silently!

      # Python example - Good (hashing)
      import hashlib
      long_key = f"user:{user_id}:session:{session_id}:preferences:{preferences_json}"
      key = "cache:" + hashlib.md5(long_key.encode()).hexdigest()
      memcache_client.set(key, data)

      // PHP - Use hashing for long keys
      $longKey = "user:{$userId}:profile:details:orders:preferences";
      $key = "cache:" . md5($longKey);
      $memcache->set($key, $data, 3600);

      // JavaScript/Node.js - crypto hashing
      const crypto = require('crypto');
      const longKey = `user:${userId}:profile:${timestamp}`;
      const key = 'cache:' + crypto.createHash('md5').update(longKey).digest('hex');
      memcached.set(key, data, 3600, (err) => {
        if (err) console.error('Memcached set error:', err);
      });

      Best practices:
      - Key format: "namespace:id:version" (short and structured)
      - Use consistent naming convention
      - Hash keys >200 bytes (leave safety margin)
      - Log original key for debugging (map hash -> original)

    references:
      - 'Memcached protocol: https://github.com/memcached/memcached/blob/master/doc/protocol.txt'
      - 'Key limitations: https://www.php.net/manual/en/memcached.set.php'

---

rule:
  id: MEMCACHED_VALUE_TOO_LARGE
  category: database
  name: Memcached Value Size Exceeds Limit
  description: |
    Memcached value size is limited to 1MB by default.
    Larger values are rejected.

    Problems:
    - Value rejected (storage failed)
    - Cache miss (item not stored)
    - Memory waste (large items)
    - Poor cache efficiency
    - Network overhead

    Keep values small or use compression.
  base_severity: 8

  patterns:
    java:
      - pattern: 'set\\([^,]*,\\s*[^,]*,\\s*new\\s+byte\\[\\d{7,}\\]'
        context: 'Java Memcached set with large byte array (>1MB)'

    python:
      - pattern: 'set\\([^,]*,\\s*[^,]*\\).*len\\([^)]*\\)\\s*>\\s*1048576'
        context: 'Python Memcached large value check'

  fixes:
    description: |
      Keep values small or use compression:

      -- Check Memcached value size limit
      memcached -I 1m  # Default: 1MB
      memcached -I 128m  # Increase to 128MB (not recommended)

      // Bad - Large value (>1MB)
      String largeHtml = generateHtmlReport();  // 5MB HTML
      client.set("report:123", 3600, largeHtml);  // REJECTED!

      // Good - Compress large values
      import java.util.zip.GZIPOutputStream;

      String largeHtml = generateHtmlReport();
      ByteArrayOutputStream baos = new ByteArrayOutputStream();
      GZIPOutputStream gzip = new GZIPOutputStream(baos);
      gzip.write(largeHtml.getBytes("UTF-8"));
      gzip.close();
      byte[] compressed = baos.toByteArray();

      if (compressed.length < 1048576) {  // <1MB
        client.set("report:123:gz", 3600, compressed);
      } else {
        // Store in database or file system, not Memcached
        saveToDatabase("report:123", largeHtml);
      }

      # Python - Compression with gzip
      import gzip
      import json

      large_data = generate_large_object()  # 5MB JSON
      json_str = json.dumps(large_data)
      compressed = gzip.compress(json_str.encode('utf-8'))

      if len(compressed) < 1048576:  # <1MB
          memcache_client.set('data:123:gz', compressed)
      else:
          # Store elsewhere (S3, database)
          save_to_s3('data:123', large_data)

      // PHP - Check size before set
      $largeData = serialize($dataObject);
      $dataSize = strlen($largeData);

      if ($dataSize > 1048576) {  // >1MB
          // Compress
          $compressed = gzip_encode($largeData);
          if (strlen($compressed) < 1048576) {
              $memcache->set("data:123:gz", $compressed, 3600);
          } else {
              // Store in Redis or database
              $redis->set("data:123", $largeData);
          }
      } else {
          $memcache->set("data:123", $largeData, 3600);
      }

      // JavaScript/Node.js - zlib compression
      const zlib = require('zlib');

      const largeData = JSON.stringify(generateLargeObject());
      zlib.gzip(largeData, (err, compressed) => {
          if (err) throw err;

          if (compressed.length < 1048576) {  // <1MB
              memcached.set('data:123:gz', compressed, 3600);
          } else {
              // Store elsewhere
              saveToDatabase('data:123', largeData);
          }
      });

      Best practices:
      - Ideal value size: <100KB (fast serialization)
      - Use compression for >10KB values
      - Split large objects into multiple keys
      - Store very large data (>1MB) in database/file system

      Monitor value sizes:
      stats sizes  # Memcached command

    references:
      - 'Size limits: https://github.com/memcached/memcached/wiki/ConfiguringServer#-I'
      - 'Compression: https://www.php.net/manual/en/memcached.compression.php'

---

rule:
  id: MEMCACHED_NO_EXPIRATION
  category: database
  name: Memcached Item Without TTL
  description: |
    Items without expiration time (TTL=0) stay forever until evicted.
    This causes memory pressure and eviction of hot items.

    Problems:
    - Memory fills up (no expiration)
    - Hot items evicted (LRU)
    - Cache hit rate drops
    - Unpredictable eviction
    - Memory waste

    Always set appropriate TTL for cache items.
  base_severity: 8

  patterns:
    java:
      - pattern: 'set\\([^,]*,\\s*0\\s*,'
        context: 'Java Memcached set with TTL=0 (no expiration)'

    python:
      - pattern: 'set\\([^,]*,\\s*[^,]*,\\s*0\\)|set\\([^,]*,\\s*[^,]*\\)(?!.*time=)'
        context: 'Python Memcached set without TTL'

    php:
      - pattern: '->set\\([^,]*,\\s*[^,]*,\\s*0\\)'
        context: 'PHP Memcached set with expiration=0'

    javascript:
      - pattern: 'set\\([^,]*,\\s*[^,]*,\\s*0\\s*,'
        context: 'Node.js Memcached set with lifetime=0'

  fixes:
    description: |
      Always set appropriate TTL:

      // Bad - No expiration (TTL=0)
      client.set("user:12345", 0, userData);  // Stays forever!
      // When memory is full, Memcached evicts items using LRU
      // Your "hot" data may be evicted to make room for old data

      // Good - Set appropriate TTL
      client.set("user:12345", 3600, userData);  // Expires in 1 hour

      // Better - Different TTLs for different data types
      // Session data: 30 minutes
      client.set("session:" + sessionId, 1800, sessionData);

      // User profile: 1 hour
      client.set("user:" + userId, 3600, userProfile);

      // Database query result: 5 minutes
      client.set("query:" + queryHash, 300, queryResult);

      // Static content: 1 day
      client.set("static:homepage", 86400, homepageHtml);

      # Python - Always specify time
      memcache_client.set('user:123', user_data, time=3600)  # 1 hour

      # Python - Use constants for readability
      TTL_SHORT = 300      # 5 minutes
      TTL_MEDIUM = 3600    # 1 hour
      TTL_LONG = 86400     # 1 day

      memcache_client.set('session:abc', session_data, time=TTL_SHORT)
      memcache_client.set('user:123', user_data, time=TTL_MEDIUM)
      memcache_client.set('config', config_data, time=TTL_LONG)

      // PHP - Set expiration
      $memcache->set('user:123', $userData, 3600);  // 1 hour

      // JavaScript/Node.js - Set lifetime
      memcached.set('user:123', userData, 3600, (err) => {
          if (err) console.error(err);
      });

      Best practices:
      - Never use TTL=0 (no expiration)
      - Short TTL (5-15 min): volatile data, DB queries
      - Medium TTL (1 hour): user sessions, API responses
      - Long TTL (24 hours): static content, config
      - Consider using Redis if you need true persistence

      Monitor evictions:
      stats  # Check 'evictions' counter
      # If evictions > 0, increase memory or reduce TTLs

    references:
      - 'Expiration times: https://github.com/memcached/memcached/wiki/Commands#storage-commands'
      - 'LRU eviction: https://github.com/memcached/memcached/wiki/NewLRU'

---

rule:
  id: MEMCACHED_NO_CONNECTION_POOL
  category: database
  name: Memcached Without Connection Pooling
  description: |
    Creating new Memcached connections for each request is inefficient.
    Connection pooling reduces overhead and improves performance.

    Problems:
    - Connection overhead (TCP handshake)
    - Socket creation/destruction cost
    - Poor performance (10-100x slower)
    - Resource exhaustion
    - Connection limit reached

    Use connection pooling for all Memcached clients.
  base_severity: 8

  patterns:
    java:
      - pattern: 'new\\s+MemcachedClient\\('
        context: 'Java creating new MemcachedClient (check pooling)'

    python:
      - pattern: 'memcache\\.Client\\(|pylibmc\\.Client\\('
        context: 'Python creating Memcache client (check pooling)'

    php:
      - pattern: 'new\\s+Memcache\\(|new\\s+Memcached\\('
        context: 'PHP creating Memcache instance (check pooling)'

  fixes:
    description: |
      Use connection pooling:

      // Bad - New connection per request (Java)
      public User getUser(int userId) {
          MemcachedClient client = new MemcachedClient(
              new InetSocketAddress("localhost", 11211)
          );  // New connection each time!

          User user = (User) client.get("user:" + userId);
          client.shutdown();  // Wasteful!
          return user;
      }

      // Good - Singleton connection (Java)
      public class CacheManager {
          private static final MemcachedClient client;

          static {
              try {
                  client = new MemcachedClient(
                      new InetSocketAddress("localhost", 11211)
                  );
              } catch (IOException e) {
                  throw new RuntimeException(e);
              }
          }

          public static MemcachedClient getClient() {
              return client;
          }
      }

      // Usage
      User user = (User) CacheManager.getClient().get("user:" + userId);

      # Python - Bad (new connection per request)
      def get_user(user_id):
          client = memcache.Client(['127.0.0.1:11211'])  # New connection!
          user = client.get(f'user:{user_id}')
          return user

      # Python - Good (module-level singleton)
      import memcache
      memcache_client = memcache.Client(['127.0.0.1:11211'])  # Once at module load

      def get_user(user_id):
          user = memcache_client.get(f'user:{user_id}')
          return user

      # Python - Better (pylibmc with connection pooling)
      import pylibmc

      memcache_pool = pylibmc.ClientPool()
      memcache_pool.fill(
          pylibmc.Client(['127.0.0.1:11211']),
          10  # Pool size
      )

      def get_user(user_id):
          with memcache_pool.reserve() as client:
              user = client.get(f'user:{user_id}')
          return user

      // PHP - Persistent connections
      $memcache = new Memcached('persistent_id');  // Persistent connection
      $memcache->setOption(Memcached::OPT_LIBKETAMA_COMPATIBLE, true);

      if (!$memcache->getServerList()) {
          $memcache->addServer('localhost', 11211);
      }

      // JavaScript/Node.js - Single client instance
      const Memcached = require('memcached');
      const memcached = new Memcached('localhost:11211', {
          poolSize: 10,  // Connection pool
          retries: 3,
          retry: 1000
      });

      module.exports = memcached;  // Export singleton

      Best practices:
      - Use singleton or dependency injection
      - Configure connection pool size (10-50 connections)
      - Enable keep-alive
      - Set connection timeout
      - Monitor connection count

    references:
      - 'Java Spymemcached: https://github.com/couchbase/spymemcached'
      - 'Python pylibmc: https://sendapatch.se/projects/pylibmc/'
      - 'PHP Memcached: https://www.php.net/manual/en/memcached.construct.php'

---

rule:
  id: MEMCACHED_CACHE_STAMPEDE
  category: database
  name: Memcached Cache Stampede
  description: |
    Cache stampede occurs when many requests try to regenerate expired cache simultaneously.
    This causes database overload.

    Problems:
    - Simultaneous cache misses
    - Database overload (thundering herd)
    - Response time spike
    - Possible database crash
    - User-facing errors

    Use cache locking or probabilistic early expiration.
  base_severity: 9

  patterns:
    java:
      - pattern: 'get\\([^)]*\\).*==\\s*null.*set\\('
        context: 'Java cache miss without locking (stampede risk)'

    python:
      - pattern: 'get\\([^)]*\\).*is\\s+None.*set\\('
        context: 'Python cache miss without locking'

  fixes:
    description: |
      Prevent cache stampede:

      // Bad - No protection against stampede
      public User getUser(int userId) {
          String key = "user:" + userId;
          User user = (User) cache.get(key);

          if (user == null) {  // Cache miss
              // If 1000 requests hit here simultaneously...
              user = database.loadUser(userId);  // 1000 DB queries!
              cache.set(key, 3600, user);
          }
          return user;
      }

      // Good - Cache locking (mutex)
      public User getUser(int userId) {
          String key = "user:" + userId;
          String lockKey = "lock:" + key;

          User user = (User) cache.get(key);
          if (user == null) {
              // Try to acquire lock
              boolean acquired = cache.add(lockKey, 30, "1");  // 30s lock

              if (acquired) {
                  try {
                      // Only ONE thread rebuilds cache
                      user = database.loadUser(userId);
                      cache.set(key, 3600, user);
                  } finally {
                      cache.delete(lockKey);  // Release lock
                  }
              } else {
                  // Wait and retry (other thread is rebuilding)
                  Thread.sleep(100);
                  return getUser(userId);  // Retry
              }
          }
          return user;
      }

      # Python - Probabilistic early expiration (better approach)
      import time
      import random

      def get_user_with_pee(user_id):
          """Probabilistic Early Expiration to prevent stampede"""
          key = f'user:{user_id}'
          cached = memcache_client.get(key)

          if cached:
              data, expire_time = cached
              time_left = expire_time - time.time()
              ttl = 3600  # Original TTL

              # Probability increases as expiration approaches
              # P = 1 - (time_left / ttl)
              if time_left > 0:
                  recompute_probability = 1 - (time_left / ttl)
                  if random.random() < recompute_probability:
                      # Recompute early (before expiration)
                      data = load_user_from_db(user_id)
                      expire_time = time.time() + ttl
                      memcache_client.set(key, (data, expire_time), time=ttl)

              return data

          # Cache miss - normal flow
          data = load_user_from_db(user_id)
          expire_time = time.time() + 3600
          memcache_client.set(key, (data, expire_time), time=3600)
          return data

      # Python - Cache locking approach
      import time

      def get_user_with_lock(user_id):
          key = f'user:{user_id}'
          lock_key = f'lock:{key}'

          user = memcache_client.get(key)
          if user is None:
              # Try to acquire lock
              acquired = memcache_client.add(lock_key, '1', time=30)  # 30s lock

              if acquired:
                  try:
                      user = load_user_from_db(user_id)
                      memcache_client.set(key, user, time=3600)
                  finally:
                      memcache_client.delete(lock_key)
              else:
                  # Wait for other thread to finish
                  time.sleep(0.1)
                  return get_user_with_lock(user_id)  # Retry

          return user

      // PHP - Cache locking
      function getUser($userId) {
          $key = "user:{$userId}";
          $lockKey = "lock:{$key}";

          $user = $memcache->get($key);
          if ($user === false) {
              // Try lock
              if ($memcache->add($lockKey, 1, 30)) {  // 30s lock
                  try {
                      $user = loadUserFromDb($userId);
                      $memcache->set($key, $user, 3600);
                  } finally {
                      $memcache->delete($lockKey);
                  }
              } else {
                  usleep(100000);  // Wait 100ms
                  return getUser($userId);  // Retry
              }
          }
          return $user;
      }

      Best practices:
      - Use probabilistic early expiration (preferred)
      - Or use cache locking with reasonable timeout
      - Monitor cache miss rate
      - Set appropriate TTLs
      - Consider using Redis with SETNX for distributed locks

    references:
      - 'Cache stampede: https://en.wikipedia.org/wiki/Cache_stampede'
      - 'Probabilistic early expiration: https://www.sobstel.org/blog/preventing-dogpile-effect/'

---

rule:
  id: MEMCACHED_SERIALIZATION_OVERHEAD
  category: database
  name: Memcached Inefficient Serialization
  description: |
    Inefficient serialization formats cause performance overhead.
    Memcached stores byte arrays, serialization format matters.

    Problems:
    - Slow serialization (CPU overhead)
    - Large serialized size (memory waste)
    - Network bandwidth waste
    - Poor cache efficiency
    - GC pressure (deserialization allocations)

    Use efficient binary serialization (MessagePack, Protobuf).
  base_severity: 7

  patterns:
    java:
      - pattern: 'ObjectOutputStream|SerializationTranscoder'
        context: 'Java object serialization (check efficiency)'

    python:
      - pattern: 'pickle\\.dumps|json\\.dumps'
        context: 'Python serialization (check efficiency)'

  fixes:
    description: |
      Use efficient serialization:

      // Java - Bad (Java Serialization - slow and bloated)
      ByteArrayOutputStream baos = new ByteArrayOutputStream();
      ObjectOutputStream oos = new ObjectOutputStream(baos);
      oos.writeObject(user);
      byte[] serialized = baos.toByteArray();
      client.set("user:123", 3600, serialized);
      // Slow + large size (includes class metadata)

      // Java - Good (JSON with Jackson - faster)
      ObjectMapper mapper = new ObjectMapper();
      String json = mapper.writeValueAsString(user);
      client.set("user:123", 3600, json);

      // Java - Better (MessagePack - binary, fast, compact)
      MessagePack msgpack = new MessagePack();
      byte[] bytes = msgpack.write(user);
      client.set("user:123", 3600, bytes);
      // 2-5x smaller than JSON, faster serialization

      # Python - Bad (pickle - slow, security risk)
      import pickle
      serialized = pickle.dumps(user_data)
      memcache_client.set('user:123', serialized)
      # Pickle is slow and has security vulnerabilities

      # Python - Good (JSON - standard)
      import json
      serialized = json.dumps(user_data)
      memcache_client.set('user:123', serialized)

      # Python - Better (MessagePack - binary, fast)
      import msgpack
      serialized = msgpack.packb(user_data)
      memcache_client.set('user:123', serialized)
      # 2-3x smaller than JSON, 2-5x faster

      # Python - Best (Protocol Buffers - schema-based, very efficient)
      import user_pb2  # Generated from .proto file

      user = user_pb2.User()
      user.id = 123
      user.name = "Alice"
      serialized = user.SerializeToString()
      memcache_client.set('user:123', serialized)

      // PHP - Bad (serialize() - slow)
      $serialized = serialize($userData);
      $memcache->set('user:123', $serialized, 3600);

      // PHP - Good (JSON)
      $json = json_encode($userData);
      $memcache->set('user:123', $json, 3600);

      // PHP - Better (MessagePack)
      $packed = msgpack_pack($userData);
      $memcache->set('user:123', $packed, 3600);

      // JavaScript/Node.js - JSON (built-in)
      const json = JSON.stringify(userData);
      memcached.set('user:123', json, 3600);

      // JavaScript/Node.js - MessagePack
      const msgpack = require('msgpack');
      const packed = msgpack.pack(userData);
      memcached.set('user:123', packed, 3600);

      Serialization format comparison:
      - Java Serialization: SLOW, LARGE (avoid)
      - Pickle: SLOW, INSECURE (avoid)
      - JSON: STANDARD, readable (good for simple data)
      - MessagePack: FAST, COMPACT (2-5x smaller than JSON)
      - Protocol Buffers: FASTEST, SMALLEST (requires schema)

      Best practices:
      - Use MessagePack for most use cases (balance of speed/size)
      - Use Protocol Buffers for high-performance needs
      - Avoid Java Serialization and Pickle
      - Benchmark serialization overhead

    references:
      - 'MessagePack: https://msgpack.org/'
      - 'Protocol Buffers: https://developers.google.com/protocol-buffers'
      - 'Serialization benchmarks: https://github.com/eishay/jvm-serializers/wiki'

---

rule:
  id: MEMCACHED_SLAB_ALLOCATION_ISSUE
  category: database
  name: Memcached Slab Allocation Problem
  description: |
    Memcached uses slab allocation which can waste memory.
    Items are allocated in fixed-size chunks (slabs).

    Problems:
    - Memory waste (internal fragmentation)
    - Inefficient memory usage
    - Eviction of items in wrong slab class
    - Poor cache hit rate
    - Wasted capacity

    Understand slab classes and tune growth factor.
  base_severity: 7

  patterns:
    sql:
      - pattern: 'memcached.*-f\\s+[23]\\.'
        context: 'Memcached growth factor configuration'

  fixes:
    description: |
      Understand and tune slab allocation:

      Memcached slab classes (default growth factor 1.25):
      - Slab 1: 96 bytes
      - Slab 2: 120 bytes (96 * 1.25)
      - Slab 3: 152 bytes (120 * 1.25)
      - ...
      - Slab 42: 1MB (max)

      Problem example:
      - You store 200-byte items
      - Memcached allocates from Slab 4 (240 bytes)
      - Waste: 40 bytes per item (17% waste!)

      -- Check current slab allocation
      stats slabs

      -- Check item distribution
      stats items

      -- Example output:
      STAT 1:chunk_size 96
      STAT 1:chunks_per_page 10922
      STAT 1:total_pages 1
      STAT 1:total_chunks 10922
      STAT 1:used_chunks 8500
      STAT 1:free_chunks 2422

      // Bad - Default growth factor (1.25) may waste memory
      memcached -m 512 -p 11211  # Default growth factor

      // Good - Tune growth factor based on your data
      // If your items are mostly 100-500 bytes:
      memcached -m 512 -p 11211 -f 1.15  # Smaller growth factor (more slab classes)

      // If your items vary widely (10 bytes to 1MB):
      memcached -m 512 -p 11211 -f 2.0  # Larger growth factor (fewer slab classes)

      // Better - Enable slab automove (Memcached 1.4.11+)
      memcached -m 512 -p 11211 -o slab_automove=1
      // Automatically moves memory between slab classes based on eviction rates

      // Best - Monitor and adjust
      // 1. Analyze your data sizes
      // 2. Choose appropriate growth factor
      // 3. Enable slab automove
      // 4. Monitor stats slabs regularly

      # Python script to analyze item sizes
      import memcache
      client = memcache.Client(['127.0.0.1:11211'])

      stats = client.get_stats('slabs')
      for server, slab_stats in stats:
          print(f"Server: {server}")
          for key, value in slab_stats.items():
              if 'chunk_size' in key or 'used_chunks' in key:
                  print(f"  {key}: {value}")

      Best practices:
      - Monitor slab allocation: stats slabs
      - Enable slab automove (Memcached 1.4.11+)
      - Tune growth factor based on your data
      - Keep item sizes consistent if possible
      - Consider using multiple Memcached instances for different item sizes

      Growth factor guidelines:
      - f=1.15: Tight packing, more slab classes (consistent item sizes)
      - f=1.25: Default, good balance
      - f=2.0: Wide range, fewer slab classes (variable item sizes)

    references:
      - 'Slab allocation: https://github.com/memcached/memcached/wiki/UserInternals'
      - 'Slab automove: https://github.com/memcached/memcached/wiki/ReleaseNotes1411'
      - 'Memory efficiency: https://blog.engineyard.com/2015/memcached-internals-memory-allocation'
