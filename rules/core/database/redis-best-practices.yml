# Redis Best Practices and Performance Rules
# Auto-generated by BugSearch2 - Database-Specific Rules
# Generated: 2025-10-14
# Category: database
# Target: Redis In-Memory Data Store

rule:
  id: REDIS_KEYS_COMMAND
  category: database
  name: Redis KEYS Command in Production
  description: |
    KEYS * is EXTREMELY DANGEROUS in production Redis.
    It BLOCKS the entire Redis server while scanning ALL keys.

    Problems:
    - Blocks ALL clients (Redis is single-threaded)
    - Scans entire keyspace (O(N) complexity)
    - Causes timeout for ALL operations
    - Can crash server with millions of keys

    NEVER use KEYS * in production. Use SCAN instead.
  base_severity: 10

  patterns:
    java:
      - pattern: '\.keys\(["\']?\*["\']?\)'
        context: 'Java Jedis/Lettuce KEYS * command'
      - pattern: 'KEYS\s+\*'
        context: 'Java Redis KEYS * command string'

    python:
      - pattern: 'keys\(["\']?\*["\']?\)'
        context: 'Python redis-py KEYS * command'
      - pattern: 'r\.keys\(\)'
        context: 'Python redis-py keys() without pattern'

    javascript:
      - pattern: 'keys\(["\']?\*["\']?\)'
        context: 'Node.js ioredis/node-redis KEYS * command'
      - pattern: '\.KEYS\(["\']?\*'
        context: 'Node.js Redis KEYS command'

    csharp:
      - pattern: 'Keys\(["\']?\*["\']?\)'
        context: 'C# StackExchange.Redis KEYS * command'
      - pattern: 'Server\.Keys\('
        context: 'C# Redis Server.Keys()'

    go:
      - pattern: 'Keys\([^)]*["\']?\*["\']?\)'
        context: 'Go redis client KEYS * command'

    php:
      - pattern: '->keys\(["\']?\*["\']?\)'
        context: 'PHP Predis/PhpRedis KEYS * command'

  fixes:
    description: |
      Use SCAN for iterative keyspace traversal:

      // Bad - KEYS * (blocks Redis server)
      Set<String> keys = jedis.keys("*");  // NEVER DO THIS!

      // Good - SCAN (non-blocking iteration)
      ScanParams params = new ScanParams().match("user:*").count(100);
      String cursor = "0";
      do {
          ScanResult<String> result = jedis.scan(cursor, params);
          List<String> keys = result.getResult();
          // Process keys...
          cursor = result.getCursor();
      } while (!cursor.equals("0"));

      Python example:
      for key in redis_client.scan_iter(match="user:*", count=100):
          # Process key
          pass

      SCAN advantages:
      - Non-blocking (returns in chunks)
      - O(1) per iteration
      - Safe for production
      - Customizable batch size

    references:
      - 'SCAN command: https://redis.io/commands/scan/'
      - 'Why KEYS is dangerous: https://redis.io/commands/keys/'

---

rule:
  id: REDIS_BIG_KEYS
  category: database
  name: Redis Big Keys Detection
  description: |
    Large keys (>1MB) cause severe performance issues in Redis.
    Redis allocates memory in blocks and big keys fragment memory.

    Problems:
    - Memory fragmentation
    - Slow serialization/deserialization
    - Network bandwidth saturation
    - Blocking during DEL/EXPIRE
    - Replication lag

    Recommended max size: 100KB per key
    Critical threshold: 1MB per key
  base_severity: 9

  patterns:
    java:
      - pattern: 'set\([^,]+,\s*[^)]{1000,}\)'
        context: 'Java Redis SET with large value'
      - pattern: 'lpush\([^,]+,.*\)'
        context: 'Java Redis LPUSH (check list size)'
      - pattern: 'hset\([^,]+,.*\)'
        context: 'Java Redis HSET (check hash size)'

    python:
      - pattern: 'set\([^,]+,\s*["\'][^"\']{1000,}'
        context: 'Python redis-py SET with large value'
      - pattern: 'lpush\(|rpush\(|sadd\(|zadd\('
        context: 'Python Redis collection operations'

    javascript:
      - pattern: 'set\([^,]+,\s*[^)]{1000,}\)'
        context: 'Node.js Redis SET with large value'

    csharp:
      - pattern: 'StringSet\([^,]+,\s*[^)]{1000,}\)'
        context: 'C# StackExchange.Redis StringSet with large value'

  fixes:
    description: |
      Split large values into smaller chunks:

      // Bad - 10MB string in single key
      redis.set("large:data", tenMBString);

      // Good - Split into 100KB chunks
      int chunkSize = 100 * 1024;  // 100KB
      for (int i = 0; i < data.length(); i += chunkSize) {
          String chunk = data.substring(i, Math.min(i + chunkSize, data.length()));
          redis.set("large:data:" + i, chunk);
      }
      redis.set("large:data:chunks", data.length() / chunkSize);

      // Better - Use Redis Streams for large datasets
      redis.xadd("mystream", "*", "field1", "value1", "field2", "value2");

      // Best - Store large data in object storage (S3)
      String s3Url = s3.upload(largeData);
      redis.set("data:url", s3Url);  // Store reference only

      Monitor big keys:
      redis-cli --bigkeys
      redis-cli --memkeys --memkeys-samples 10000

      Key size limits:
      - String: 512MB (theoretical), 100KB (recommended)
      - List: 2^32-1 elements, 100K elements (recommended)
      - Hash: 2^32-1 fields, 1000 fields (recommended)

    references:
      - 'Memory optimization: https://redis.io/topics/memory-optimization'
      - 'Big keys problem: https://redis.io/topics/latency'

---

rule:
  id: REDIS_NO_PIPELINE
  category: database
  name: Redis Multiple Commands Without Pipeline
  description: |
    Executing commands sequentially causes round-trip overhead.
    Each command requires network round-trip (RTT).

    Problems:
    - N commands = N round-trips (100ms latency × 1000 = 100s!)
    - Wasted bandwidth
    - Reduced throughput
    - High latency for bulk operations

    Use pipeline to batch commands (10-100x speedup).
  base_severity: 8

  patterns:
    java:
      - pattern: 'for\s*\([^)]+\)\s*\{[^}]*jedis\.set\('
        context: 'Java Redis SET in loop without pipeline'
      - pattern: 'for\s*\([^)]+\)\s*\{[^}]*jedis\.get\('
        context: 'Java Redis GET in loop without pipeline'
      - pattern: 'while\s*\([^)]+\)\s*\{[^}]*jedis\.'
        context: 'Java Redis command in while loop'

    python:
      - pattern: 'for\s+\w+\s+in\s+.*:\s*\n\s*r\.set\('
        context: 'Python Redis SET in for loop'
      - pattern: 'for\s+\w+\s+in\s+.*:\s*\n\s*r\.get\('
        context: 'Python Redis GET in for loop'

    javascript:
      - pattern: 'for\s*\([^)]+\)\s*\{[^}]*redis\.set\('
        context: 'Node.js Redis SET in for loop'

    csharp:
      - pattern: 'for\s*\([^)]+\)\s*\{[^}]*db\.StringSet\('
        context: 'C# Redis StringSet in for loop'

  fixes:
    description: |
      Use pipeline for bulk operations:

      // Bad - 1000 round-trips
      for (int i = 0; i < 1000; i++) {
          jedis.set("key" + i, "value" + i);  // Each call = 1 RTT
      }

      // Good - 1 round-trip with pipeline
      Pipeline pipeline = jedis.pipelined();
      for (int i = 0; i < 1000; i++) {
          pipeline.set("key" + i, "value" + i);
      }
      pipeline.sync();  // Execute all commands

      Python example:
      pipe = redis_client.pipeline()
      for i in range(1000):
          pipe.set(f"key{i}", f"value{i}")
      pipe.execute()

      Node.js example:
      const pipeline = redis.pipeline();
      for (let i = 0; i < 1000; i++) {
          pipeline.set(`key${i}`, `value${i}`);
      }
      await pipeline.exec();

      Performance improvement:
      - Sequential: 1000 × 1ms RTT = 1000ms
      - Pipeline: 1 × 1ms RTT = 1ms
      - Speedup: 1000x!

    references:
      - 'Pipelining: https://redis.io/topics/pipelining'
      - 'Benchmark: https://redis.io/topics/benchmarks'

---

rule:
  id: REDIS_TRANSACTION_MISUSE
  category: database
  name: Redis Transaction Misuse
  description: |
    Redis MULTI/EXEC is NOT like SQL transactions.
    It does NOT provide isolation or rollback.

    Redis MULTI/EXEC only provides:
    - Command batching (like pipeline)
    - Atomicity (all or nothing execution)

    Redis transactions do NOT provide:
    - Isolation (other clients can modify keys)
    - Rollback (errors don't abort transaction)
    - ACID guarantees

    Use Lua scripts for atomic read-modify-write operations.
  base_severity: 7

  patterns:
    java:
      - pattern: 'multi\(\)'
        context: 'Java Redis MULTI command'
      - pattern: 'Transaction\s+\w+\s*='
        context: 'Java Redis Transaction object'

    python:
      - pattern: '\.multi\(\)'
        context: 'Python redis-py MULTI command'
      - pattern: 'pipeline\(transaction=True\)'
        context: 'Python redis-py transaction pipeline'

    javascript:
      - pattern: '\.multi\(\)'
        context: 'Node.js Redis MULTI command'

  fixes:
    description: |
      Use Lua scripts for atomic operations:

      // Bad - Race condition with MULTI/EXEC
      Transaction tx = jedis.multi();
      String value = jedis.get("counter");  // Read
      int newValue = Integer.parseInt(value) + 1;
      tx.set("counter", String.valueOf(newValue));  // Write
      tx.exec();
      // Problem: Another client can modify "counter" between GET and SET!

      // Good - Lua script (atomic)
      String script =
          "local val = redis.call('GET', KEYS[1]) " +
          "if val then " +
          "  return redis.call('SET', KEYS[1], val + 1) " +
          "else " +
          "  return redis.call('SET', KEYS[1], 1) " +
          "end";
      jedis.eval(script, 1, "counter");

      // Better - Use Redis built-in atomic commands
      jedis.incr("counter");  // Atomic increment

      Lua script advantages:
      - Truly atomic (no race conditions)
      - Server-side execution (1 round-trip)
      - Access to all Redis commands
      - Can return complex values

      MULTI/EXEC use cases:
      - Batching unrelated commands (use pipeline instead)
      - Ensuring commands execute together (use Lua script instead)

    references:
      - 'Transactions: https://redis.io/topics/transactions'
      - 'Lua scripting: https://redis.io/commands/eval/'

---

rule:
  id: REDIS_WRONG_DATA_STRUCTURE
  category: database
  name: Redis Inappropriate Data Structure
  description: |
    Using wrong data structure causes memory waste and poor performance.

    Common mistakes:
    - Using String for collections (JSON serialization)
    - Using List for unique items (should use Set)
    - Using Hash for simple key-value (overhead)
    - Using Sorted Set for simple sorting (should use List)

    Choose data structure based on access patterns.
  base_severity: 7

  patterns:
    java:
      - pattern: 'set\([^,]+,\s*["\']?\{.*\}["\']?\)'
        context: 'Java Redis SET with JSON string'
      - pattern: 'set\([^,]+,\s*gson\.toJson\('
        context: 'Java Redis SET with JSON serialization'
      - pattern: 'set\([^,]+,\s*new\s+ObjectMapper\('
        context: 'Java Redis SET with Jackson serialization'

    python:
      - pattern: 'set\([^,]+,\s*json\.dumps\('
        context: 'Python Redis SET with JSON dumps'
      - pattern: 'set\([^,]+,\s*pickle\.dumps\('
        context: 'Python Redis SET with pickle'

    javascript:
      - pattern: 'set\([^,]+,\s*JSON\.stringify\('
        context: 'Node.js Redis SET with JSON.stringify'

  fixes:
    description: |
      Use native Redis data structures:

      // Bad - JSON string for user object
      redis.set("user:1", '{"name":"Alice","age":30,"email":"alice@example.com"}');

      // Good - Redis Hash
      redis.hset("user:1", "name", "Alice");
      redis.hset("user:1", "age", "30");
      redis.hset("user:1", "email", "alice@example.com");

      // Or use HMSET for multiple fields
      redis.hmset("user:1", Map.of(
          "name", "Alice",
          "age", "30",
          "email", "alice@example.com"
      ));

      Data structure selection guide:

      1. String (SET/GET):
         - Simple values, counters, bitmaps
         - Max 512MB

      2. Hash (HSET/HGET):
         - Objects with multiple fields
         - Memory efficient for <100 fields
         - Use for user profiles, settings

      3. List (LPUSH/RPUSH/LRANGE):
         - Ordered collections, queues, stacks
         - Recent items (LTRIM for max size)
         - Use for activity feeds, logs

      4. Set (SADD/SMEMBERS):
         - Unique items, tags
         - Set operations (union, intersection)
         - Use for tagging, user following

      5. Sorted Set (ZADD/ZRANGE):
         - Ordered unique items with scores
         - Leaderboards, time-series
         - Use for rankings, priority queues

      Memory comparison (1000 items):
      - JSON String: ~50KB
      - Hash: ~10KB (5x smaller)

    references:
      - 'Data types: https://redis.io/topics/data-types'
      - 'Data structures intro: https://redis.io/topics/data-types-intro'

---

rule:
  id: REDIS_NO_EXPIRATION
  category: database
  name: Redis Keys Without Expiration
  description: |
    Keys without TTL cause unbounded memory growth.
    Redis will eventually hit maxmemory limit.

    Problems:
    - Memory exhaustion
    - Eviction of important keys (if maxmemory-policy=allkeys-lru)
    - Server OOM crash (if maxmemory-policy=noeviction)

    Always set TTL for cache data.
  base_severity: 8

  patterns:
    java:
      - pattern: 'set\([^,]+,\s*[^)]+\)(?!.*expire|.*setex)'
        context: 'Java Redis SET without expiration'
      - pattern: '\.set\(.*\);\s*(?!.*\.expire)'
        context: 'Java Redis SET followed by no expire'

    python:
      - pattern: 'set\([^,]+,\s*[^,)]+(?!.*ex=|.*px=)\)'
        context: 'Python redis-py SET without ex/px parameter'

    javascript:
      - pattern: 'set\([^,]+,\s*[^)]+\)(?!.*EX|.*PX)'
        context: 'Node.js Redis SET without EX/PX'

    csharp:
      - pattern: 'StringSet\([^,]+,\s*[^,)]+\)(?!.*TimeSpan)'
        context: 'C# Redis StringSet without expiry'

  fixes:
    description: |
      Always set TTL for cache data:

      // Bad - No expiration (unbounded memory growth)
      redis.set("session:123", sessionData);

      // Good - SET with EX (seconds)
      redis.setex("session:123", 3600, sessionData);  // 1 hour

      // Or use SET with EX option
      redis.set("session:123", sessionData, "EX", 3600);

      // Good - SET with PX (milliseconds)
      redis.psetex("session:123", 3600000, sessionData);

      Python example:
      redis_client.set("session:123", session_data, ex=3600)

      Node.js example:
      await redis.set("session:123", sessionData, "EX", 3600);

      C# example:
      db.StringSet("session:123", sessionData, TimeSpan.FromHours(1));

      TTL recommendations:
      - Session data: 30-60 minutes
      - API cache: 5-15 minutes
      - User cache: 1-24 hours
      - Permanent data: No TTL (use database)

      Monitor keys without TTL:
      redis-cli --scan --pattern '*' | xargs -L1 redis-cli TTL | grep -c '^-1$'

      Set default maxmemory policy:
      maxmemory-policy allkeys-lru  # Evict any key (LRU)
      maxmemory 2gb  # Set memory limit

    references:
      - 'Key expiration: https://redis.io/commands/expire/'
      - 'Eviction policies: https://redis.io/topics/lru-cache'

---

rule:
  id: REDIS_PUBSUB_FIRE_FORGET
  category: database
  name: Redis Pub/Sub Reliability Issues
  description: |
    Redis Pub/Sub is fire-and-forget with NO persistence or reliability.

    Problems:
    - Messages lost if no subscribers
    - Messages lost if subscriber disconnects
    - No message queue (immediate delivery only)
    - No acknowledgment

    Use Redis Streams for reliable messaging.
  base_severity: 7

  patterns:
    java:
      - pattern: 'publish\([^,]+,\s*[^)]+\)'
        context: 'Java Redis PUBLISH command'
      - pattern: 'subscribe\([^)]+\)'
        context: 'Java Redis SUBSCRIBE command'

    python:
      - pattern: '\.publish\([^,]+,\s*[^)]+\)'
        context: 'Python redis-py PUBLISH'
      - pattern: '\.subscribe\([^)]+\)'
        context: 'Python redis-py SUBSCRIBE'

    javascript:
      - pattern: '\.publish\([^,]+,\s*[^)]+\)'
        context: 'Node.js Redis PUBLISH'

  fixes:
    description: |
      Use Redis Streams for reliable messaging:

      // Bad - Pub/Sub (message lost if no subscriber)
      redis.publish("events", "important_event");

      // Good - Redis Streams (persistent, consumer groups)
      redis.xadd("events", "*", "type", "important_event", "data", eventData);

      // Consumer with consumer group (exactly-once delivery)
      redis.xreadgroup(
          "mygroup", "consumer1",
          "BLOCK", "0",
          "STREAMS", "events", ">"
      );

      // Acknowledge message
      redis.xack("events", "mygroup", messageId);

      Python example:
      # Producer
      redis_client.xadd("events", {"type": "order", "data": order_data})

      # Consumer
      messages = redis_client.xreadgroup(
          "mygroup", "consumer1",
          {"events": ">"},
          count=10, block=1000
      )
      for stream, message_list in messages:
          for message_id, data in message_list:
              # Process message
              redis_client.xack("events", "mygroup", message_id)

      Redis Streams advantages:
      - Persistent (survives restarts)
      - Consumer groups (load balancing)
      - Message acknowledgment
      - Pending entries tracking
      - Exactly-once delivery

      Pub/Sub use cases:
      - Real-time notifications (OK to lose)
      - Cache invalidation
      - Live updates (stocks, sports)

    references:
      - 'Pub/Sub limitations: https://redis.io/topics/pubsub'
      - 'Redis Streams: https://redis.io/topics/streams-intro'

---

rule:
  id: REDIS_LUA_SCRIPT_BLOCKING
  category: database
  name: Redis Blocking Lua Script
  description: |
    Long-running Lua scripts BLOCK entire Redis server.
    Redis is single-threaded; scripts run atomically.

    Problems:
    - Blocks ALL clients
    - Timeout for all operations
    - Server appears frozen
    - No other commands can execute

    Keep Lua scripts short (<5ms execution time).
  base_severity: 9

  patterns:
    java:
      - pattern: 'eval\(["\'][^"\']{1000,}'
        context: 'Java Redis EVAL with long script'
      - pattern: 'for\s+\w+\s*=.*,\s*\d{4,}'
        context: 'Lua script with large loop'

    python:
      - pattern: 'eval\(["\'][^"\']{1000,}'
        context: 'Python Redis EVAL with long script'

    javascript:
      - pattern: 'eval\(["\'][^"\']{1000,}'
        context: 'Node.js Redis EVAL with long script'

  fixes:
    description: |
      Keep Lua scripts short and simple:

      // Bad - Long-running Lua script
      String script =
          "for i=1,1000000 do " +  // Huge loop!
          "  redis.call('SET', 'key' .. i, i) " +
          "end";
      redis.eval(script);

      // Good - Short atomic operation
      String script =
          "local current = redis.call('GET', KEYS[1]) " +
          "if current then " +
          "  return redis.call('INCR', KEYS[1]) " +
          "else " +
          "  return redis.call('SET', KEYS[1], 1) " +
          "end";
      redis.eval(script, 1, "counter");

      // Better - Use pipelining for bulk operations
      Pipeline pipeline = jedis.pipelined();
      for (int i = 1; i <= 1000000; i++) {
          pipeline.set("key" + i, String.valueOf(i));
      }
      pipeline.sync();

      Lua script best practices:
      - Keep execution time < 5ms
      - Avoid loops > 1000 iterations
      - Use redis.call() not redis.pcall()
      - Test with redis-cli --eval --ldb
      - Monitor with SLOWLOG

      Monitor long scripts:
      redis-cli SLOWLOG GET 10

      Script timeout (milliseconds):
      lua-time-limit 5000  # Kill scripts > 5 seconds

    references:
      - 'Lua scripting: https://redis.io/commands/eval/'
      - 'Script debugging: https://redis.io/topics/ldb'
