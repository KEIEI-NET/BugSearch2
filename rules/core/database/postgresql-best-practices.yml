# PostgreSQL Best Practices and Performance Issues
# Auto-generated by BugSearch2 - Database-Specific Rules
# Generated: 2025-10-14
# Category: database
# Target: PostgreSQL Advanced RDBMS

rule:
  id: POSTGRESQL_NO_VACUUM
  category: database
  name: PostgreSQL Missing VACUUM Configuration
  description: |
    PostgreSQL's MVCC creates dead tuples that must be cleaned by VACUUM.
    Without proper VACUUM, tables bloat and performance degrades.

    Problems:
    - Table bloat (dead tuples accumulate)
    - Index bloat (unused index entries)
    - Transaction ID wraparound (database shutdown)
    - Query performance degradation
    - Disk space waste

    Monitor autovacuum and configure thresholds properly.
  base_severity: 9

  patterns:
    sql:
      - pattern: 'autovacuum\s*=\s*off'
        context: 'PostgreSQL autovacuum disabled'
      - pattern: 'ALTER\s+TABLE.*SET\s+\(autovacuum_enabled\s*=\s*false\)'
        context: 'Table-level autovacuum disabled'

    python:
      - pattern: 'VACUUM'
        context: 'Manual VACUUM (should be automated)'

  fixes:
    description: |
      Enable and tune autovacuum:

      -- Bad - Autovacuum disabled
      ALTER TABLE my_table SET (autovacuum_enabled = false);

      -- Good - Enable autovacuum with proper thresholds
      ALTER TABLE my_table SET (
        autovacuum_enabled = true,
        autovacuum_vacuum_scale_factor = 0.1,  -- Vacuum when 10% dead tuples
        autovacuum_analyze_scale_factor = 0.05 -- Analyze when 5% changes
      );

      -- Better - Aggressive autovacuum for high-churn tables
      ALTER TABLE high_update_table SET (
        autovacuum_vacuum_scale_factor = 0.01,  -- Vacuum when 1% dead tuples
        autovacuum_vacuum_cost_delay = 0        -- No throttling
      );

      Monitor bloat:
      SELECT schemaname, tablename,
             pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size
      FROM pg_tables
      ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;

    references:
      - 'Autovacuum tuning: https://www.postgresql.org/docs/current/routine-vacuuming.html'
      - 'Bloat monitoring: https://wiki.postgresql.org/wiki/Show_database_bloat'

---

rule:
  id: POSTGRESQL_NO_CONNECTION_POOL
  category: database
  name: PostgreSQL Connection Without Pooling
  description: |
    PostgreSQL has expensive connection overhead (fork process per connection).
    Without connection pooling, applications waste resources and hit connection limits.

    Problems:
    - Connection overhead: 50-100ms per connection
    - Process fork overhead on server
    - max_connections limit (default: 100)
    - Memory waste (each connection uses 10MB+)
    - Startup delays

    Use PgBouncer or application-level pooling.
  base_severity: 9

  patterns:
    java:
      - pattern: 'DriverManager\.getConnection'
        context: 'Java JDBC without connection pool'
      - pattern: 'new\s+org\.postgresql\.Driver'
        context: 'Java PostgreSQL driver without pooling'

    python:
      - pattern: 'psycopg2\.connect\('
        context: 'Python psycopg2 direct connection (no pool)'
      - pattern: 'create_engine\([^)]*poolclass=NullPool'
        context: 'SQLAlchemy with NullPool (no pooling)'

    javascript:
      - pattern: 'new\s+Client\('
        context: 'Node.js pg.Client (no pooling)'

    csharp:
      - pattern: 'new\s+NpgsqlConnection\('
        context: 'C# Npgsql without connection pooling'

    go:
      - pattern: 'sql\.Open\(['"]postgres'
        context: 'Go database/sql (check max_open_conns setting)'

    php:
      - pattern: 'new\s+PDO\([\'"]pgsql:'
        context: 'PHP PDO without persistent connections'

  fixes:
    description: |
      Use connection pooling:

      // Bad - New connection per request
      Connection conn = DriverManager.getConnection(
          "jdbc:postgresql://localhost/mydb", "user", "pass"
      );
      // ... use connection
      conn.close();  // Expensive!

      // Good - HikariCP connection pool (Java)
      HikariConfig config = new HikariConfig();
      config.setJdbcUrl("jdbc:postgresql://localhost/mydb");
      config.setMaximumPoolSize(20);
      config.setMinimumIdle(5);
      HikariDataSource ds = new HikariDataSource(config);
      Connection conn = ds.getConnection();

      # Python - SQLAlchemy with pooling
      from sqlalchemy import create_engine
      engine = create_engine(
          'postgresql://user:pass@localhost/mydb',
          pool_size=20,
          max_overflow=10,
          pool_pre_ping=True  # Verify connections
      )

      # Node.js - pg.Pool (recommended)
      const { Pool } = require('pg');
      const pool = new Pool({
        max: 20,
        min: 5,
        connectionTimeoutMillis: 5000
      });

      // Or use PgBouncer (external pooler)
      // postgresql.conf:
      max_connections = 200
      // PgBouncer config:
      [databases]
      mydb = host=localhost port=5432 dbname=mydb
      [pgbouncer]
      pool_mode = transaction
      max_client_conn = 1000
      default_pool_size = 20

    references:
      - 'Connection pooling: https://wiki.postgresql.org/wiki/Number_Of_Database_Connections'
      - 'PgBouncer: https://www.pgbouncer.org/'

---

rule:
  id: POSTGRESQL_LONG_TRANSACTION
  category: database
  name: PostgreSQL Long-Running Transaction
  description: |
    Long-running transactions prevent VACUUM from cleaning dead tuples.
    This causes table bloat and performance degradation.

    Problems:
    - VACUUM cannot reclaim dead tuples (bloat accumulates)
    - Holds back transaction ID horizon
    - Blocks DDL operations
    - Causes replication lag
    - Risk of transaction ID wraparound

    Keep transactions short and commit frequently.
  base_severity: 8

  patterns:
    java:
      - pattern: 'setAutoCommit\(false\).*sleep\(|setAutoCommit\(false\).*Thread\.sleep'
        context: 'Java JDBC long transaction with sleep'

    python:
      - pattern: 'begin\(\).*time\.sleep|BEGIN.*time\.sleep'
        context: 'Python long transaction with sleep'
      - pattern: 'with\s+engine\.begin\(\):.*time\.sleep'
        context: 'SQLAlchemy long transaction'

    sql:
      - pattern: 'BEGIN.*pg_sleep|BEGIN\s+TRANSACTION.*pg_sleep'
        context: 'SQL transaction with pg_sleep'

  fixes:
    description: |
      Keep transactions short:

      // Bad - Long transaction holding locks
      conn.setAutoCommit(false);
      Statement stmt = conn.createStatement();
      stmt.executeUpdate("UPDATE users SET status = 'processing'");

      // ... complex processing that takes 5 minutes
      Thread.sleep(300000);

      conn.commit();  // Holds locks for 5 minutes!

      // Good - Short transaction with intermediate commits
      conn.setAutoCommit(false);
      Statement stmt = conn.createStatement();
      stmt.executeUpdate("UPDATE users SET status = 'processing'");
      conn.commit();  // Release locks immediately

      // ... complex processing (no transaction)

      conn.setAutoCommit(false);
      stmt.executeUpdate("UPDATE users SET status = 'completed'");
      conn.commit();

      // Better - Use advisory locks for long operations
      SELECT pg_advisory_lock(123);
      -- Complex processing without holding transaction
      SELECT pg_advisory_unlock(123);

      Monitor long transactions:
      SELECT pid, now() - xact_start as duration, state, query
      FROM pg_stat_activity
      WHERE xact_start IS NOT NULL
        AND now() - xact_start > interval '5 minutes'
      ORDER BY xact_start;

    references:
      - 'Transaction management: https://www.postgresql.org/docs/current/mvcc-intro.html'
      - 'Advisory locks: https://www.postgresql.org/docs/current/explicit-locking.html#ADVISORY-LOCKS'

---

rule:
  id: POSTGRESQL_MISSING_INDEX
  category: database
  name: PostgreSQL Query Without Index
  description: |
    Queries without proper indexes cause full table scans.
    PostgreSQL has multiple index types for different use cases.

    Problems:
    - Sequential scan (O(N) instead of O(log N))
    - High I/O and CPU usage
    - Query timeout on large tables
    - Locks held longer

    Choose appropriate index type: B-tree, Hash, GIN, GiST, BRIN.
  base_severity: 9

  patterns:
    sql:
      - pattern: 'SELECT.*WHERE.*(?!.*INDEX)'
        context: 'SQL SELECT with WHERE but no index hint'
      - pattern: 'CREATE\s+TABLE.*(?!.*INDEX|.*PRIMARY\s+KEY)'
        context: 'Table without any indexes'

  fixes:
    description: |
      Use appropriate index types:

      -- Bad - No index on frequently queried column
      CREATE TABLE users (
        id SERIAL PRIMARY KEY,
        email TEXT,
        created_at TIMESTAMP
      );
      SELECT * FROM users WHERE email = 'user@example.com';  -- Seq Scan!

      -- Good - B-tree index for equality/range queries
      CREATE INDEX idx_users_email ON users (email);
      SELECT * FROM users WHERE email = 'user@example.com';  -- Index Scan

      -- Better - Partial index for specific queries
      CREATE INDEX idx_active_users_email ON users (email)
      WHERE deleted_at IS NULL;

      -- GIN index for full-text search
      CREATE INDEX idx_users_search ON users USING GIN (to_tsvector('english', name || ' ' || bio));

      -- GiST index for geometric data
      CREATE INDEX idx_locations ON places USING GiST (location);

      -- BRIN index for time-series data (very large tables)
      CREATE INDEX idx_events_time ON events USING BRIN (created_at);

      Check missing indexes:
      SELECT schemaname, tablename, attname, n_distinct, correlation
      FROM pg_stats
      WHERE schemaname = 'public'
        AND n_distinct > 100
        AND correlation < 0.1;

      Analyze query plans:
      EXPLAIN (ANALYZE, BUFFERS) SELECT * FROM users WHERE email = 'user@example.com';

    references:
      - 'Index types: https://www.postgresql.org/docs/current/indexes-types.html'
      - 'Index tuning: https://wiki.postgresql.org/wiki/Index_Maintenance'

---

rule:
  id: POSTGRESQL_JSONB_WITHOUT_INDEX
  category: database
  name: PostgreSQL JSONB Query Without GIN Index
  description: |
    JSONB queries without GIN indexes cause full table scans.
    GIN indexes provide efficient JSONB search.

    Problems:
    - Sequential scan of entire table
    - JSON parsing overhead for every row
    - Query timeout on large tables
    - High CPU usage

    Create GIN index on JSONB columns with frequent queries.
  base_severity: 8

  patterns:
    sql:
      - pattern: 'SELECT.*->>|SELECT.*->|SELECT.*@>'
        context: 'SQL JSONB query operators'
      - pattern: 'WHERE.*jsonb_path_query'
        context: 'JSONB path query'

    java:
      - pattern: 'jsonb->>|jsonb->|jsonb @>'
        context: 'Java JDBC JSONB query'

    python:
      - pattern: '\.filter\(.*\[[\'"]\w+[\'"]'
        context: 'SQLAlchemy JSONB filter'

  fixes:
    description: |
      Use GIN indexes for JSONB:

      -- Bad - No index on JSONB column
      CREATE TABLE products (
        id SERIAL PRIMARY KEY,
        attributes JSONB
      );
      SELECT * FROM products WHERE attributes->>'category' = 'electronics';  -- Seq Scan!

      -- Good - GIN index on entire JSONB column
      CREATE INDEX idx_products_attributes ON products USING GIN (attributes);
      SELECT * FROM products WHERE attributes @> '{"category": "electronics"}';  -- Index Scan

      -- Better - GIN index on specific JSONB path
      CREATE INDEX idx_products_category ON products USING GIN ((attributes->'category'));

      -- Best - Expression index for frequent queries
      CREATE INDEX idx_products_category_text ON products ((attributes->>'category'));
      SELECT * FROM products WHERE attributes->>'category' = 'electronics';

      JSONB operators:
      - @> : contains (indexed)
      - -> : get JSON object (not indexed)
      - ->> : get text (not indexed, use expression index)

      Check JSONB query plans:
      EXPLAIN (ANALYZE, BUFFERS)
      SELECT * FROM products
      WHERE attributes @> '{"category": "electronics"}';

    references:
      - 'JSONB indexing: https://www.postgresql.org/docs/current/datatype-json.html#JSON-INDEXING'
      - 'GIN indexes: https://www.postgresql.org/docs/current/gin-intro.html'

---

rule:
  id: POSTGRESQL_N_PLUS_ONE
  category: database
  name: PostgreSQL N+1 Query Problem
  description: |
    N+1 queries execute 1 query to fetch N records, then N queries to fetch related data.
    This causes severe performance issues.

    Problems:
    - N+1 round trips to database
    - Network latency multiplied by N
    - Connection pool exhaustion
    - Query timeout

    Use JOIN or subquery to fetch related data in single query.
  base_severity: 10

  patterns:
    java:
      - pattern: 'for\s*\([^)]*\)\s*\{[^}]*executeQuery'
        context: 'Java loop with executeQuery (potential N+1)'
      - pattern: 'stream\(\)\.map\([^)]*execute'
        context: 'Java stream with execute (potential N+1)'

    python:
      - pattern: 'for\s+\w+\s+in\s+[^:]+:\s*\w+\.query\(|for\s+\w+\s+in\s+[^:]+:\s*session\.execute'
        context: 'Python loop with query (potential N+1)'

    javascript:
      - pattern: 'forEach\([^)]*query\(|map\([^)]*query\('
        context: 'Node.js loop with query (potential N+1)'

  fixes:
    description: |
      Use JOIN or eager loading:

      // Bad - N+1 queries
      List<User> users = session.createQuery("FROM User", User.class).list();
      for (User user : users) {
          // N queries!
          List<Order> orders = session.createQuery(
              "FROM Order WHERE user_id = :userId", Order.class
          ).setParameter("userId", user.getId()).list();
      }

      // Good - Single query with JOIN
      List<User> users = session.createQuery(
          "SELECT DISTINCT u FROM User u LEFT JOIN FETCH u.orders",
          User.class
      ).list();

      # Python SQLAlchemy - Bad
      users = session.query(User).all()
      for user in users:
          orders = session.query(Order).filter_by(user_id=user.id).all()  # N queries!

      # Python SQLAlchemy - Good (eager loading)
      from sqlalchemy.orm import joinedload
      users = session.query(User).options(joinedload(User.orders)).all()

      -- SQL - Better (single query with subquery)
      SELECT u.*,
             (SELECT json_agg(o.*) FROM orders o WHERE o.user_id = u.id) as orders
      FROM users u;

      Monitor queries:
      SET log_min_duration_statement = 100;  -- Log queries > 100ms
      SET log_statement = 'all';              -- Log all queries (dev only)

    references:
      - 'N+1 problem: https://stackoverflow.com/questions/97197/what-is-the-n1-selects-problem'
      - 'Query optimization: https://www.postgresql.org/docs/current/using-explain.html'

---

rule:
  id: POSTGRESQL_TEXT_SEARCH_NO_INDEX
  category: database
  name: PostgreSQL Full-Text Search Without GIN Index
  description: |
    Full-text search without GIN index scans entire table.
    PostgreSQL provides powerful full-text search but requires proper indexing.

    Problems:
    - Sequential scan with tsvector generation
    - High CPU usage (text parsing)
    - Query timeout on large tables
    - Poor search performance

    Create GIN index on tsvector column for fast full-text search.
  base_severity: 8

  patterns:
    sql:
      - pattern: 'to_tsvector\([^)]*\)\s*@@\s*to_tsquery'
        context: 'Full-text search without materialized tsvector'
      - pattern: 'LIKE\s+[\'"]%.*%[\'"]|ILIKE\s+[\'"]%.*%[\'"]'
        context: 'LIKE with leading wildcard (consider full-text search)'

  fixes:
    description: |
      Use GIN index with tsvector:

      -- Bad - Full-text search without index
      SELECT * FROM articles
      WHERE to_tsvector('english', title || ' ' || content) @@ to_tsquery('postgresql');
      -- Seq Scan + expensive tsvector generation!

      -- Good - Materialized tsvector column with GIN index
      ALTER TABLE articles ADD COLUMN search_vector tsvector;

      UPDATE articles SET search_vector =
        to_tsvector('english', coalesce(title, '') || ' ' || coalesce(content, ''));

      CREATE INDEX idx_articles_search ON articles USING GIN (search_vector);

      -- Auto-update trigger
      CREATE TRIGGER articles_search_update
      BEFORE INSERT OR UPDATE ON articles
      FOR EACH ROW EXECUTE FUNCTION
        tsvector_update_trigger(search_vector, 'pg_catalog.english', title, content);

      -- Now fast search
      SELECT * FROM articles
      WHERE search_vector @@ to_tsquery('english', 'postgresql');
      -- Index Scan with GIN!

      -- Better - Ranked search with ts_rank
      SELECT title, ts_rank(search_vector, query) AS rank
      FROM articles, to_tsquery('english', 'postgresql & database') query
      WHERE search_vector @@ query
      ORDER BY rank DESC
      LIMIT 10;

      -- Advanced - Multi-language search
      ALTER TABLE articles ADD COLUMN language TEXT DEFAULT 'english';
      CREATE INDEX idx_articles_search_lang ON articles
      USING GIN (to_tsvector(language::regconfig, title || ' ' || content));

    references:
      - 'Full-text search: https://www.postgresql.org/docs/current/textsearch.html'
      - 'GIN indexes: https://www.postgresql.org/docs/current/textsearch-indexes.html'

---

rule:
  id: POSTGRESQL_REPLICATION_LAG
  category: database
  name: PostgreSQL Replication Lag Detection
  description: |
    Replication lag causes stale reads and data inconsistency.
    Monitor replication lag and tune parameters.

    Problems:
    - Stale data on replicas
    - Read-after-write inconsistency
    - Failover with data loss
    - Application errors

    Monitor pg_stat_replication and tune wal_sender/receiver.
  base_severity: 8

  patterns:
    sql:
      - pattern: 'wal_sender_timeout|max_wal_senders|wal_keep_segments'
        context: 'PostgreSQL replication configuration'

  fixes:
    description: |
      Monitor and tune replication:

      -- Check replication lag (on primary)
      SELECT
        client_addr,
        state,
        pg_wal_lsn_diff(pg_current_wal_lsn(), sent_lsn) AS send_lag_bytes,
        pg_wal_lsn_diff(pg_current_wal_lsn(), replay_lsn) AS replay_lag_bytes,
        write_lag,
        flush_lag,
        replay_lag
      FROM pg_stat_replication;

      -- Check replication delay (on replica)
      SELECT
        now() - pg_last_xact_replay_timestamp() AS replication_delay;

      -- Tune replication parameters (postgresql.conf)
      wal_level = replica
      max_wal_senders = 10
      wal_keep_size = 1GB               -- PostgreSQL 13+
      max_replication_slots = 10
      hot_standby = on
      hot_standby_feedback = on         -- Prevent query conflicts

      -- Tune wal_sender_timeout
      wal_sender_timeout = 60s          -- Default 60s

      -- Tune replica parameters
      max_standby_streaming_delay = 30s -- Max delay before query cancel
      hot_standby_feedback = on         -- Prevent vacuum conflicts

      -- Create replication slot (prevents WAL deletion)
      SELECT pg_create_physical_replication_slot('replica_slot');

      -- Monitor lag with query
      CREATE VIEW replication_lag AS
      SELECT
        CASE
          WHEN pg_last_wal_receive_lsn() = pg_last_wal_replay_lsn() THEN 0
          ELSE EXTRACT(EPOCH FROM now() - pg_last_xact_replay_timestamp())
        END AS lag_seconds;

    references:
      - 'Replication monitoring: https://www.postgresql.org/docs/current/monitoring-stats.html#MONITORING-PG-STAT-REPLICATION-VIEW'
      - 'Streaming replication: https://www.postgresql.org/docs/current/warm-standby.html'

---

rule:
  id: POSTGRESQL_PREPARED_STATEMENT_LEAK
  category: database
  name: PostgreSQL Prepared Statement Not Closed
  description: |
    Prepared statements not closed cause memory leak on server.
    PostgreSQL limits prepared statements per connection.

    Problems:
    - Memory leak on server
    - "Too many prepared statements" error
    - Connection pool exhaustion
    - Server OOM

    Always close prepared statements or use statement pooling.
  base_severity: 9

  patterns:
    java:
      - pattern: 'prepareStatement\([^)]*\)(?!.*\.close\(\))'
        context: 'Java PreparedStatement without close()'

    python:
      - pattern: 'cursor\(\)(?!.*\.close\(\))'
        context: 'Python cursor without close()'

    csharp:
      - pattern: 'new\s+NpgsqlCommand\([^)]*\)(?!.*\.Dispose\(\))'
        context: 'C# NpgsqlCommand without Dispose()'

  fixes:
    description: |
      Always close prepared statements:

      // Bad - Prepared statement leak
      public void updateUser(int id, String name) {
          PreparedStatement pstmt = conn.prepareStatement(
              "UPDATE users SET name = ? WHERE id = ?"
          );
          pstmt.setString(1, name);
          pstmt.setInt(2, id);
          pstmt.executeUpdate();
          // Missing pstmt.close()!
      }

      // Good - Try-with-resources (Java 7+)
      public void updateUser(int id, String name) throws SQLException {
          try (PreparedStatement pstmt = conn.prepareStatement(
              "UPDATE users SET name = ? WHERE id = ?"
          )) {
              pstmt.setString(1, name);
              pstmt.setInt(2, id);
              pstmt.executeUpdate();
          }  // Auto-closed
      }

      # Python - Context manager
      with conn.cursor() as cursor:
          cursor.execute("UPDATE users SET name = %s WHERE id = %s", (name, id))
      # Auto-closed

      // C# - Using statement
      using (var cmd = new NpgsqlCommand("UPDATE users SET name = @name WHERE id = @id", conn))
      {
          cmd.Parameters.AddWithValue("name", name);
          cmd.Parameters.AddWithValue("id", id);
          cmd.ExecuteNonQuery();
      }  // Auto-disposed

      -- Monitor prepared statements (PostgreSQL)
      SELECT name, statement, parameter_types
      FROM pg_prepared_statements;

      -- Deallocate all prepared statements
      DEALLOCATE ALL;

    references:
      - 'Prepared statements: https://www.postgresql.org/docs/current/sql-prepare.html'
      - 'JDBC best practices: https://jdbc.postgresql.org/documentation/head/server-prepare.html'
