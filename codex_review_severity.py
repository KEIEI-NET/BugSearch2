#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
codex_review_severity.py â€” é‡è¦åº¦é †ã‚½ãƒ¼ãƒˆç‰ˆ
å•é¡Œã®é‡å¤§åº¦ã«å¿œã˜ã¦ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¸¦ã³æ›¿ãˆã€é‡å¤§ãªå•é¡Œã‚’ä¸Šä½ã«ã€å•é¡Œãªã—ã‚’ä¸‹ä½ã«é…ç½®

é‡ç‚¹ã‚«ãƒ†ã‚´ãƒªï¼ˆé‡è¦åº¦é †ï¼‰:
  1) DBè² è·ï¼ˆN+1/å…¨ä»¶/æœªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹/å¤šé‡JOINï¼‰- æœ€é‡è¦
  2) é‡‘é¡é¢ã®ä¸æ•´åˆï¼ˆä¸¸ã‚/ç¨/å°æ•°/é€šè²¨å‹ï¼‰- é‡è¦
  3) UI/UXï¼ˆXSS/æœªæ¤œè¨¼å…¥åŠ›/å¤šé‡ã‚¯ãƒªãƒƒã‚¯ï¼‰- ä¸­ç¨‹åº¦
  4) å°åˆ·ç³»ï¼ˆé€”ä¸­åœæ­¢/ãƒšãƒ¼ã‚¸æ¬ ã‘/ãƒ¬ãƒãƒ¼ãƒˆç³»ï¼‰- ä½ç¨‹åº¦

pip install chromadb openai scikit-learn joblib regex
ç’°å¢ƒå¤‰æ•°: OPENAI_API_KEYï¼ˆå¿…é ˆ: AIãƒ¢ãƒ¼ãƒ‰ï¼‰, OPENAI_MODELï¼ˆä»»æ„: æ—¢å®š 'gpt-5'ï¼‰
"""
from __future__ import annotations
import argparse, hashlib, json, os, pathlib, re, sys, time
from dataclasses import dataclass, asdict
from typing import Any, Dict, List, Tuple

# ===== Config =====
IGNORE_DIRS = {".git","node_modules","dist","build","out","bin","obj",".idea",".vscode",".next","coverage","target"}
DEFAULT_MAX_FILE_BYTES = 4_000_000  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ4MB
ENV_FILE = ".env"
INDEX_PATH = ".advice_index.jsonl"
VEC_PATH = ".advice_tfidf.pkl"
MATRIX_PATH = ".advice_matrix.pkl"
BATCH_SIZE = 5  # AIã«ä¸€åº¦ã«é€ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«æ•°
MAX_PROMPT_SIZE = 8000  # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æœ€å¤§æ–‡å­—æ•°
AI_TIMEOUT = 240  # 240ç§’ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ

# é‡è¦åº¦å®šç¾©
SEVERITY_SCORES = {
    # DBé–¢é€£ï¼ˆæœ€é‡è¦: 10ç‚¹ï¼‰
    "DB: ãƒ«ãƒ¼ãƒ—å†…SELECT (N+1) ç–‘ã„": 10,
    "DB: SELECT * â†’è² è·å¢—ã€‚åˆ—é™å®š": 8,
    "DB: å¤šé‡JOINâ†’é…å»¶ã®æã‚Œã€‚è¨ˆç”»/IDXç¢ºèª": 7,
    "DB: å¤§OFFSETâ†’é…å»¶ã€‚IDã‚«ãƒ¼ã‚½ãƒ«æ¨å¥¨": 6,

    # é‡‘é¡é–¢é€£ï¼ˆé‡è¦: 8ç‚¹ï¼‰
    "é‡‘é¡: æµ®å‹•å°æ•°ã§é‡‘é¡â†’èª¤å·®ã€‚Decimal/é€šè²¨å‹ã¸": 9,
    "é‡‘é¡: ç¨è¾¼/ç¨æŠœãƒ»ç«¯æ•°å‡¦ç†ã®æ··åœ¨æ³¨æ„": 7,

    # UI/ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ï¼ˆä¸­ç¨‹åº¦: 5ç‚¹ï¼‰
    "UI: XSSè„†å¼±æ€§ã®ç–‘ã„ï¼ˆæœªã‚µãƒ‹ã‚¿ã‚¤ã‚ºHTMLï¼‰": 8,
    "UI: å…¥åŠ›æ¤œè¨¼ãŒä¸ååˆ†": 5,
    "UI: å¤šé‡ã‚¯ãƒªãƒƒã‚¯é˜²æ­¢ãªã—": 3,

    # å°åˆ·é–¢é€£ï¼ˆä½ç¨‹åº¦: 2ç‚¹ï¼‰
    "å°åˆ·: ã‚¨ãƒ©ãƒ¼å‡¦ç†ãªã—â†’é€”ä¸­åœæ­¢ãƒªã‚¹ã‚¯": 4,
    "å°åˆ·: æ”¹ãƒšãƒ¼ã‚¸åˆ¤å®šãŒä¸ååˆ†": 2,
}

# ===== Optional deps =====
SKLEARN_OK = False
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    import joblib
    SKLEARN_OK = True
except Exception:
    pass

OPENAI_OK = False
try:
    from openai import OpenAI
    OPENAI_OK = True
except Exception:
    pass

# ===== Utils =====
if ENV_FILE and pathlib.Path(ENV_FILE).exists():
    with open(ENV_FILE, "r", encoding="utf-8") as f:
        for line in f:
            if "=" in line: key, val = line.strip().split("=", 1); os.environ[key] = val

@dataclass
class Doc:
    path: str; lang: str; size: int; sha1: str; tags: List[str]; summary: str; text: str

def sha1_bytes(data: bytes) -> str:
    return hashlib.sha1(data).hexdigest()[:10]

def detect_lang(p: pathlib.Path) -> str:
    ext = p.suffix.lower()
    if ext in (".py",): return "python"
    if ext in (".js",".mjs",".cjs",".jsx"): return "javascript"
    if ext in (".ts",".tsx"): return "typescript"
    if ext in (".cs",".csx"): return "csharp"
    if ext in (".java",): return "java"
    if ext in (".pas",".dfm",".dpr"): return "delphi"
    if ext in (".rb",): return "ruby"
    if ext in (".go",): return "go"
    if ext in (".rs",): return "rust"
    if ext in (".php",): return "php"
    if ext in (".c",".h"): return "c"
    if ext in (".cpp",".hpp",".cc",".hh",".cxx",".hxx"): return "cpp"
    return "other"

def make_summary(text: str) -> str:
    lines = text.splitlines()[:10]; return " / ".join(l.strip()[:80] for l in lines if l.strip())[:400]

def write_large_file_log(large_files: List[Tuple[str, int]], output_dir: pathlib.Path, max_file_bytes: int) -> pathlib.Path|None:
    if not large_files: return None
    reports_dir = output_dir / "reports"; reports_dir.mkdir(parents=True, exist_ok=True)
    log_path = reports_dir / "large_files_over_limit.log"
    with open(log_path, "w", encoding="utf-8") as f:
        f.write(f"Large files exceeding limit ({max_file_bytes:,} bytes):\n\n")
        for file_path, size in sorted(large_files, key=lambda x: x[1], reverse=True):
            f.write(f"{size:12,} bytes  {file_path}\n")
        f.write(f"\nTotal: {len(large_files)} files\n")
    return log_path

# ===== Rule engine =====
def scan_money(text: str) -> List[str]:
    m: List[str] = []
    if re.search(r"\b(float|double|real)\b.*\b(price|cost|money|amount|tax|total|sum|charge|fee|pay)", text, re.IGNORECASE):
        m.append("é‡‘é¡: æµ®å‹•å°æ•°ã§é‡‘é¡â†’èª¤å·®ã€‚Decimal/é€šè²¨å‹ã¸")
    if re.search(r"\b(total|sum|amount)\s*=.*\+.*tax", text, re.IGNORECASE) and re.search(r"(Math\.(floor|round)|parseInt|truncate)", text, re.IGNORECASE):
        m.append("é‡‘é¡: ç¨è¾¼/ç¨æŠœãƒ»ç«¯æ•°å‡¦ç†ã®æ··åœ¨æ³¨æ„")
    return m

def scan_print(text: str) -> List[str]:
    m: List[str] = []
    if re.search(r"(window\.print|PrintDocument|Printer|PrintPage|HasMorePages|NewPage|PrintPreview)", text, re.IGNORECASE) and not re.search(r"(try|catch|error|exception)", text, re.IGNORECASE):
        m.append("å°åˆ·: ã‚¨ãƒ©ãƒ¼å‡¦ç†ãªã—â†’é€”ä¸­åœæ­¢ãƒªã‚¹ã‚¯")
    if re.search(r"\.HasMorePages\s*=\s*true", text, re.IGNORECASE) and not re.search(r"(yPos|currentY|pageHeight|margin)", text, re.IGNORECASE):
        m.append("å°åˆ·: æ”¹ãƒšãƒ¼ã‚¸åˆ¤å®šãŒä¸ååˆ†")
    return m

def scan_ui(text: str) -> List[str]:
    m: List[str] = []
    if re.search(r"(button|submit|click|tap|press)", text, re.IGNORECASE) and not re.search(r"(disabled|loading|spinner|prevent|debounce|throttle)", text, re.IGNORECASE):
        m.append("UI: å¤šé‡ã‚¯ãƒªãƒƒã‚¯é˜²æ­¢ãªã—")
    if re.search(r"(innerHTML\s*=|dangerouslySetInnerHTML|v-html)", text) and not re.search(r"(sanitize|escape|DOMPurify|textContent)", text, re.IGNORECASE):
        m.append("UI: XSSè„†å¼±æ€§ã®ç–‘ã„ï¼ˆæœªã‚µãƒ‹ã‚¿ã‚¤ã‚ºHTMLï¼‰")
    if re.search(r"(input|textarea|select)", text, re.IGNORECASE) and not re.search(r"(validate|pattern|required|maxlength|regex|test|match)", text, re.IGNORECASE):
        m.append("UI: å…¥åŠ›æ¤œè¨¼ãŒä¸ååˆ†")
    return m

def scan_db(text: str) -> List[str]:
    m: List[str] = []
    if re.search(r"SELECT\s+\*\s+FROM", text, re.IGNORECASE):
        m.append("DB: SELECT * â†’è² è·å¢—ã€‚åˆ—é™å®š")
    if re.search(r"(for|while|foreach|map).*\n.*\n.*\n.*(SELECT|INSERT|UPDATE|DELETE)", text, re.IGNORECASE):
        m.append("DB: ãƒ«ãƒ¼ãƒ—å†…SELECT (N+1) ç–‘ã„")
    if re.search(r"(JOIN.*){3,}", text, re.IGNORECASE):
        m.append("DB: å¤šé‡JOINâ†’é…å»¶ã®æã‚Œã€‚è¨ˆç”»/IDXç¢ºèª")
    if re.search(r"(OFFSET|LIMIT)\s+\d{4,}", text, re.IGNORECASE):
        m.append("DB: å¤§OFFSETâ†’é…å»¶ã€‚IDã‚«ãƒ¼ã‚½ãƒ«æ¨å¥¨")
    return m

def make_tags(text: str) -> List[str]:
    t = []
    if re.search(r"\b(price|cost|money|amount|tax|total|sum|charge|fee|pay|invoice|billing|currency|decimal|round)", text, re.IGNORECASE): t.append("money")
    if re.search(r"\b(print|printer|page|paper|report|pdf|export|PrintDocument|PrintPage|HasMorePages)", text, re.IGNORECASE): t.append("print")
    if re.search(r"\b(button|click|submit|form|input|select|modal|dialog|toast|spinner|loading|disabled)", text, re.IGNORECASE): t.append("uiux")
    if re.search(r"\b(SELECT|INSERT|UPDATE|DELETE|JOIN|WHERE|FROM|query|sql|database|transaction|connection)", text, re.IGNORECASE): t.append("db")
    if re.search(r"\b(api|http|fetch|axios|request|response|endpoint|REST|webhook|curl)", text, re.IGNORECASE): t.append("net")
    if re.search(r"\b(file|fs|stream|read|write|upload|download|path|directory|folder)", text, re.IGNORECASE): t.append("io")
    return sorted(set(t))

# ===== Indexer =====
def should_index(p: pathlib.Path, exclude_langs: set) -> bool:
    if p.is_dir(): return False
    if any(part.startswith(".") for part in p.parts): return False
    lang = detect_lang(p)
    # æŒ‡å®šã•ã‚ŒãŸè¨€èªã‚’é™¤å¤–
    if lang in exclude_langs:
        return False
    if lang == "other":
        important_ext = {".xml", ".json", ".yaml", ".yml", ".toml", ".ini", ".config", ".md", ".txt", ".html", ".htm", ".css"}
        if p.suffix.lower() in important_ext: return True
        if p.name in {"Dockerfile", "Makefile", "Rakefile", "Gemfile", ".env", ".env.example"}: return True
        return False
    return True

def cmd_index(repo: pathlib.Path, index_path: pathlib.Path, exclude_langs: set = None, max_file_bytes: int = None):
    if exclude_langs is None:
        exclude_langs = set()
    if max_file_bytes is None:
        max_file_bytes = DEFAULT_MAX_FILE_BYTES

    paths = []; large_files = []
    for p in repo.rglob("*"):
        if p.is_file() and not any(d in p.parts for d in IGNORE_DIRS):
            file_size = p.stat().st_size
            if file_size > max_file_bytes:
                large_files.append((str(p.relative_to(repo)), file_size))
                continue
            if should_index(p, exclude_langs): paths.append(p)
    count = 0
    with open(index_path, "w", encoding="utf-8") as w:
        for p in sorted(paths):
            lang = detect_lang(p)
            try: txt = p.read_text(encoding="utf-8", errors="ignore")
            except Exception: continue
            tags = make_tags(txt)
            rel_path = str(p.relative_to(repo))
            encoded = txt.encode("utf-8", errors="ignore")
            doc = Doc(path=rel_path, lang=lang, size=len(encoded), sha1=sha1_bytes(encoded), tags=tags, summary=make_summary(txt), text=txt)
            w.write(json.dumps(asdict(doc), ensure_ascii=False) + "\n"); count += 1
    print(f"[OK] Indexed {count} files -> {index_path}")
    log_path = write_large_file_log(large_files, index_path.parent.resolve(), max_file_bytes)
    if log_path:
        threshold_mb = max_file_bytes / 1_000_000
        try:
            rel_log = log_path.relative_to(index_path.parent.resolve())
        except ValueError:
            rel_log = log_path
        print(f"[WARNING] Skipped {len(large_files)} files exceeding ~{threshold_mb:.1f} MB. Details: {rel_log}")

# ===== Retrieval =====
def load_index(index_path: pathlib.Path) -> List[Dict[str,Any]]:
    arr: List[Dict[str,Any]] = []
    with open(index_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line: arr.append(json.loads(line))
    return arr

def retrieve(query: str, index: List[Dict[str,Any]], topk: int = 10) -> List[Dict[str,Any]]:
    if SKLEARN_OK:
        vec_p = pathlib.Path(VEC_PATH); mat_p = pathlib.Path(MATRIX_PATH)
        if vec_p.exists() and mat_p.exists():
            return retrieve_tfidf(query, index, vec_p, mat_p, topk)
    return retrieve_naive(query, index, topk)

def retrieve_naive(query: str, index: List[Dict[str,Any]], topk: int) -> List[Dict[str,Any]]:
    q_words = set(w.lower() for w in re.split(r"\W+", query) if w)
    for doc in index:
        txt_words = set(w.lower() for w in re.split(r"\W+", doc.get("text","")[:3000]) if w)
        doc["_score"] = len(q_words & txt_words) / (len(q_words) + 0.01)
    return sorted(index, key=lambda x: x.get("_score",0), reverse=True)[:topk]

def retrieve_tfidf(query: str, index: List[Dict[str,Any]], vec_path: pathlib.Path, mat_path: pathlib.Path, topk: int) -> List[Dict[str,Any]]:
    from sklearn.metrics.pairwise import cosine_similarity
    vec = joblib.load(vec_path); mat = joblib.load(mat_path)
    q_vec = vec.transform([query])
    scores = cosine_similarity(q_vec, mat).flatten()
    for i, doc in enumerate(index):
        if i < len(scores):
            doc["_score"] = float(scores[i])
    return sorted(index, key=lambda x: x.get("_score",0), reverse=True)[:topk]

def cmd_vectorize(index_path: pathlib.Path):
    if not SKLEARN_OK:
        print("scikit-learnæœªå°å…¥ã€‚pip install scikit-learn joblib")
        return
    index = load_index(index_path)
    texts = [d.get("text","")[:10000] for d in index]
    vec = TfidfVectorizer(max_features=5000, ngram_range=(1,2), stop_words=None)
    mat = vec.fit_transform(texts)
    joblib.dump(vec, VEC_PATH); joblib.dump(mat, MATRIX_PATH)
    print(f"[OK] Vectorized -> {VEC_PATH}, {MATRIX_PATH}")

# ===== Advice rules =====
def rule_advices(d: Dict[str,Any]) -> List[str]:
    text = d.get("text", "")[:20000]
    out: List[str] = []
    out += scan_money(text)
    out += scan_print(text)
    out += scan_ui(text)
    out += scan_db(text)
    return out[:8]

# ===== é‡è¦åº¦ã‚¹ã‚³ã‚¢è¨ˆç®— =====
def calculate_severity_score(suspicions: List[str]) -> int:
    """å•é¡Œãƒªã‚¹ãƒˆã‹ã‚‰é‡è¦åº¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
    total_score = 0
    for suspicion in suspicions:
        # å®Œå…¨ä¸€è‡´ã¾ãŸã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¹ã‚³ã‚¢
        total_score += SEVERITY_SCORES.get(suspicion, 1)
    return total_score

def make_advice_entry_with_severity(d: Dict[str,Any]) -> Dict[str,Any]:
    """é‡è¦åº¦ã‚¹ã‚³ã‚¢ä»˜ãã®ã‚¨ãƒ³ãƒˆãƒªã‚’ä½œæˆ"""
    suspicions = rule_advices(d)
    severity_score = calculate_severity_score(suspicions)

    return {
        "path": d["path"],
        "lang": d["lang"],
        "score": round(d.get("_score",0.0),4),
        "tags": d.get("tags",[]),
        "suspicions": suspicions,
        "severity_score": severity_score,  # é‡è¦åº¦ã‚¹ã‚³ã‚¢è¿½åŠ 
        "severity_level": get_severity_level(severity_score)  # é‡è¦åº¦ãƒ¬ãƒ™ãƒ«è¿½åŠ 
    }

def get_severity_level(score: int) -> str:
    """ã‚¹ã‚³ã‚¢ã‹ã‚‰é‡è¦åº¦ãƒ¬ãƒ™ãƒ«ã‚’åˆ¤å®š"""
    if score >= 10:
        return "ğŸ”´ ç·Šæ€¥"
    elif score >= 7:
        return "ğŸŸ  é«˜"
    elif score >= 4:
        return "ğŸŸ¡ ä¸­"
    elif score >= 1:
        return "ğŸ”µ ä½"
    else:
        return "âšª å•é¡Œãªã—"

# ===== GPT-5 =====
def ai_review(query: str, snippets: List[str]) -> str:
    if not OPENAI_OK: return "(AIæœªæœ‰åŠ¹: openaiæœªå°å…¥)"
    api_key = os.environ.get("OPENAI_API_KEY")
    if not api_key: return "(AIæœªæœ‰åŠ¹: OPENAI_API_KEY æœªè¨­å®š)"
    # GPT-5ãŒç©ºã®å¿œç­”ã‚’è¿”ã™ãŸã‚ã€gpt-4oã‚’ä½¿ç”¨
    model = os.environ.get("OPENAI_MODEL", "gpt-4o")
    client = OpenAI(api_key=api_key)

    prompt = (
        "ã‚ãªãŸã¯ã‚·ãƒ‹ã‚¢ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼å ±å‘Š: \n" +
        json.dumps(query, ensure_ascii=False) +
        "\næ¬¡ã®ã‚³ãƒ¼ãƒ‰æ–­ç‰‡ã‚’æ ¹æ‹ ã«ã€(1)é‡‘é¡ (2)å°åˆ· (3)UI/UX (4)DBè² è· ã‚’é‡ç‚¹ã«åŠ©è¨€ã ã‘ã‚’å‡ºã—ã¦ãã ã•ã„ã€‚\n" +
        "ç¢ºåº¦ãŒä½ã„æŒ‡æ‘˜ã¯ã€å¯èƒ½æ€§ã€ã¨æ˜è¨˜ã€‚ä¿®æ­£ã‚³ãƒ¼ãƒ‰ã¯å‡ºåŠ›ã—ãªã„ã€‚\n=== å€™è£œã‚³ãƒ¼ãƒ‰ ===\n" +
        json.dumps(snippets, ensure_ascii=False)[:5000]
    )
    try:
        resp = client.chat.completions.create(
            model=model,
            messages=[{"role":"user","content":prompt}],
            temperature=1,
            timeout=60
        )
        return resp.choices[0].message.content.strip()
    except Exception as e:
        return f"(AIã‚¨ãƒ©ãƒ¼: {str(e)[:100]})"

# ===== Report (é‡è¦åº¦é †) =====
def render_report_sorted(title: str, items: List[Dict[str,Any]], ai_summary: str|None) -> str:
    lines: List[str] = [
        f"# {title}",
        "",
        f"ç”Ÿæˆ: {time.strftime('%Y-%m-%d %H:%M:%S')} (èª­å–å°‚ç”¨)",
        ""
    ]

    if ai_summary:
        lines += ["## AIãƒ¬ãƒ“ãƒ¥ãƒ¼è¦ç´„ (GPT-5)", "", ai_summary, ""]

    # é‡è¦åº¦ã§ã‚½ãƒ¼ãƒˆï¼ˆé«˜ã„é †ï¼‰
    sorted_items = sorted(items, key=lambda x: x.get('severity_score', 0), reverse=True)

    # é‡è¦åº¦åˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    critical_items = [it for it in sorted_items if it.get('severity_score', 0) >= 10]
    high_items = [it for it in sorted_items if 7 <= it.get('severity_score', 0) < 10]
    medium_items = [it for it in sorted_items if 4 <= it.get('severity_score', 0) < 7]
    low_items = [it for it in sorted_items if 1 <= it.get('severity_score', 0) < 4]
    no_issue_items = [it for it in sorted_items if it.get('severity_score', 0) == 0]

    # çµ±è¨ˆæƒ…å ±
    lines += [
        "## ğŸ“Š å•é¡Œã®åˆ†å¸ƒ",
        f"- ğŸ”´ ç·Šæ€¥: {len(critical_items)}ä»¶",
        f"- ğŸŸ  é«˜: {len(high_items)}ä»¶",
        f"- ğŸŸ¡ ä¸­: {len(medium_items)}ä»¶",
        f"- ğŸ”µ ä½: {len(low_items)}ä»¶",
        f"- âšª å•é¡Œãªã—: {len(no_issue_items)}ä»¶",
        f"- **åˆè¨ˆ**: {len(items)}ä»¶",
        ""
    ]

    # é‡è¦åº¦åˆ¥ã«å‡ºåŠ›
    if critical_items:
        lines.append("## ğŸ”´ ç·Šæ€¥å¯¾å¿œãŒå¿…è¦ãªå•é¡Œ\n")
        for i, it in enumerate(critical_items, 1):
            lines += format_item(i, it)

    if high_items:
        lines.append("## ğŸŸ  é«˜å„ªå…ˆåº¦ã®å•é¡Œ\n")
        for i, it in enumerate(high_items, len(critical_items) + 1):
            lines += format_item(i, it)

    if medium_items:
        lines.append("## ğŸŸ¡ ä¸­å„ªå…ˆåº¦ã®å•é¡Œ\n")
        for i, it in enumerate(medium_items, len(critical_items) + len(high_items) + 1):
            lines += format_item(i, it)

    if low_items:
        lines.append("## ğŸ”µ ä½å„ªå…ˆåº¦ã®å•é¡Œ\n")
        for i, it in enumerate(low_items, len(critical_items) + len(high_items) + len(medium_items) + 1):
            lines += format_item(i, it)

    if no_issue_items:
        lines.append("## âšª å•é¡ŒãŒæ¤œå‡ºã•ã‚Œãªã‹ã£ãŸãƒ•ã‚¡ã‚¤ãƒ«\n")
        for i, it in enumerate(no_issue_items, len(items) - len(no_issue_items) + 1):
            lines += format_item_minimal(i, it)

    return "\n".join(lines)

def format_item(num: int, item: Dict[str,Any]) -> List[str]:
    """å•é¡Œã‚ã‚Šã‚¢ã‚¤ãƒ†ãƒ ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    lines = [
        f"### {num}. {item['path']}",
        f"- **è¨€èª**: {item['lang']}",
        f"- **é‡è¦åº¦**: {item.get('severity_level', 'ä¸æ˜')} (ã‚¹ã‚³ã‚¢: {item.get('severity_score', 0)})",
        f"- **æ¤œç´¢ã‚¹ã‚³ã‚¢**: {item.get('score', 0)}",
        f"- **ã‚¿ã‚°**: {', '.join(item.get('tags', [])) or '-'}",
        "- **æ¤œå‡ºã•ã‚ŒãŸå•é¡Œ**:"
    ]

    susp = item.get("suspicions", [])
    if susp:
        for s in susp:
            severity = SEVERITY_SCORES.get(s, 1)
            emoji = "ğŸ”´" if severity >= 8 else "ğŸŸ " if severity >= 5 else "ğŸŸ¡" if severity >= 3 else "ğŸ”µ"
            lines.append(f"  - {emoji} {s}")
    else:
        lines.append("  - (ãªã—)")

    lines.append("")
    return lines

def format_item_minimal(num: int, item: Dict[str,Any]) -> List[str]:
    """å•é¡Œãªã—ã‚¢ã‚¤ãƒ†ãƒ ã®ç°¡æ½”ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    return [
        f"### {num}. {item['path']} ({item['lang']})",
        f"- ã‚¿ã‚°: {', '.join(item.get('tags', [])) or '-'}",
        ""
    ]

# ===== Commands =====
def cmd_query(query: str, topk: int, mode: str, index_path: pathlib.Path, out: pathlib.Path|None):
    index = load_index(index_path)
    cands = retrieve(query, index, topk=topk)
    items = [make_advice_entry_with_severity(d) for d in cands]

    ai_summary = None
    if mode in ("ai","hybrid"):
        ai_summary = ai_review(query, [d["text"] for d in cands[:5]])

    rep = render_report_sorted(f"æ¤œç´¢ãƒ¬ãƒãƒ¼ãƒˆ: {query}", items, ai_summary)
    if out:
        out.parent.mkdir(parents=True, exist_ok=True)
        out.write_text(rep, encoding="utf-8")
        print(f"[OK] wrote {out}")
    else:
        print(rep)

def cmd_advise(topk: int, mode: str, index_path: pathlib.Path, out: pathlib.Path|None):
    seed_query = "é‡‘é¡ ä¸æ•´åˆ ç¨ å°æ•° å°åˆ· Print HasMorePages NewPage UI UX ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£ DB N+1 JOIN é…ã„"
    index = load_index(index_path)
    cands = retrieve(seed_query, index, topk=topk)
    items = [make_advice_entry_with_severity(d) for d in cands]

    ai_summary = None
    if mode in ("ai","hybrid"):
        ai_summary = ai_review("æ¨ªæ–­åŠ©è¨€ï¼ˆé‡‘é¡/å°åˆ·/UI/DBï¼‰", [d["text"] for d in cands[:5]])

    rep = render_report_sorted("å…¨ä½“åŠ©è¨€ï¼ˆæ¨ªæ–­ãƒã‚§ãƒƒã‚¯ï¼‰", items, ai_summary)
    if out:
        out.parent.mkdir(parents=True, exist_ok=True)
        out.write_text(rep, encoding="utf-8")
        print(f"[OK] wrote {out}")
    else:
        print(rep)

# ===== CLI =====
if __name__ == "__main__":
    ap = argparse.ArgumentParser(description="èª­å–å°‚ç”¨ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆé‡è¦åº¦é †ã‚½ãƒ¼ãƒˆç‰ˆï¼‰")
    sub = ap.add_subparsers(dest="cmd", required=True)

    ap_idx = sub.add_parser("index", help="ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ï¼ˆèª­å–ã®ã¿ï¼‰")
    ap_idx.add_argument("repo", type=str)
    ap_idx.add_argument("--exclude-langs", type=str, nargs="*", help="é™¤å¤–ã™ã‚‹è¨€èª (ä¾‹: delphi java)")
    ap_idx.add_argument("--max-file-mb", type=float, default=4.0, help="æœ€å¤§ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º(MB) ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ4MB")

    ap_vec = sub.add_parser("vectorize", help="(ä»»æ„) TF-IDFãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆ")
    ap_vec.add_argument("--index", type=str, default=INDEX_PATH)

    ap_q = sub.add_parser("query", help="è‡ªç„¶è¨€èªã§é–¢é€£ã‚½ãƒ¼ã‚¹ã‚’æ¢ç´¢ï¼ˆé‡è¦åº¦é †ï¼‰")
    ap_q.add_argument("query", type=str, help="æ¤œç´¢ã‚¯ã‚¨ãƒª")
    ap_q.add_argument("--topk", type=int, default=30, help="æ¤œç´¢ä¸Šä½Nä»¶")
    ap_q.add_argument("--mode", choices=["rules","ai","hybrid"], default="hybrid")
    ap_q.add_argument("--index", type=str, default=INDEX_PATH)
    ap_q.add_argument("--out", type=str, help="å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ« (Markdown)")

    ap_adv = sub.add_parser("advise", help="å…¨ä½“åŠ©è¨€ï¼ˆé‡è¦åº¦é †ï¼‰")
    ap_adv.add_argument("--topk", type=int, default=80, help="åŠ©è¨€å¯¾è±¡ä¸Šä½Nä»¶")
    ap_adv.add_argument("--mode", choices=["rules","ai","hybrid"], default="hybrid")
    ap_adv.add_argument("--index", type=str, default=INDEX_PATH)
    ap_adv.add_argument("--out", type=str, help="å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ« (Markdown)")

    args = ap.parse_args()
    if args.cmd == "index":
        repo = pathlib.Path(args.repo)
        exclude_langs = set(args.exclude_langs) if args.exclude_langs else set()
        max_file_bytes = int(args.max_file_mb * 1_000_000)
        cmd_index(repo, pathlib.Path(INDEX_PATH), exclude_langs, max_file_bytes)
    elif args.cmd == "vectorize":
        cmd_vectorize(pathlib.Path(args.index))
    elif args.cmd == "query":
        out = pathlib.Path(args.out) if args.out else None
        cmd_query(args.query, args.topk, args.mode, pathlib.Path(args.index), out)
    elif args.cmd == "advise":
        out = pathlib.Path(args.out) if args.out else None
        cmd_advise(args.topk, args.mode, pathlib.Path(args.index), out)